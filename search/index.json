[{"content":"各类型编程语言对比 面向对象式编程(Java、C++)：面向对象编程，是一种程序设计范式，也是一种编程语言的分类。它以对象作为程序的基本单元，将算法和数据封装其中，程序可以访问和修改对象关联的数据。在面向对象编程中，我们可以操作对象，而不需要关心对象的内部结构和实现。\n面向过程式编程(C)：是一种以过程为中心的编程思想, 分析出解决问题的所需要的步骤,然后用函数把这些步骤一步一步实现,然后依次调用;\n面向函数式编程(scala)：函数式编程（Functional Programming, FP）是一种编程范式，它将计算视为数学中函数的求值过程，并避免使用程序状态及可变数据。在函数式编程中，函数被视为\u0026quot;第一等公民\u0026quot;，这意味着函数可以作为参数传递、作为返回值，甚至可以赋值给变量。函数式编程强调使用一系列的函数来处理数据，而不是依赖于数据的状态变化。\n封装、继承与多态 封装 概念： 将一些属性和相关方法封装在一个对象中，对外隐藏内部具体实现细节。内部实现，外界不需要关心，外界只需要根据“内部提供的接口”去使用就可以。\n好处： 使用起来更加方便：因为已经把很多相关的功能，封装成一个整体，类似于像外界提供一个工具箱，针对于不同的场景，使用不同的工具箱就可以；\n保证数据的安全：针对于安全级别高的数据，可以设置成”私有“，可以控制数据为只读（外界无法修改），也可以拦截数据的写操作（进行数据校验和过滤）；\n利于代码维护：如果后期，功能代码需要维护，则直接修改这个类内部代码即可；只要保证接口名称不变，外界不需要做任何代码修改。\n继承 概念： 通过必要的说明能够实现某个类无需重新定义就能拥有另一个类的某些属性和方法，这种关系就称为继承，并且允许多层的继承关系。先定义的类称为父类（基类、超类），后定义的类称为子类（派生类）。\n注：Java中只能单继承（一个子类继承一个父类），而Python、C++中则支持多继承\n多态 多态是指父类的变量可以指向子类对象。允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用）。\n重写与重载 方法的重写(Overriding)和重载(Overloading)是Java多态性的不同表现。\n重写是父类与子类之间多态性的一种表现，重载是一个类中多态性的一种表现。\n如果在子类中定义某方法与其父类有相同的名称和参数，我们说该方法被重写。子类的对象使用这个方法时将调用子类的定义，对它而言，父类中的定义如果被覆盖了。\n如果一个类中定义了多个同名的方法，它们或有不同的参数个数，或有不同的参数类型，则称为方法的重载。重载的方法可以修改返回值的类型。\n构造方法的特殊之处 构造方法必须比类目相同 构造方法没有返回值，但不用void声明 构造方法不能使用static、final、abstract、synchonized和native等修饰符 构造方法不能像一般方法那样用对象.构造方法()显示地直接调用，应用new关键字调用构造方法，给新对象初始化 实例方法与类方法 static修饰地方法称为类方法（或静态方法），而没用static修饰地方法称为是实例方法，二者调用方式不同。 实例方法属于实例，必须通过实例调用；类方法属于类，一般通过类名调用，也可以通过实例调用。二者访问地成员不同。实例方法可以直接访问该类地实例变量和实例方法，也可以访问类变量和类方法；类方法只能访问该类地类变量和类方法，不同直接访问实例变量和实例方法。\n类方法要访问实例变量或调用实例方法，必须首先获得该实例，然后通过该实例访问相关地实例变量或调用实例方法。\n抽象类与接口 相同点：抽象类和接口都可以有抽象方法，都不可以被实例化\n不同点：\n创建类关键字不同：抽象类用关键字abstract，接口用关键字interface创建 成员变量不同：抽象类可以包含普通的变量，接口内的变量只能是final的 方法不同：抽象类可以包含普通的方法，接口内只能有抽象的方法，且都是public的 继承/实现不同：接口可以被多实现，抽象类只能被单继承 ","date":"2024-10-25T21:13:16+08:00","permalink":"https://rusthx.github.io/p/java%E9%9D%A2%E7%BB%8F%E5%A4%87%E5%BF%98%E5%BD%95/","title":"Java面经备忘录"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 usage: hive -d,--define \u0026lt;key=value\u0026gt; Variable substitution to apply to Hive commands. e.g. -d A=B or --define A=B --database \u0026lt;databasename\u0026gt; Specify the database to use -e \u0026lt;quoted-query-string\u0026gt; SQL from command line (SQL来自命令行) -f \u0026lt;filename\u0026gt; SQL from files (SQL来自文件) -H,--help Print help information --hiveconf \u0026lt;property=value\u0026gt; Use value for given property (通过命令行参数的方式进行配置信息的设置) --hivevar \u0026lt;key=value\u0026gt; Variable substitution to apply to Hive commands. e.g. --hivevar A=B -i \u0026lt;filename\u0026gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) (详细模式,在控制台输出SQL执行) ","date":"2024-10-25T21:12:09+08:00","permalink":"https://rusthx.github.io/p/hive%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0/","title":"Hive启动参数"},{"content":"定义 数据库死锁是在多个事务执行过程中发生的一种状态，其中每个事务都在等待其他事务释放它们需要的资源，而这些资源又被其他事务占用。这种相互等待的情况导致事务无法继续执行，因为没有任何事务能够获取它们所需的全部资源来完成操作。\n死锁死循环四要素 互斥条件：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。 请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。 死锁避免方案 数据库管理系统通常会实现死锁检测和解决机制。当检测到死锁时，系统会选择一个事务进行回滚，以解除死锁状态。选择回滚哪个事务通常基于事务的复杂度，系统会尽量选择代价最小的事务进行回滚。\n为了避免死锁，可以采取以下措施：\n保持一致的加锁顺序：确保所有事务都以相同的顺序请求锁。\n减少锁的持有时间：尽快完成事务操作并释放锁，避免长时间持有锁。\n使用锁超时：设置锁的超时时间，超时后事务自动回滚，释放锁。\n检测死锁并重试：在应用程序中捕获死锁异常，并实现重试机制。\n","date":"2024-10-13T21:32:40+08:00","permalink":"https://rusthx.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%AD%BB%E9%94%81/","title":"数据库死锁"},{"content":" 参考资料：B站@左美美_ 相关视频\n数据倾斜定义 任务进度长时间维持在99%，查看任务监异页面，发现只有少量1个或几个reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多，最长时长远大于平均时长。\n数据倾斜产生原因 map输出数据按keyHash的分配到reduce中，由于key分布不均匀、业务数据本身的特性、建表时考虑不周、某些SQL语句本身就有数据倾斜等原因，造成的reduce上的数据量差异过大，所以如何将数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。\nKey为空引起数据倾斜 倾斜原因 join的key值发生倾斜，key值包含很多空值或是异常值。\n解决方案 对值为空的key进行打散，为空key赋一个随机的值，使得key值为空的数据随机均匀地分布到不同的reducer上。\n测试案例 设置多个reduce任务\nset mapreduce.job.reduces = 5;\n两张大表join，做全连接\n1 2 3 4 5 6 7 select t.id ,t.year ,t.temperature ,s.state from temperature t full join station s on nvl(t.id,rand())=s.id limit 10; 备注：nvl()为空值转换函数，rand()为随机函数。也可以使用ifnull()、coalesce()\ngroup by 引起数据倾斜 倾斜原因 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n解决方案 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。 实现方式1\n1）是否在Map端进行聚合，默认为True（使用Combiner局部合并）\nset hive.map.aggr = true;\n2）设置map端预聚合的行数阈值\nset hive.groupby.mapaggr.checkinterval=100000;\n实现方式2\n有数据倾斜的时候进行负载均衡（默认是false）\nset hive.groupby.skelindata= true;\n当遇到数据倾斜时，groupby会启动两个MR job。第一个job会将map端数据随机输入reducer，每个reducer做部分聚合，相同的key就会分布在不同的reducer中。第二个job再将前面预处理过的数据按key聚合并输出结果，这样就起到了均衡的效果。\n测试案例 1 2 3 set hive.map.aggr =true; set hive.groupby.mapaggr.checkinterval= 100000; set hive.groupby.skewindata= true; 1 2 3 select id,count(*) from temperature group by id; count distinct引起数据倾斜 倾斜原因 count distinct聚合时存在大量特殊值，比如存在大量值为NULL或空的记录。\n解决方案 做count distinct时，将值为空的情况单独处理。\n1）如果只是统计去重后的记录数，可以不用处理空值，先把空值过滤掉，然后在最后结果中加1即可\n2）如果还包含其他计算，需要进行groupby操作，先将值为空的记录单独处理，然后再跟其他计算结果union操作。\njoin操作引起数据倾斜 大表join小表(hive旧版本) 新版本(Hive3)已经自动自动优化\n1）产生原因\n业务数据本身就存在key分布不均匀的情况，一般情况会产生数据倾斜\n2）解决方式\n使用map join让小的维度表先进内存，在map端完成join\n3）实现原理\n使用map join，直接在map端就完成表的join操作，进入map端的数据都是经过split得到的，没有根据key分区这一操作，所以数据都是相对均匀地分布在每个maptask中的，所以就不会产生数据倾斜。\n大表join大表 1）产生原因\n业务数据本身的特性，导致两个表都是大表。\n2）解决方式\n业务消减\n3）实现原理\n业务数据有数据倾斜的风险，但是这些导致数据倾斜风险的key一般都是无效的，如uid为空，因为uid为空的记录是没有意义的。\n所以当业务数据很大，但是数据中的大部分（一般都是80%）可能都是无效数据，那么就可以在join时过滤掉空值uid，没有了这些无效数据，自然就不存在这么大量集中的key，数据倾斜的风险就会消失。\n","date":"2024-09-19T08:54:12+08:00","permalink":"https://rusthx.github.io/p/%E6%95%B0%E4%BB%93hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/","title":"数仓(Hive)数据倾斜产生原因及处理方式"},{"content":"问题 有一张交易流水表（transaction），主键为账号，每个账号有所属公司。有一张公司信息表（company_info），主键为公司id，表中有上级公司id。\n需要得到每个公司的交易信息（资金流入流出余额），但是每个公司的数据都应该是该公司及下属公司的汇总。 但是数据库并不支持树形结构也不支持层级结构。\n方案 主要难题在于公司信息表，公司只有上面一级的信息，没有更上面的信息，也没有下属公司信息。\n所以需要做一张公司树表（company_tree），记录上下关系和层级信息，加工方案如下：\n设置字段为公司id，公司id树（从最顶层公司到当前公司，类似'0011-0022\u0026rsquo;），公司层级,上级公司id(sup_comp_id)。\n插入最顶层公司信息（\u0026lsquo;0011\u0026rsquo;）,设置该公司层级为1级。id树为'0011\u0026rsquo; 然后查询company_info表，插入上级为（\u0026lsquo;0011\u0026rsquo;）的公司信息，设置层级为2级，拼接上级公司id树(\u0026lsquo;0011\u0026rsquo;)和当前公司id作为当前公司的id树。 然后插入三级公司、四级公司，直到最底层公司。不知道到底有多少层可以插入一层后观察company_tree表有无新增数据。\n注意：三级及更下级的公司需要冗余多行，每行的上级公司id不同（分别为顶级到当前公司的父级公司的公司id）\n关联company_tree表和transaction,group by sup_comp_id,再对资金流入流出余额进行sum()聚合。 这里加一个按层级的过滤条件，一层一层给地查，然后再union查询结果即可得到所有公司的信息 观察上面的方案不难发现，这张加工的company_tree不好用，公司信息比较少变（除了股市），一般都是作为数仓的维度表。 但是每次需要关联查询company_tree时写的查询语句都很复杂，那么怎样才能不用union各层公司信息呢？\n设计company_tree的时候再加一个up_comp_tree字段。这样聚合的时候group by up_comp_tree就能拿到所有公司的汇总信息。 （因为公司的下属公司的所有下一级公司的up_comp_tree都是当前公司的id树）\n","date":"2024-09-14T17:49:28+08:00","permalink":"https://rusthx.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0%E6%A0%91%E7%8A%B6%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84/","title":"数据库实现树状（层级）结构"},{"content":"DataX介绍 DataX的Github介绍如下：\nDataX 是阿里云 DataWorks数据集成 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具/平台。DataX 实现了包括 MySQL、Oracle、OceanBase、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、Hologres、DRDS, databend 等各种异构数据源之间高效的数据同步功能。\nDataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\n问题 数仓开发时需要从异构数据源获取数据作为ODS层（原始数据层），同时还需要在数仓间进行ETL(数据抽取、转换、加载)。通过DataX来实现数据同步。\n实现 方案1 shell实现 1.写通过DataX将数据同步到ODS层的配置文件（reader、writter）\n2.通过shell脚本传入传入target_dir(数据库表名，判断是否是需要全量同步的表，若是，则进行全量同步)和datax_config(即步骤1的配置文件)\n3.将数仓间ETL的SQL脚本也封装成shell脚本\n4.通过dolphinscheduler调度步骤2和步骤3的shell脚本实现数据同步到数仓ODS层并在数仓间ETL\n缺点：系统复杂度高，不易维护\n优点：更加灵活，在部分自定义函数功能支持不是很友好的框架（Hive）里也能很好地发挥作用\n方案2 python实现 1.写通过DataX将数据同步到ODS层的配置文件（reader、writter）\n2.将异构数据源配置信息写成文件，然后在python脚本中调用配置信息，拼接成完整的DataX同步命令。\n3.将数仓间ETL的SQL脚本封装成自定义函数，再在步骤2的python脚本中连接数仓执行自定义函数\n4.在服务器上通过contrab设置定时任务实现自动执行\n缺点：Hive的自定义函数支持不是特别友好，部分功能可能没法实现。通用性差\n优点：系统复杂度低，DataX和python脚本都不用一直在线，定时调度任务开始执行时才会启动python脚本，降低了服务器负担\n补充：DataX的增量同步 数仓的增量同步一般都是用消息队列+数据库监听框架+数据同步框架（kafka+Maxwell+Flume）。 但是在离线数仓实时性要求不高的场景下也可以用DataX来实现增量同步。 实现原理：配置文件中reader部分支持column和querySQL,可以在querySQL中加入过滤条件来获取较新的数据。\n比如如果需要获取最新的每日交易数据，就可以加一个时间为最新日期或者传入日期参数的过滤条件。\n如果表中没有时间这种自然增长的字段也可以使用单调递增的id之类的。总之就是获取最新的一批数据就可以。\n然后为了保证数据一致性，还需要在wrritter的preSQL中删除符合reader的querySQL的数据。\n补充：crontab命令 参考资料：https://cloud.tencent.com/developer/article/2359335\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 （1）语　法： crontab [-u \u0026lt;用户名称\u0026gt;][配置文件] 或 crontab { -l | -r | -e } -u #\u0026lt;用户名称\u0026gt; 是指设定指定\u0026lt;用户名称\u0026gt;的定时任务，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设定自己的定时任务。 -l #列出该用户的定时任务设置。 -r #删除该用户的定时任务设置。 -e #编辑该用户的定时任务设置。 （2）命令时间格式 : * * *　*　*　command 分　时　日　月　周　命令 第1列表示分钟1～59 每分钟用*或者 */1表示 第2列表示小时1～23（0表示0点） 第3列表示日期1～31 第4列表示月份1～12 第5列标识号星期0～6（0表示星期天） 第6列要运行的命令 （3）一些Crontab定时任务例子： 30 21 * * * /usr/local/etc/rc.d/lighttpd restart #每晚的21:30 重启apache 45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart #每月1、10、22日的4 : 45重启apache 10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart #每周六、周日的1 : 10重启apache 0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart #每天18 : 00至23 : 00之间每隔30分钟重启apache 0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart #每星期六的11 : 00 pm重启apache * 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart #晚上11点到早上7点之间，每隔一小时重启apache * */1 * * * /usr/local/etc/rc.d/lighttpd restart #每一小时重启apache 0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart #每月的4号与每周一到周三的11点重启apache 0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart #一月一号的4点重启apache */30 * * * * /usr/sbin/ntpdate cn.pool.ntp.org #每半小时同步一下时间 0 */2 * * * /sbin/service httpd restart #每两个小时重启一次apache 50 7 * * * /sbin/service sshd start #每天7：50开启ssh服务 50 22 * * * /sbin/service sshd stop #每天22：50关闭ssh服务 0 0 1,15 * * fsck /home #每月1号和15号检查/home 磁盘 1 * * * * /home/bruce/backup #每小时的第一分执行 /home/bruce/backup这个文件 00 03 * * 1-5 find /home \u0026#34;*.xxx\u0026#34; -mtime +4 -exec rm {} \\; #每周一至周五3点钟，在目录/home中，查找文件名为*.xxx的文件，并删除4天前的文件。 30 6 */10 * * ls #每月的1、11、21、31日是的6：30执行一次ls命令 ","date":"2024-09-14T16:58:47+08:00","permalink":"https://rusthx.github.io/p/%E9%80%9A%E8%BF%87datax%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE/","title":"通过DataX同步数据仓库数据"},{"content":"分类 join有如下种类\n(inner) join\nleft (outer) join\nright (outer) join\ncross join :笛卡尔积，与inner join不指定on等效\nstraight_join :效果等同于inner join，只是指定左表为驱动表\nfull (outer) join :全外连接，MySQL中不支持。可用left join union right join 实现。\n例如：\n1 2 3 select t1.* from t1 full join t2 on t1.id =t2.id 在MySQL中可以用下列方式实现：\n1 2 3 4 5 6 7 select t1.* from t1 left join t2 on t1.id =t2.id union select t1.* from t1 right join t2 on t1.id =t2.id 驱动表与被驱动表 inner join :由驱动器决定\nleft join :左表为驱动表，右表为被驱动表\nright join :左表为驱动表，右表为被驱动表\nstraight_join :固定左表为驱动表，右表为被驱动表\njoin执行流程 每次取驱动表一行数据，去和被驱动表匹配，即双重for循环\njoin执行的实现方式 Nest Loop Join(NLJ):单纯的双层循环\nBlock Nest Loop Join(BNLJ):在NLJ的基础上，利用Join Buffer，一次取出一批驱动表数据，可以减少循环匹配次数\nIndex Nest Loop Join(INLJ):在NLJ的基础上，利用被驱动表连接字段的索引直接找到匹配数据，可以减少循环次数\n小表驱动大表 参考资料：https://www.bilibili.com/video/BV1ms4y177mr/?spm_id_from=333.337.search-card.all.click\u0026amp;vd_source=2db7c64d895a2907954a5b8725db55d5\n小表驱动大表是一种常见的SQL优化手段，其原因如下： 两表关联时会产生一个Join Buffer(关联缓存区)。Join Buffer是优化器用于处理连接查询操作时的临时缓冲区，简单来说当需要比较两个或多个表的数据进行Join操作时，JOin Buffer可以帮助MySQL临时存储结果，以减少磁盘读取和CPU负担，提高查询效率。需要注意的是每个join都有一个单独的缓冲区。\nBNLJ会将驱动表数据加载到Join Buffer里，然后再批量与被驱动表进行匹配，如果驱动表数据流量较大，Join Buffer无法一次性装载驱动表的结果集，将会分阶段与被驱动表进行批量数据匹配，然后记录结果并将结果返回。如果数据量过大，Join Buffer无法一次性加载完成就会分阶段匹配，增大了磁盘读取，降低了效率\n所以总结如下：\n小表可以被完全加载到内存（Join Buffer）中： 小表的数据量相对较少，可以被完整加载到内存中，减少了磁盘IO的开销。而大表的数据量较大，可能无法完全加载到内存，需要进行磁盘IO操作，会导致性能下降。\n减少了数据传输量： 将小表作为驱动表可以先获取小表的结果集，再根据小表的结果集进行大表的关联查询。这样可以减少传输到被驱动表的数据量，减少网络传输的开销。\n利用索引优化(INLJ)： MySQL的查询优化器通常会选择使用索引来优化关联查询。将小表作为驱动表可以更好地利用索引，因为小表的索引更容易被缓存并快速定位。\njoin on on后跟连接条件，一般必须指定，且只对被驱动表有效，即使对驱动表加了过滤条件，该条件也无效。\n所以，在join on之后，驱动表包含全部数据，被驱动表只包含on条件过滤后的数据。\n注：inner join 后的数据只会是下面两个椭圆的交集\non和where on 在join时就会过滤数据，而where是join完成后再对数据进行过滤，所以on比where先作用。\n所以，理论上过滤条件放在on后比放在where后性能更好，因为这样可以有更少的数据进入磁盘IO。\n但是，由于on后的条件只对被驱动表有效，过滤条件放在on后和where后的结果可能会不一致，所以谨慎在on后加驱动表的过滤条件。\n对于inner join，on和where就没有区别了\n多表关联查询优化 加过滤条件要想清楚，先对被驱动表过滤还是join完后再一起过滤\n尽量小表驱动大表，这是针对left join和right join的情况，inner join会由优化器自行选择\nexplain分析SQL语句得到的执行计划的第一行即是驱动表\n优化join思路：一切为了减少join 时驱动表匹配被驱动表时的循环次数。如果join后的数据量很大，并且还要进行聚合操作，在不影响查询结果的情况下可以考虑先聚合出临时表再进行join\n减少单表数据量，如水平分表、垂直分表\n静态的数据可以在后端进行缓存\n补充：分表设计 分表原理：一个大表按照一定的规则分解成多张具有独立存储空间的实体表。这些表可以分布在同一块磁盘上，也可以在不同的磁盘上。\n通过分表实现用户在访问数据时，因不同的条件而访问不同的表，将数据分散在各个实体表中，减少单表的访问压力，提升数据查询效率\n水平分表 以字段为依据，按照一定的策略，使用hash、range、list等方式将一个表的数据拆分成多个相同结构的表中。 水平分表是为了降低单表的数据量，解决单表的热点问题。\n比如按时间特性进行划分，将表数据分成历年数据表或者历史数据表（已完成）+在线数据表（正在进行）。 水平分表后的表通过union能还原回原来的表。 垂直分表 根据字段查询频率将表中数据拆分为不同结构的表（主表和扩展表）。 例如将热点数据和非热点数据分块存储，这样在查询热点数据时就能将数据缓存起来，减少了随机读取IO，提高了命中率。 适用于由于字段较多引起数据量和访问量较大的情况，且每个业务场景只访问部分字段。\n例如：用户对商品感兴趣才会查看详细描述，而详细描述占用存储较多（Text）,可以将该字段垂直分割 垂直分表后的表通过join可以还原回原来的表\n垂直分表的优点：\n不同的业务场景访问不同的内容，数据量小，提升性能 集成中数据传输量小 不同业务场景业务量访问频率不一样，表的操作更新可以更加灵活地控制 降低业务耦合度 垂直分割可使行数据变小，一个数据块就能存更多数据，在查询时可以有效减少IO次数。垂直分表可以有效利用Cache 分区分表对比 定义：分区是在一张表中，根据某种规则将数据分散到不同的物理存储区域。分表则是将一张大表拆分成多张小表。 数据访问：在分区中，用户无需知道数据在哪个分区，可以像访问普通的表一样访问数据，但在分表中，用户必须先知道数据在哪张表中才能访问到所需数据 适用场景：分区适用于数据量大，但查询范围有限的场景，而分表适用于数据量大，查询范围广的场景。 性能：分区可以提高查询性能，因为查询只需要在一个分区内进行（过滤条件使用了分区字段），而不是在整张表中。分表可以提高整体性能，因为每个表的数据量都变少了 管理：分区可以减少数据的恢复。分表可以使每个表的大小更容易得到控制 广播表与分布式表 广播表：小表广播功能能提高跨库场景的性能和简化跨库场景的开发。 将需要广播的数据推送到目标库，冗余了表数据，方便在库内关联查询。\n1 2 3 4 5 create table config( id int primary key, config_key varchar(255), config_value varchar(255) )broadcast; 分布式表：分布式表是指其数据根据某种分片策略在分布式数据库系统的不同节点上。 数据被分成多个分段，每个分段存储在不同的节点上。 分布式表的分布策略可以基于hash、range、list等方式。 分布式表适用于数据量大且需要水平扩展的场景.\n1 2 3 4 5 create table users( user_id int primary key, uname varchar(255), email varchar(255) )distributed by hash(user_id); 反范式 属性冗余：在一个表中除了存储关联表的主键外，将关联表的非键字段也存储到此表的处理方式。 有点像垂直分表的反向操作\n1 2 3 (user_id check_date score),(user_id user_name telephone) -\u0026gt; (user_id check_date score user_name telephone) 级联属性冗余：多表关联时，A关联B,B关联C。在查询时需要同时获取A、B、C三个表的属性或以它们的属性进行条件过滤 ，为了减少表关联以提高性能，可以考虑在B表中冗余需要访问的C表字段，减少频繁的表关联操作。\n例如：在员工表中冗余部门表的信息（部门id 部门名称 部门负责人）\n表冗余：针对数据记录进行冗余，即A表的数据复制多份，或者多表关联的结果数据存储成一张表。\n直接复制：“广播表模式”，可以提高跨库访问的性能 加工派生：冗余的数据是源表加工后的数据或多表关联的结果 ","date":"2024-09-12T23:32:36+08:00","permalink":"https://rusthx.github.io/p/mysql-join%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BC%98%E5%8C%96/","title":"MySQL Join介绍与优化"},{"content":"我写的这篇博客只是我学习的一点总结，并不系统，也不全面。想要学习较为全面的知识可以看 小林coding的图解MySQL相关部分\n密集列索引 密集列，是指一张表上列的数据没有离散度，列的取值范围较小，高度集中在少数几个值中（如性别）。\n一般来说，密集列由于没有离散度，不适合作为索引列。使用这类密集索引的实际效率可能会低于全表扫描。但是，假如密集列的倾斜度很高时，例如有一个状态列表示是否停用，大部分对象都为停用，那么就可用使用密集列作为索引 需要保证要查询的数据分布较少，（低于总数据量的5%）。同时还要确定是否有为这一个查询单独优化的需要，因为索引也会占用空间，建太多索引反而会降低查询效率，甚至会出现加了索引，查询效率反而变慢的情况。优化并不是将每个查询都单独优化到1s内，而是衡量损失后在妥协中得到一个平衡，让慢查询只占很少比例，优先保证查询次数多的语句。 可能会频繁变更的列不宜作为索引列，因为索引列变更会导致索引重排，也即是B+树的树结构变更。这会导致修改效率大幅下降 索引合并(index_merge) 当数据过滤条件分布在多个索引列上时（无论是and还是or），MySQL为了提升数据访问效率会使用多个索引合并的方式过滤数据。、\n索引合并会导致单个索引过滤的数据量越大，查询效率下降越明显\n锁 MySQL的行锁是通过对索引加锁来实现的。数据修改时会根据过滤条件匹配对应的索引，在对应的索引上加锁。如果语句中修改了索引的值，就还会在修改后的值上加锁。其他事务使用相同的索引值修改数据不管是否是相同的行均会被阻塞。\n多线程下应该使用主键来修改数据降低阻塞的概率。\n如果update/delete的where条件没有使用索引或者没有where条件，就会全表扫描，会对所有记录加上next-key锁（record锁+gap锁）,相当于把整张表锁住了\n","date":"2024-09-12T23:05:47+08:00","permalink":"https://rusthx.github.io/p/mysql-%E7%B4%A2%E5%BC%95/","title":"MySQL 索引"},{"content":"删除 删除未在他表出现的数据 下面有一条效率较差的删除语句，主要功能是将t1表中id未出现在t2表的记录删除。效率差的原因是in中用了子查询，导致删除语句不会走索引，从而导致锁全表，继而导致删除效率差。\n1 2 3 4 5 delete from t1 where id not in ( select id from t2 ) 优化方法:取消子查询，改用关联删除，这样就可以使用建在id列上的索引。关联删除/更新在建立索引的情况下效率远高于in/exists\n优化后的SQL语句如下：\n1 2 3 delete t1 from t1 left join t2 on t1.id=t2.id where t2.id is null 注：上面这条是MySQL独有的优化，达梦数据库和Oracle中可用如下语句。（拾人牙慧，未经验证，用这两个数据库的朋友可以自行验证一下）(+)表示单侧关联，该符号在哪边哪边就是副表。\n1 2 3 4 5 6 delete from t1 where rowid in( select t1.rowid from t1,t2 where t2.id(+)=t1.id and t2.id is null ) 删除多表相同数据 好的删除语句如下：\n1 2 3 delete t1,t2 from t1,t2 where t1.id=t2.id 这样可以同时删除从主从表删除。比如一条记录记录在多张表中，删除这条记录需要同时删除两张表的记录。这样可以保证要么全部删除，要么全部不删，间接满足了事务一致性。（数据库只会从一个状态转移至另外一个状态，即拥有这条记录和没有这条记录的两个状态。这一条记录可以看成是一个入库记录、一张支票）\n删除全表数据 用truncate替代delete。这里涉及delete的机制，delete并不是直接在磁盘中删除记录，而是将记录加一个标记，并设置为不可见，然后在数据库压力小时异步删除磁盘中的数据。但是这样有一个问题，虽然标记为删除后，查询表记录不可见。但是记录仍然占有着磁盘空间，这会拖慢查询数据库的速度。\n1 truncate table t1; 更新 差的更新语句如下：\n1 2 3 update t1 set c1=\u0026#39;\u0026#39; where id in (...) 如删除篇中所说，由于in中属性过多，in不会再走索引（当属性值大于4个后就不会再走索引，in中是子查询的话就不会走索引）。所以这里的更新效率慢，并且还会锁住整表。\n关于为什么全表扫描会锁住整张表可以看小林的教程：update 没加索引会锁全表？\n优化方式：\n创建id临时表，临时表只有id一个字段 批量插入临时表，记录为上面差的更新语句中的记录，也即是需要更新的记录的id 将临时表于原表关联更新 补充：插入更新 插入一条数据，如果存在主键或唯一键冲突，则更新记录\n注：使用此语句时，必须在表中定义主键或唯一约束\n1 2 3 4 5 insert into [table_name] (column1,column2,column3...) values (values1,values2,values3...) on duplicate key update column1 = values(column1), column2 = values(column2),... 例如：\n1 2 3 4 insert into users (id,`name`) values (1,\u0026#39;Alice\u0026#39;) on duplicate key update name = values(`name`) PostgreSQL中用法如下：\n1 2 3 4 insert into users (id,`name`) values (1,\u0026#39;Alice\u0026#39;) on conflicate (id) do update set name = excluded.name 如果想同时修改多个字段也可用下面的写法\n1 2 3 4 insert into users (id,`name`，age) values (1,\u0026#39;Alice\u0026#39;,18) on conflicate (id) do update set (`name`,age) = excluded.(`name`,age) 如果想遇见冲突主键不做处理可用如下语句\n1 2 3 4 insert into users (id,`name`) values (1,\u0026#39;Alice\u0026#39;) on conflicate (id) do nothing ","date":"2024-09-12T22:15:33+08:00","permalink":"https://rusthx.github.io/p/mysql%E5%88%A0%E9%99%A4%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96/","title":"MySQL删除修改数据优化"},{"content":"题目 给定一个整数数组，进行随机交换，要求交换后的数组中每个元素都不在其原来的位置上\n思路 1.遍历数组，随机一个不为当前下标的下标，将两个位置的元素交换位置\n2.可能会出现某个元素交换多次后又回到原位置的情况，所有需要再遍历一遍数组，如果出现此情况就将交换过的数组再递归交换，直到没有这种情况\n答案 注意：数组在交换前需要先拷贝一份，这样才能验证每个元素是否在原位置上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public static int[] randomSwap(int[] nums){ int[] arr = nums.clone(); Random random = new Random(); for (int i = 0; i \u0026lt; nums.length; i++) { int j=i; while(j==i){ j=random.nextInt(nums.length-1); } int temp =arr[j]; arr[j]=arr[i]; arr[i]=temp; } for (int i = 0; i \u0026lt; nums.length; i++) { if (arr[i] == nums[i]) { return randomSwap(nums); } } return arr; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import random def derange_array(array): n = len(array) result = array[:] for i in range(n): j = i while j == i: j = random.randint(0, n - 1) # 交换元素 result[i], result[j] = result[j], result[i] # 确保没有元素在其原来的位置上 for i in range(n): if result[i] == array[i]: return derange_array(array) # 如果有元素在原来位置上就递归交换 return result # 验证测试 array = [1, 2, 3, 4, 5] result = derange_array(array) print(result) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import scala.util.Random object Derangement { def main(args: Array[String]): Unit = { val array = Array(1, 2, 3, 4, 5) val result = derangeArray(array) println(result.mkString(\u0026#34;, \u0026#34;)) } def derangeArray(array: Array[Int]): Array[Int] = { val n = array.length val result = array.clone() val rand = new Random() for (i \u0026lt;- 0 until n) { var j = i while (j == i) { j = rand.nextInt(n) } // 交换元素 val temp = result(i) result(i) = result(j) result(j) = temp } // 确保没有元素在其原来的位置上 for (i \u0026lt;- 0 until n) { if (result(i) == array(i)) { return derangeArray(array) // 如果有元素在原来位置上就递归交换 } } result } } ","date":"2024-09-11T14:43:00+08:00","permalink":"https://rusthx.github.io/p/java%E9%9A%8F%E6%9C%BA%E4%BA%A4%E6%8D%A2%E6%B4%97%E7%89%8C/","title":"Java随机交换(洗牌)"},{"content":" 参考资料：《SQL进阶》P106 (鹿书)\n关系（表）结构 现有一张住宿表(stay_people)如下\nguest(入住客人) start_date(入住时间) end_date(退房时间) 阿良良木历 2006-10-26 2006-10-27 阿良良木月火 2006-10-28 2006-10-31 阿良良木火怜 2006-10-31 2006-11-01 忍野忍 2006-10-29 2006-11-01 忍野扇 2006-10-28 2006-11-02 战场原黑仪 2006-10-28 2006-10-30 千石抚子 2006-10-30 2006-11-02 问题：判断这些客人住店时间存在重叠，如果存在重叠，则展示客人的名字、入住时间和退房时间\n问题分析 很明显这道题的重点是判断两个时间段是否相交，那么时间相交有如下三种情况：\n答案 1.自关联然后判断是否为三种情况之一，如果符合一种，那么时间相交\n1 2 3 4 5 6 7 select t1.guest ,t1.start_date ,t2.start_date from stay_people t1,stay_people t2 where (t1.start_date\u0026lt;=t2.end_date and t1.start_date\u0026gt;=t2.start_date) or (t1.end_date\u0026gt;=t2.start_date and t1.start_date\u0026lt;=t2.start_date) or (t1.start_date\u0026gt;=t2.start_date and t1.end_date\u0026lt;=t2.end_date) 2.比较自关联后一行的最小的end_date和最大的start_date来判断两个时间段是否相交。\n1 2 3 4 5 select t1.guest ,t1.start_date ,t2.start_date from stay_people t1,stay_people t2 where greatest(t1.start_date,t2.start_date)\u0026lt;=least(t1.end_date,t2.end_date) 3 使用数据库的内置函数判断时间段是否相交\n1 2 3 4 5 select t1.guest ,t1.start_date ,t2.start_date from stay_people t1,stay_people t2 where (t1.start_date,t1.end_date) overlaps (t2.start_date,t2.end_date) 但是，这种判断默认的时间段是左闭右开的，即认为住宿时间为[start_date,end_date),并且这个函数只有SQL Server、PostgreSQL、Oracle支持，MySQL并不支持这种写法。未列举的数据库不一定不支持，可以查一下相关文档\nPostgreSQL时间函数文档\n我写的只是三种类型的处理方法，除了我的写法，还有许多别的写法，我只是做一个简单总结\n","date":"2024-09-08T10:15:20+08:00","permalink":"https://rusthx.github.io/p/sql%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E4%BA%A4%E9%9B%86/","title":"SQL计算时间交集"},{"content":"这里以老师的指导文档为例，实现一个基于flask和redis的web网页，用户每输入网址浏览一次就加一次浏览量并显示在网页上。\nDocker拉取镜像 拉取Python镜像\n1 sudo docker pull python 拉取redis镜像\n1 sudo docker pull redis 查看镜像\n1 docker images 编辑所需文件 编辑Python程序，实现一个基于flask的web应用，在redis中存一个数值，初值为0，每访问一次网站，计数加1。 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import time import redis from flask import Flask app = Flask(__name__) cache = redis.Redis(host=\u0026#39;127.0.0.1\u0026#39;, port=6379) def get_hit_count(): retries = 5 while True: try: return cache.incr(\u0026#39;hits\u0026#39;) except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5) @app.route(\u0026#39;/\u0026#39;) def hello(): count = get_hit_count() return \u0026#39;Hello rust! Hello hx! I have been seen {} times.\\n\u0026#39;.format(count) 编写dockerfile\n1 2 3 4 5 6 7 8 FROM python:latest WORKDIR /code ENV FLASK_APP hxapp.py ENV FLASK_RUN_HOST 0.0.0.0 RUN pip install redis flask -i https://mirror.baidu.com/pypi/simple COPY hxapp.py hxapp.py EXPOSE 5000 CMD [\u0026#34;flask\u0026#34;,\u0026#34;run\u0026#34;] 豆瓣源下redis和flask会报错如下，根据Windows下载这两个包的经验，猜测应该是镜像源的问题，换成百度源，解决问题。 生成新的镜像 1 sudo docker build -t onccn/myflask:v2 -f ./dockerfile . 从kubernetes拉取docker本地镜像 编辑flaskredisdeploy.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: apps/v1 kind: Deployment metadata: name: flaskredis spec: selector: matchLabels: app: flaskredis template: metadata: labels: app: flaskredis spec: containers: - name: flaskredis image: onccn/myflask:v2 imagePullPolicy: Never resources: limits: memory: \u0026#34;1500Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; - name: redis image: redis:latest imagePullPolicy: Never resources: limits: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 1 kubectl apply -f flaskredisdeploy.yaml 访问Web和Windows访问 虚拟器中浏览器输入上面查看日志得到的链接 编辑service.yaml\n1 gedit myflaskservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: myflaskservice spec: selector: app: flaskredis ports: - protocol: TCP port: 8080 #Service的端口号 targetPort: 5000 #容器暴露的真实端口号 nodePort: 30081 #node的真实端口号 type: NodePort 创建service\n1 kubectl apply -f myflaskservice.yaml 遇见报错,这个报错的意思是该端口已经被占用，修改端口号 成功创建service,查看service\n1 kubectl get svc -o wide Windows访问k8s集群的service，进而访问相应的pod ip地址为node的地址（k8s集群中应用service的节点） ","date":"2024-09-07T12:50:40+08:00","permalink":"https://rusthx.github.io/p/kubernetesk8spod%E9%83%A8%E7%BD%B2/","title":"Kubernetes（K8s）Pod部署"},{"content":" 参考资料：https://www.youtube.com/watch?v=3mdCiFu52XA\u0026amp;t=8s\nvscode安装k8s插件 值得一提的是安装k8s插件后编辑k8s所需yaml文件会非常简单。比如需要一个deploy，那只需要输入deploy再按一下TAB键就可以使用自动补全，得到一份模板代码。同时这个插件还能检查k8s语法，比如container是否限制资源。 在yaml文件中加入参数imagePullPolicy: Never，这个参数的意思是禁用docker注册表和docker hub，从本地的docker拉取镜像。 ","date":"2024-09-07T12:48:13+08:00","permalink":"https://rusthx.github.io/p/kubernetesk8s%E4%BD%BF%E7%94%A8%E6%9C%AC%E5%9C%B0docker%E9%95%9C%E5%83%8F/","title":"Kubernetes(K8s)使用本地Docker镜像"},{"content":" 参考资料:https://cloud.tencent.com/developer/article/2347138\n禁用Ubuntu Swap 1 sudo gedit /etc/fatab Ubuntu安装网桥工具 1 sudo apt-get install bridge-utils -y 添加k8s镜像源 1 2 3 4 5 sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026#34;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\u0026#34; sudo curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - sudo add-apt-repository \u0026#34;deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\u0026#34; 配置containerd 这里需要安装containerd，但是我在docker安装的时候已经装好了，相关命令为\n1 sudo apt-get install containerd.io （1）Containerd安装完成后，其自带的配置文件/etc/containerd/config.toml中的内容，需要用打印出的containerd默认配置替换。 （2）Containerd的Cgroup设为systemd，以和k8s默认的Cgroup保持一致。 （3）pause镜像路径改为国内源registry.aliyuncs.com/google_containers/pause:3.9。\n1 2 3 4 5 sudo cp /etc/containerd/config.toml /etc/containerd/config.toml.ori sudo chmod 777 /etc/containerd/config.toml sudo containerd config default \u0026gt; /etc/containerd/config.toml 1 sudo gedit /etc/containerd/config.toml 配置后，重启containerd服务，并保证containerd状态正确\n1 2 sudo systemctl restart containerd.service sudo systemctl status containerd.service 安装Kubernetes 安装k8s软件，这里会默认下载最新的kubernetes（阿里云镜像源上的），后面指定版本时需要根据自己的版本进行修改。这里也可以手动指定kubernetes版本。\n1 sudo apt install kubelet kubeadm kubectl 此时k8s没有启动成功是正常的，因为kubelet服务成功启动的先决条件，需要kubelet的配置文件，所在目录/var/lib/kubelet还没有建立。\nk8s配置单机节点 查看k8s版本\n1 kubeadm config images list 将kubernetes的控制面的几个镜像拉到本地，为了保证镜像和安装的k8s软件版本严格一致，这里的镜像拉取时，显性指定版本。\n1 sudo kubeadm config images pull --kubernetes-version v1.28.10 --image-repository registry.aliyuncs.com/google_containers kubernetes初始化 1 sudo kubeadm init --control-plane-endpoint=192.168.146.111 --kubernetes-version v1.28.10 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers 初始化成功如下图 克隆虚拟机作为从节点 关闭虚拟机，克隆虚拟机作为从节点。 右键虚拟机，克隆虚拟机。选择当前状态，创建完整克隆，完成克隆。 克隆完成后修改从节点的hosts、hostname和ip。 为了便于管理，修改主节点的hosts为k8s1,从节点分别取名k8s2和k8s3。对应ip指定为192.168.146.112和192.168.146.113。（这里的ip和host要根据自己的情况进行修改，主节点的名字也可以通过修改hotsname来改变） 创建hosts映射。 配置Kubernetes集群 在主节点上跟着初始化成功后的提示继续后续工作\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看集群状态\n1 2 kubectl cluster-info kubectl get nodes 在k8s1上创建永久token\n1 kubeadm token create --print-join-command 如果在克隆虚拟机作为从节点之前就已经跟着k8s提示完成了上面几步，那么在从节点上执行提示的join命令，会遇见报错。 原因：拷贝虚拟机时已完成k8s集群配置文件创建，应该在初始化后，配置文件创建前进行克隆。 解决办法：在虚拟机重置节点后加入集群。这个重置节点的命令也可以在后续k8s出现问题时恢复默认设置重做时使用。\n1 2 sudo rm -rf /etc/kubernetes/kubelet.conf /etc/kubernetes/pki/ca.crt sudo kubeadm reset 重新加入集群（join命令为主节点创建永久token时回显的命令） 在k8s3上进行相同步骤。 在k8s1上查看集群节点，集群搭建成功。 细心的读者可能发现了我的1号机之前交VirtualClass，但是现在却叫k8s1，这是因为我在1号机初始化之前没有修改一号机的名字，导致集群建立后节点名字如下，但是这个看着不太舒服，我就把三台机都使用上面的重置命令重置后再重做了一遍上述步骤。 配置Kubernetes网络插件Calico 1 2 curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O kubectl apply -f calico.yaml 成功截图如下（我这里因为网络问题没有成功，姑且偷一张图），如果失败可以使用另外一种手动方法如下（我的办法）。 手动解决办法： 打开链接(可能需要借助某些上网工具) https://projectcalico.docs.tigera.io/manifests/calico.yaml 全选复制，粘贴到~/calico.yaml 重新执行\n1 kubectl apply -f calico.yaml 成功截图，这里要看见所有的pod状态都为Running才是成功，如果是ContainerCreating则表明正在制作容器，需要稍等一会（可能会很慢，但也不会超过十分钟）。如果过了很久还没Running，可以通过kubectl describe pods \u0026lt;pod-name\u0026gt; -n kube-system 查看pod进度。 -n的意思是指定命名空间（namespace），默认的pod命名空间是default，所以使用kubectl get pods会得到defalut下的pods。\n1 2 kubectl get pods -n kube-system kubectl get nodes k8s命名空间查看，更多命令及参数可以通过kubectl --help查看\n1 kubectl get namespace ","date":"2024-09-07T12:35:29+08:00","permalink":"https://rusthx.github.io/p/kubernetesk8s%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/","title":"Kubernetes（K8s）集群安装"},{"content":"我的docker是安装在Ubuntu22.04虚拟机上的，不同的操作系统细微差别，请自行必应搜索。\nDocker安装 更新apt数据源 1 sudo apt-get update 下载依赖 1 sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common -y 添加Docker的官方GPG密钥 从这里开始可以选择docker官方的密钥和仓库，也可以选择国内镜像（阿里云）\n1 2 3 4 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg # 阿里云密钥 curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 设置稳定仓库 1 2 3 4 sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; # 阿里云仓库 sudo add-apt-repository \u0026#34;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\u0026#34; 安装docker 1 sudo apt-get install docker-ce docker-ce-cli containerd.io 添加docker用户组，将登陆用户加入到docker用户组中，更新用户组 1 2 3 sudo groupadd docker sudo gpasswd -a $USER docker newgrp docker docker测试 1 sudo docker run hello-world 配置加速镜像和Cgroup，后面再docker使用过程中遇见了拉取镜像缓慢的问题，于是我又多加了几个镜像源。不过事后想想，可能是那几天校园网太卡的问题 1 sudo gedit /etc/docker/daemon.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://kfwkfulq.mirror.aliyuncs.com\u0026#34; ], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } Docker简单使用 拉取nginx和tomcat的镜像 1 2 sudo docker pull nginx sudo docker pull tomee 1 sudo docker network create testnet 启动 两个tomcat 创建两个目录，分别挂载到tomcat的跟目录上，内容可以调整，主要区分是哪个服务上的文件。\n1 2 3 4 5 6 7 cd ~ sudo mkdir tomcatone echo \u0026#34;tomcat onet\u0026#34; \u0026gt; index.html sudo mkdir tomcattwo sudo cp index.html tomcatone/ echo \u0026#34;tomcat two\u0026#34; \u0026gt; index.html sudo cp index.html tomcattwo 运行容器\n1 sudo docker run -id --name tomcatone -p 8088:8080 --network testnet --network-alias tomcatone -v $PWD/tomcatone:/usr/local/tomee/webapps/a tomee 查看网页显示如下 启动nginx，命令使用两次 1 sudo docker run -it -d -p 8080:80 --name web -v ~/nginx:/etc/nginx/conf.d -v ~/nginxweb:/usr/share/nginx/html --network testnet --network-alias nginxs nginx 刷新后网页显示如下 ","date":"2024-09-07T12:27:13+08:00","permalink":"https://rusthx.github.io/p/docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"Docker安装与基本操作"},{"content":" 参考：https://www.bilibili.com/video/BV11A411L7CK?p=188\u0026amp;vd_source=2db7c64d895a2907954a5b8725db55d5\n终端打印大量日志影响结果查看可以看我首页的博客解决。 踩坑如下： 1.socket编程不会，写socket发送数据查了很多资料才写出来 2.Windows没有netcat命令，但是MACOS和Ubuntu有,所以理所当然的想到用虚拟机的端口来收集数据和输入数据，事实上这个想法确实没有问题，分别做的话是能正常实现的，但是这也为后续的错误埋下了大坑。想当然的把socket当成kafka用（producer和consumer），是我踩坑的一大原因。 3.被教程误导，socket发送数据到端口，但是不知道socket有服务器和客户端之分，发送数据和处理数据的都是客户端，导致发送端可以和nc -lk 结合使用，能正常监听到数据；接收端也能和nc -lk 结合使用，在监听的端口出输入数据可以正常计算；但是两者结合就没办法计算了\nWindows安装netcat 下载链接： https://nmap.org/download.html#windows 下载的是一个exe包，点击exe包一路next即可完成安装。\n端口同时接收数据和计算数据时使用命令监听会无法访问，即启动socket数据发送程序和SparkStreaming数据计算程序后无法监听。但是监听命令可以用来分别调试两个程序。\n注意：Windows的netcat命令与Ubuntu和MacOS都不一样。Windows的命令是 ncat -lk \u0026lt;Port\u0026gt;，参数的意思可以通过ncat -h查看。 SparkStreaming socket编程 题目：1）写一个应用程序利用套接字每隔2秒生成20条大学主页用户访问日志（可以自定义内容），数据形式如下：“系统时间戳，位置城市，用户ID+姓名，访问大学主页”。其中城市自定义 9个，用户ID 10个，大学主页 7个。 2）写第二个程序每隔2秒不断获取套接字产生的数据，并将词频统计结果打印出来。 导入依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-streaming_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package SparkStreaming1 import java.io.{BufferedWriter, IOException, OutputStreamWriter} import java.net.ServerSocket import scala.util.Random object task11 { def main(args: Array[String]): Unit = { // 定义城市和用户ID val cities = Seq(\u0026#34;杭州\u0026#34;, \u0026#34;南京\u0026#34;, \u0026#34;长沙\u0026#34;, \u0026#34;天津\u0026#34;, \u0026#34;北京\u0026#34;, \u0026#34;上海\u0026#34;, \u0026#34;成都\u0026#34;, \u0026#34;广州\u0026#34;, \u0026#34;深圳\u0026#34;) val userIds = Seq(\u0026#34;0:阿良良木历\u0026#34;, \u0026#34;1:忍野忍\u0026#34;, \u0026#34;2:战场原黑仪\u0026#34;, \u0026#34;3:羽川翼\u0026#34;, \u0026#34;4:八九寺真宵\u0026#34;, \u0026#34;5:神原骏河\u0026#34;, \u0026#34;6:千石抚子\u0026#34;, \u0026#34;7:阿良良木火怜\u0026#34;, \u0026#34;8:阿良良木月火\u0026#34;, \u0026#34;9:姬丝秀忒·雅赛劳拉莉昂·刃下心\u0026#34;) val universityUrls = Seq(\u0026#34;www.nju.edu.cn\u0026#34;, \u0026#34;www.ustc.edu.cn\u0026#34;, \u0026#34;www.zju.edu.cn\u0026#34;, \u0026#34;www.fudan.edu.cn\u0026#34;, \u0026#34;www.tsinghua.edu.cn\u0026#34;, \u0026#34;www.pku.edu.cn\u0026#34;, \u0026#34;www.scu.edu.cn\u0026#34;) try { // 创建一个 socket 连接 // val socket = new Socket(\u0026#34;hadoop3\u0026#34;, 9765) val socketServer = new ServerSocket(9765) val client = socketServer.accept() println(\u0026#34;连接！\u0026#34;) val out = new BufferedWriter(new OutputStreamWriter(client.getOutputStream)) //val in = new BufferedReader(new InputStreamReader(client.getInputStream)) while (true){ for (_ \u0026lt;- 1 to 20) { // 发送多条数据 val currentTime = System.currentTimeMillis() val city = cities(Random.nextInt(cities.length)) val userId = userIds(Random.nextInt(userIds.length)) val universityUrl = universityUrls(Random.nextInt(universityUrls.length)) val logLine = s\u0026#34;$currentTime $city $userId $universityUrl\u0026#34; out.write(logLine + \u0026#34;\\n\u0026#34;) // 添加换行符以区分消息 out.flush() // 确保数据被发送出去 println(logLine) } Thread.sleep(2000) // 休眠2秒，模拟连续发送 } // 关闭 socket out.close() // socket.close() client.close() } catch { case e: IOException =\u0026gt; e.printStackTrace() } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package SparkStreaming1 import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.apache.spark.streaming.{Seconds, StreamingContext} object task12{ def main(args: Array[String]): Unit = { val sparkConf: SparkConf = new SparkConf().setMaster(\u0026#34;local[*]\u0026#34;).setAppName(\u0026#34;job7task12\u0026#34;) val spark = SparkSession.builder().config(sparkConf).getOrCreate() val ssc = new StreamingContext(spark.sparkContext, Seconds(2)) // 从套接字获取数据流 val lines = ssc.socketTextStream(\u0026#34;localhost\u0026#34;, 9765) lines.map(_.split(\u0026#34; \u0026#34;)(2)) .map((_, 1)) .reduceByKey(_ + _) .print() ssc.start() ssc.awaitTermination() } } 启动时需要先启动数据计算程序，再启动数据发送程序。 成功运行截图： ","date":"2024-09-07T11:55:09+08:00","permalink":"https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/","title":"SparkStreaming使用socket"},{"content":"连接MySQL 参考链接：https://www.cnblogs.com/Jaryer/p/13671449.html\nmaven添加依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.33\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 连接数据库 1 2 3 4 5 val host = \u0026#34;localhost\u0026#34; val port = 3306 val database = \u0026#34;sparktest\u0026#34; val jdbcUrl = s\u0026#34;jdbc:mysql://$host:$port/$database?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, \u0026#34;root\u0026#34;, \u0026#34;123456\u0026#34;) 执行查询 SQL语句在执行时有三种：executeQuery,executeUpdate,execute。具体细节可查看此节开头的参考资料。\n1 2 3 4 5 6 7 8 9 val statement: Statement = mysqlConn.createStatement() //插入数据 statement.executeUpdate(\u0026#34;insert into employee values (3,\u0026#39;Mary\u0026#39;,\u0026#39;F\u0026#39;,26)\u0026#34;) statement.executeUpdate(\u0026#34;insert into employee values (4,\u0026#39;Tom\u0026#39;,\u0026#39;M\u0026#39;,23)\u0026#34;) val result: ResultSet = statement.executeQuery(\u0026#34;select max(age) as max_age,avg(age) as avg_age from employee\u0026#34;) while (result.next()) { println(result.getString(\u0026#34;max_age\u0026#34;),result.getString(\u0026#34;avg_age\u0026#34;)) } 完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package sparkjob5 import java.sql.{Connection, DriverManager, ResultSet, Statement} object task3 { def main(args: Array[String]): Unit = { //连接mysql val host = \u0026#34;localhost\u0026#34; val port = 3306 val database = \u0026#34;sparktest\u0026#34; val jdbcUrl = s\u0026#34;jdbc:mysql://$host:$port/$database?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, \u0026#34;root\u0026#34;, \u0026#34;123456\u0026#34;) val statement: Statement = mysqlConn.createStatement() //插入数据 statement.executeUpdate(\u0026#34;insert into employee values (3,\u0026#39;Mary\u0026#39;,\u0026#39;F\u0026#39;,26)\u0026#34;) statement.executeUpdate(\u0026#34;insert into employee values (4,\u0026#39;Tom\u0026#39;,\u0026#39;M\u0026#39;,23)\u0026#34;) val result: ResultSet = statement.executeQuery(\u0026#34;select max(age) as max_age,avg(age) as avg_age from employee\u0026#34;) while (result.next()) { println(result.getString(\u0026#34;max_age\u0026#34;),result.getString(\u0026#34;avg_age\u0026#34;)) } result.close() statement.close() } } 连接Hive 参考链接：https://www.jianshu.com/p/27a798013990\n连接Hive前需要开启Hive的metastore和hiverserver2。开启命令如下。\n开启Hadoop集群 1 start-all.sh 开启Hive,第二三行的启动命令需要分别开一个终端启动，输出的日志在/usr/local/hive/logs。 1 2 3 cd /usr/local/hive hive --service metastore \u0026gt;logs/metastore.log 2\u0026gt;\u0026amp;1 hive --service hiveserver2 \u0026gt;logs/hiveServer2.log 2\u0026gt;\u0026amp;1 添加依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-hive_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 完整依赖如下（包含了Scala连接MySQL的依赖）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;Spark\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;sparkCore\u0026lt;/artifactId\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-core_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-sql_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.33\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-hive_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;17\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;17\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!-- 该插件用于将 Scala 代码编译成 class 文件 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.alchim31.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.2\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;!-- 声明绑定到 maven 的 compile 阶段 --\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;descriptorRefs\u0026gt; \u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt; \u0026lt;/descriptorRefs\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;make-assembly\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 修改配置文件hive-site.xml 在resource下新建一个hive-site.xml，填入下列内容。注意：要把hadoop1修改成自己的Hadoop集群主节点名字或者ip。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 添加文件调用 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.exec.scratchdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/user/hive/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/user/hive/warehouse\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.querylog.location\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/user/hive/log\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定存储元数据要连接的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://hadoop1:9083\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的URL --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true\u0026amp;amp;characterEncodeing=UTF-8\u0026amp;amp;allowPublicKeyRetrieval=true\u0026amp;amp;useSSL=false\u0026amp;amp;serverTimezone=GMT\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的Driver--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的username--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的password --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的host --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.bind.host\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的端口号 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.active.passive.ha.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Scala代码 在spark.sql()里写上正常的SQL语句即可完成查询。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package sparkjob5 import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession object task4 { val driverName = \u0026#34;org.apache.hive.jdbc.HiveDriver\u0026#34; try { Class.forName(driverName) } catch { case e: ClassNotFoundException =\u0026gt; println(\u0026#34;Missing Class\u0026#34;, e) } def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\u0026#34;local[3]\u0026#34;).setAppName(\u0026#34;hive\u0026#34;) val spark = SparkSession.builder().config(conf).enableHiveSupport().getOrCreate() spark.sql(\u0026#34;use spark_test\u0026#34;) spark.sql(\u0026#34;show tables\u0026#34;).show() spark.close() } } 补充：将查询结果保存到hdfs上，如果想保存到本地，则可以将save的路径改成本地路径。\n1 2 3 4 5 6 7 val dataFrame = spark.sql(\u0026#34;select uid,keyword from sougou_records where keyword like \u0026#39;%仙剑奇侠传%\u0026#39;\u0026#34;) dataFrame.write .format(\u0026#34;csv\u0026#34;) .option(\u0026#34;header\u0026#34;, \u0026#34;false\u0026#34;) .option(\u0026#34;sep\u0026#34;, \u0026#34;\\t\u0026#34;) .save(\u0026#34;hdfs://hadoop1:8020/xianJianTest\u0026#34;) 如果想以表格保存到MySQL或者Hive,可以使用saveAsTable()。\n1 2 3 4 5 6 7 8 9 val host = \u0026#34;localhost\u0026#34; val port = 3306 val database = \u0026#34;sparktest\u0026#34; val jdbcUrl = s\u0026#34;jdbc:mysql://$host:$port/$database?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; val connectionProperties = new java.util.Properties() connectionProperties.put(\u0026#34;user\u0026#34;, \u0026#34;root\u0026#34;) connectionProperties.put(\u0026#34;password\u0026#34;, \u0026#34;123456\u0026#34;) df.write.mode(SaveMode.Overwrite).jdbc(jdbcUrl,\u0026#34;company\u0026#34;,connectionProperties) 1 df.write.mode(SaveMode.Overwrite).saveAsTable(\u0026#34;spark_test.company\u0026#34;) ","date":"2024-09-07T11:47:55+08:00","permalink":"https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/","title":"Scala连接MySQL和Hive"},{"content":"Scala中的部分函数和RDD中的部分算子名字一样，功能一样，用起来也差不多。但是为什么一个叫函数，一个却要叫算子，函数和算子的区别在哪，这让我有些好奇。于是查看了源码，对函数和算子进行了比较。下面以map为例。\nScala中的map函数 Scala中的map通常定义在集合类中，例如Map、List、Seq、Set。作用是对该可迭代集合的所有元素应用一个函数，从而建立一个新的可迭代集合。\nBuilds a new iterable collection by applying a function to all elements of this iterable collection.\n注意：List属于scala.collection.immutable的子类，而Map、Seq、Set属于scala.collection的子类 map函数最底层的源码应该是scala.collection里的如下代码 具体实现以List中的map举例 源码为\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 final override def map[B](f: A =\u0026gt; B): List[B] = { if (this eq Nil) Nil else { val h = new ::[B](f(head), Nil) var t: ::[B] = h var rest = tail while (rest ne Nil) { val nx = new ::(f(rest.head), Nil) t.next = nx t = nx rest = rest.tail } releaseFence() h } } List中的map重写了其父类collection.IterableOnce的map函数，定义了一个匿名函数(f:A=\u0026gt;B),对List中的每个元素A处理后输出B类型的List。比如List[String]经过map处理后可以变成List[Interger] 其他collection的代码可以自行查看，也可以查看Scala的官方文档\nhttps://www.scala-lang.org/api/current/scala/collection\nSpark中的map算子 Spark中的算子分为转换算子（Transformations (return a new RDD)）和行动算子（Actions (launch a job to return a value to the user program)）， 转换算子根据数据处理方式的不同将算子整体上分为 Value 类型、双 Value 类型和 Key-Value 类型 。具体不再细讲，可以自行查询。 RDD中的map代码如下：\n1 2 3 4 5 6 7 /** * Return a new RDD by applying a function to all elements of this RDD. */ def map[U: ClassTag](f: T =\u0026gt; U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u0026gt; iter.map(cleanF)) } 可以看出，这里的map首先创建了一个MapPartitionsRDD并生成可迭代对象iter,然后调用了Scala的map函数处理iter。\n结论 Scala里的map函数首先定义在scala.collection里，然后子类（List、Set）重写父类的函数。因为Scala里的类型是隐式的，并且查看源代码在一个子类里也只发现了一个map函数，所以Scala里的map并没有重载，而是通过定义父类，子类重写父类函数的方法实现对不同数据结构的操作。 Spark里的map是对RDD进行操作的算子，实际使用了可迭代对象来调用Scala中的map函数。算子本身的定义就是对RDD操作的函数，所以算子应该也可以被称为是函数，但是为了区分Scala中的函数，所以使用了不同的名字。\n补充 Spark中的算子并非全都有同名函数，原因可以从RDD的原理上分析。 行动算子需要进行shuffle操作，在shuffle时需要按键分区，对每个分区进行操作后输出。Scala中并没有Shuffle操作，所以行动算子没有同名函数。 而转换算子是生成RDD或者将RDD转换成另外的RDD，Scala本身也有将collection转换为collection的函数，并且转换算子本身就调用了Scala的函数，所以有同名的也正常。\n","date":"2024-09-07T11:46:08+08:00","permalink":"https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"Scala中函数与Spark的算子的区别"},{"content":"Spark程序在启动后会在控制台打印大量日志，找了很多教程也没有解决，本来一直可以忍受的。但是学SparkStreaming时实在受不了了，日志已经严重影响到我查看计算结果。遂痛下决心，解决这个一直困扰我的问题。如果使用方法1没有解决的可以直接去看第三步，第二步是解决日志依赖冲突的问题的\n常规解决办法 在resource下建立一个log4j.properties，填入下列内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 log4j.rootCategory=ERROR, console log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.target=System.err log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{yy/MM/ddHH:mm:ss} %p %c{1}: %m%n # Set the default spark-shell log level to ERROR. When running the spark-shell,the # log level for this class is used to overwrite the root logger\u0026#39;s log level, so that # the user can have different defaults for the shell and regular Spark apps. log4j.logger.org.apache.spark.repl.Main=ERROR # Settings to quiet third party logs that are too verbose log4j.logger.org.spark_project.jetty=ERROR log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR log4j.logger.org.apache.parquet=ERROR log4j.logger.parquet=ERROR # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR 查看日志寻求解决办法 常规解决办法没有正常解决，遂查看日志寻求解决办法，查看日志可以明显看出有日志依赖冲突\n1 2 3 4 5 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/D:/maven-3.8.6/respository/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/D:/maven-3.8.6/respository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 怀疑是日志冲突的问题（事实证明不是），分析日志依赖 在SparkCore下面发现了slf4j的依赖，在spark-core的dependency里加入下列内容以屏蔽日志包。\n1 2 3 4 5 6 \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-reload4j\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; 还是日志冲突，把所有dependency下面都加了排除日志依赖的标签 还是不起作用，观察日志可知冲突是因为reload4hj，干脆把仓库里的reload4j删掉，成功解决日志冲突 但是但是，日志依赖冲突的问题解决了，大量info日志的问题却还在\n最终解决办法 在resource下新建一个log4j2.xml文件，填入下面内容即可解决。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;error\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34; /\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; ","date":"2024-09-07T11:39:08+08:00","permalink":"https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","title":"Spark程序大量Info日志问题解决"},{"content":"Spark启动方式有：local模式、standalone模式、Yarn模式、K8S和Mesos模式，本教程只涉及前三种模式，另外两种可以自行查找资料。\nLocal模式 1.下载Spark https://archive.apache.org/dist/spark/ 由于我的Hadoop版本是3.1.3，所以下载的Spark版本也是Spark3,这里下的是Spark3.3.1，只要是Spark3都可以和Hadoop3兼容。\n2.解压Spark压缩包 解压Spark的压缩包，移动到/usr/local/下，修改文件夹的名字为spark\n1 2 3 4 cd ~/Downloads sudo tar -zxvf spark-3.3.1-bin-hadoop3.tgz -C /usr/local/ cd /usr/local/ mv spark-3.3.1-bin-hadoop3.2 spark 3.Local模式启动Spark 1 bin/spark-shell 启动成功后，可以输入网址主机名：4040进行 Web UI 监控页面访问 Standalone模式 1.进入spark文件夹下的conf目录，修改workers.template文件名为workers 1 2 cd conf/ mv workers.template workers 2.修改workers文件，添加worker节点 1 vim workers 3.修改spark-env.sh.template文件名为spark-env.sh 4.修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点 Java默认安装路径如下，手动安装的Java可以指定自己的Java路径 5.分发Spark 6.Standalone模式启动Spark集群 1 2 3 cd spark/ sbin/start-all.sh xcall jps 7.查看进程 Spark正常启动输入网址主机名:8080进行监控 8.提交应用测试Spark 1 2 3 4 bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://hadoop1:7077 ./examples/jars/spark-examples_2.12-3.3.1.jar 10 注意：\u0026ndash;master后面指定的主机名要改成自己的主机名（hadoop1改成自己的主机名） 指定的jar包要指定为自己的jar包，不同版本的示例jar包名字不同。 10是指当前应用的任务数量 提交任务时会有一个SparkSubmit进程，任务结束后进程停止 Yarn 模式 1.修改Hadoop配置文件 修改/usr/local/hadoop/etc/hadoop/yarn-site.xml, 并分发\n1 vim /usr/local/hadoop/etc/hadoop/yarn-site.xml 1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认 是true --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.pmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认 是true --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.vmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 分发修改后的配置文件\n1 xsync /usr/local/hadoop/etc/hadoop/yarn-site.xml 2. 修改conf/spark-env.sh，添加 JAVA_HOME 和 YARN_CONF_DIR 配置 1 vim conf/spark-env.sh 3.分发更改后的Spark-env.sh 1 xsync conf/spark-env.sh 4.Yarn模式提交任务测试 Client模式 1 2 3 4 bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.3.1.jar 10 Cluster模式 1 2 3 4 bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.3.1.jar 10 5.在hadoop1:8088查看，程序运行成功 补充：提交参数说明 参数 解释 可选值举例 \u0026ndash;class Spark 程序中包含主函数的类 \u0026ndash;master Spark 程序运行的模式(环境) 模式：local[*]、spark://hadoop1:7077、Yarn \u0026ndash;executor-memory 1G 指定每个executor 可用内存为1G 符合集群内存配置即可，具体情况具体分析。 \u0026ndash;total-executor-cores 2 指定所有executor使用的cpu核数析。为2个 \u0026ndash;executor-cores 指定每个executor使用的cpu核数 application-jar 打包好的应用 jar，包含依赖。这个URL 在集群中全局可见。比如 hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的 path 都包含同样的 jar application-arguments 传给 main()方法的参数 ","date":"2024-09-07T11:22:39+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/","title":"Ubuntu22.04配置Spark3.3.1集群"},{"content":"前置准备：配置Hive的MySQL连接用户 MySQL的配置可参考我的教程 MySQL安装教程\n创建Hive元数据库 1 create database metastore; 创建用户hive，设置密码为123456 1 2 3 create user \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;123456\u0026#39;; grant all privileges on metastore.* to \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39; with grant option; flush privileges; 安装Hive 参考资料：B站尚硅谷 062.Hive的安装部署_哔哩哔哩_bilibili\n下载Hive安装包 注意：apache原装的Hive只支持Spark2.3.0，不支持Spark3.3.0，需要重新编译Hive的源码，尚硅谷已经编译好了，这里我就直接使用了\n修改配置文件（cd 到hive下的conf文件夹，这里我已经将Hive安装包改名为hive并移动到/usr/local/下） 1 2 sudo mv hive-default.xml.template hive-default.xml sudo vim hive-site.xml 将以下内容写入文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- jdbc连接的URL --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true\u0026amp;amp;characterEncodeing=UTF-8\u0026amp;amp;allowPublicKeyRetrieval=true\u0026amp;amp;useSSL=false\u0026amp;amp;serverTimezone=GMT\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的Driver--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的username--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的password --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- Hive默认在HDFS的工作目录 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/user/hive/warehouse\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定存储元数据要连接的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://hadoop1:9083\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的host --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.bind.host\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的端口号 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.active.passive.ha.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.cli.print.header\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.cli.print.current.db\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 配置环境变量 1 sudo vim ~/.bashrc 在下面添加\n1 2 export HIVE_HOME=/usr/local/hive export PATH=$PATH:$HIVE_HOME/bin 启动Hive 启动Hadoop 1 start-all.sh 初始化Hive 1 2 cd /usr/local/hive ./bin/schematool -dbType mysql -initSchema 正常初始化会日志刷屏并出现大片空白，然后最后一行出现succeed或者complete的字样 如果没有正常初始化就复制最下面几行中的报错信息，粘贴到必应进行查找\n启动Hive 启动Hive前需要先启动Hive的元数据库metastore和hiveserver2 注意：这里的metastore和hiveserver2每个都要单独开启一个终端，开启一个后再开一个新的终端进行命令 日志被重定向到了logs文件夹下，需要查看日志可以在这个文件夹下查看\n1 2 3 cd /usr/local/hive/ hive --service metastore \u0026gt;logs/metastore.log 2\u0026gt;\u0026amp;1 hive --service hiveserver2 \u0026gt;logs/hiveServer2.log 2\u0026gt;\u0026amp;1 1 bin/hive 正常启动会出现一个交互界面如下：\n1 hive(default)\u0026gt; 解决Hive shell中打印大量日志的问题 当在Hive的命令行中查询时出现大量日志时，可以在conf下新建日志配置文件如下\n1 2 cd /usr/local/conf/ vim log4j.properties 粘贴如下内容\n1 2 3 4 log4j.rootLogger=WARN, CA log4j.appender.CA=org.apache.log4j.ConsoleAppender log4j.appender.CA.layout=org.apache.log4j.PatternLayout log4j.appender.CA.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n Hive on Spark配置 在官网下载纯净版Spark（不带Hadoop依赖的） http://spark.apache.org/downloads.html\n解压Spark 1 2 tar -zxvf spark-3.3.1-bin-without-hadoop.tgz -C /usr/local/ mv /usr/local/spark-3.3.1-bin-without-hadoop /usr/local/spark 修改spark-env.sh配置文件 1 2 mv /usr/local/spark/conf/spark-env.sh.template /usr/local/spark/conf/spark-env.sh vim /usr/local/spark/conf/spark-env.sh 增添下面内容\n1 export SPARK_DIST_CLASSPATH=$(hadoop classpath) 配置Spark环境变量 1 sudo vim ~/.bashrc 添加下列内容\n1 2 export SPARK_HOME=/usr/local/spark export PATH=$PATH:$SPARK_HOME/bin 在Hive中创建spark配置文件 1 vim /usr/local/hive/conf/spark-defaults.conf 添加如下内容\n1 2 3 4 5 spark.master yarn spark.eventLog.enabled true spark.eventLog.dir hdfs://hadoop1:8020/spark-history spark.executor.memory 1g spark.driver.memory\t1g 在HDFS中创建如下路径，用于存储历史日志\n1 hadoop fs -mkdir /spark-history 向HDFS上传Spark纯净版jar包 说明1：采用Spark纯净版jar包，不包含hadoop和hive相关依赖，能避免依赖冲突。 说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到\n1 2 hadoop fs -mkdir /spark-jars hadoop fs -put /usr/local/spark/jars/* /spark-jars 修改hive-site.xml文件 1 vim /usr/local/hive/conf/hive-site.xml 添加如下内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;spark.yarn.jars\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/spark-jars/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--Hive执行引擎--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.execution.engine\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;spark\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--连接超时时间--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.spark.client.connect.timeout\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;30000ms\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Hive on Spark测试 启动hive客户端 1 2 cd /usr/local/hive/ bin/hive 创建一张测试表 1 hive (default)\u0026gt; create table student(id int, name string); 通过insert测试效果 1 hive (default)\u0026gt; insert into table student values(1,\u0026#39;abc\u0026#39;); 结果如下则配置成功 Yarn环境配置 增加ApplicationMaster资源比例 容量调度器对每个资源队列中同时运行的Application Master占用的资源进行了限制，该限制通过yarn.scheduler.capacity.maximum-am-resource-percent参数实现，其默认值是0.1，表示每个资源队列上Application Master最多可使用的资源为该队列总资源的10%，目的是防止大部分资源都被Application Master占用，而导致Map/Reduce Task无法执行。 生产环境该参数可使用默认值。但学习环境，集群资源总数很少，如果只分配10%的资源给Application Master，则可能出现，同一时刻只能运行一个Job的情况，因为一个Application Master使用的资源就可能已经达到10%的上限了。故此处可将该值适当调大。\n在hadoop1的/usr/local/hadoop/etc/hadoop/capacity-scheduler.xml文件中修改如下参数值 1 vim capacity-scheduler.xml 1 2 3 4 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.scheduler.capacity.maximum-am-resource-percent\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;0.8\u0026lt;/value\u0026gt; \u0026lt;/property 分发capacity-scheduler.xml配置文件 1 xsync capacity-scheduler.xml 重启集群 1 2 stop-all.sh start-all.sh ","date":"2024-09-07T11:15:48+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/","title":"Ubuntu22.04配置Hive及Hive on Spark"},{"content":"安装依赖 使用以下命令安装依赖\n1 sudo apt-get install build-essential libssl-dev libffi-dev python3-dev python3-pip libsasl2-dev libldap2-dev default-libmysqlclient-dev 配置Superset元数据库 本教程使用MySQL数据库作为Superset的数据库（Superset支持MySQL和PostgreSQL)，安装完成后需进行以下配置。\n在MySQL中创建Superset元数据库 1 CREATE DATABASE superset DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; 创建Superset用户，用户名为superset,密码为superset。superset用户拥有所有数据库的全部权限。 1 2 3 create user superset@\u0026#39;%\u0026#39; identified WITH mysql_native_password BY \u0026#39;superset\u0026#39;; grant all privileges on *.* to superset@\u0026#39;%\u0026#39; with grant option; flush privileges; 为Superset创建Python虚拟环境 更新pip 1 sudo pip install --upgrade pip 下载必要的虚拟环境包 1 sudo pip install virtualenv -i https://pypi.tuna.tsinghua.edu.cn/simple 创建虚拟环境 1 virtualenv superset 激活虚拟环境 1 source superset/bin/activate 安装Superset 1 pip install apache-superset -i https://pypi.tuna.tsinghua.edu.cn/simple 安装其他Python依赖 1 pip install gunicorn pymysql mysqlclient -i https://pypi.tuna.tsinghua.edu.cn/simple 说明：gunicorn是一个Python Web Server，可以和java中的TomCat类比\n配置Superset 在~/superset/bin目录下创建superset配置文件superset_config.py，详细配置可参考官方文档（https://superset.apache.org/docs/installation/configuring-superset/）或者GitHub（https://github.com/apache/superset/）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 # Superset specific config ROW_LIMIT = 5000 #SUPERSET_WEBSERVER_PORT = 8088 # Flask App Builder configuration # Your App secret key will be used for securely signing the session cookie # and encrypting sensitive information on the database # Make sure you are changing this key for your deployment with a strong key. # You can generate a strong key using `openssl rand -base64 42` \u0026#39;\u0026#39;\u0026#39; 使用命令“openssl rand -base64 42”创建SECRET_KEY填写到下面\u0026#39;\u0026#39;\u0026#39; SECRET_KEY = \u0026#39;\u0026#39; # The SQLAlchemy connection string to your database backend # This connection defines the path to the database that stores your # superset metadata (slices, connections, tables, dashboards, ...). # Note that the connection information to connect to the datasources # you want to explore are managed directly in the web UI \u0026#39;\u0026#39;\u0026#39; 数据库连接，我是用的是MySQL数据库 链接字符串：mysql+pymysql://\u0026lt;数据库用户\u0026gt;:\u0026lt;密码\u0026gt;@\u0026lt;主机名/ip\u0026gt;/\u0026lt;数据库名\u0026gt;\u0026#39;\u0026#39;\u0026#39; SQLALCHEMY_DATABASE_URI = \u0026#39;mysql+pymysql://superset:superset@hadoop01:3306/superset?charset=utf8\u0026#39; ENABLE_CSRF_PROTECTION = True # Flask-WTF flag for CSRF WTF_CSRF_ENABLED = True WTF_CSRF_CHECK_DEFAULT = True # Add endpoints that need to be exempt from CSRF protection #WTF_CSRF_EXEMPT_LIST = [] # A CSRF token that expires in 1 year #WTF_CSRF_TIME_LIMIT = 60 * 60 * 24 * 365 #不填这个会出现登录界面输入正确的用户名和密码后登录无反应的现象 #但是关掉这个可能会降低安全性，可能是superset版本太新（3.0.0），旧版本貌似没有这个问题 TALISMAN_ENABLED=False # Set this API key to enable Mapbox visualizations MAPBOX_API_KEY = \u0026#39;\u0026#39; COMPRESS_REGISTER = False #默认中文 BABEL_DEFAULT_LOCALE = \u0026#34;zh\u0026#34; #superset支持的语言 LANGUAGES = { \u0026#34;en\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;us\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;English\u0026#34;}, \u0026#34;es\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;es\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Spanish\u0026#34;}, \u0026#34;it\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;it\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Italian\u0026#34;}, \u0026#34;fr\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;fr\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;French\u0026#34;}, \u0026#34;zh\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;cn\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Chinese\u0026#34;}, \u0026#34;ja\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;jp\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Japanese\u0026#34;}, \u0026#34;de\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;de\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;German\u0026#34;}, \u0026#34;pt\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;pt\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Portuguese\u0026#34;}, \u0026#34;pt_BR\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;br\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Brazilian Portuguese\u0026#34;}, \u0026#34;ru\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;ru\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Russian\u0026#34;}, \u0026#34;ko\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;kr\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Korean\u0026#34;}, \u0026#34;sk\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Slovak\u0026#34;}, \u0026#34;sl\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;si\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Slovenian\u0026#34;}, \u0026#34;nl\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;nl\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Dutch\u0026#34;}, } SHOW_STACKTRACE = False DEBUG = False APP_NAME = \u0026#34;Superset\u0026#34; #不填这个会导致报错如下 #ModuleNotFoundError: No module named \u0026#39;MySQL_db\u0026#39; SQLALCHEMY_TRACK_MODIFICATIONS = False 初始化Superset 初始化数据库 1 2 (superset) rust@hadoop01:~$ export FLASK_APP=superset (superset) rust@hadoop01:~$ superset db upgrade 在元数据库中创建一个superset管理员用户 1 (superset) rust@hadoop01:~$ superset fab create-admin 初始化superset 1 (superset) rust@hadoop01:~$ superset init 启动Superset 1 (superset) rust@hadoop01:~$gunicorn --workers 5 --timeout 120 --bind hadoop01:8787 \u0026#34;superset.app:create_app()\u0026#34; --daemon 说明： \u0026ndash;workers：指定进程个数 \u0026ndash;timeout：worker进程超时时间，超时会自动重启 \u0026ndash;bind：绑定本机地址，即为Superset访问地址 \u0026ndash;daemon：后台运行\n登录Supperset 访问http://hadoop102:8787，并使用初始化Supperset中创建的管理员账户进行登录。\n停止Superset 停止gunicorn进程 1 (superset) rust@hadoop01:~$ ps -ef | awk \u0026#39;/superset/ \u0026amp;\u0026amp; !/awk/{print $2}\u0026#39; | xargs kill -9 退出superset虚拟环境 1 2 3 (superset) rust@hadoop01:~$ ps -ef |grep superst #查到进程号后杀掉进程 (superset) rust@hadoop01:~$ kill -9 pid Superset启停脚本 创建superset.sh文件 1 vim superset.sh 内容如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #!/bin/bash superset_status(){ result=`ps -ef | awk \u0026#39;/gunicorn/ \u0026amp;\u0026amp; !/awk/{print $2}\u0026#39; | wc -l` if [[ $result -eq 0 ]]; then return 0 else return 1 fi } superset_start(){ source ~/.bashrc superset_status \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [[ $? -eq 0 ]]; then source ~/superset/bin/activate ; gunicorn --workers 5 --timeout 120 --bind hadoop01:8787 --daemon \u0026#39;superset.app:create_app()\u0026#39; else echo \u0026#34;superset正在运行\u0026#34; fi } superset_stop(){ superset_status \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [[ $? -eq 0 ]]; then echo \u0026#34;superset未在运行\u0026#34; else ps -ef | awk \u0026#39;/gunicorn/ \u0026amp;\u0026amp; !/awk/{print $2}\u0026#39; | xargs kill -9 fi } case $1 in start ) echo \u0026#34;启动Superset\u0026#34; superset_start ;; stop ) echo \u0026#34;停止Superset\u0026#34; superset_stop ;; restart ) echo \u0026#34;重启Superset\u0026#34; superset_stop superset_start ;; status ) superset_status \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [[ $? -eq 0 ]]; then echo \u0026#34;superset未在运行\u0026#34; else echo \u0026#34;superset正在运行\u0026#34; fi esac 加执行权限 1 sudo chmod +x superset.sh 测试 1 2 3 4 #启动superset superset.sh start #停止superset superset.sh stop ","date":"2024-09-07T11:09:06+08:00","permalink":"https://rusthx.github.io/p/ubuntu-22.04%E9%83%A8%E7%BD%B2apache-superset/","title":"Ubuntu 22.04部署Apache Superset"},{"content":"更新软件包 1 sudo apt-get update 下载MySQL 1 sudo apt-get install mysql-server 登入MySQL MySQL安装完成后会有默认用户和密码，通过默认的用户和密码登入MySQL后可以新建用户并对该用户赋权\n查看默认用户和密码的命令 1 sudo cat /etc/mysql/debian.cnf 使用默认用户和密码登入数据库 mysql -u用户名 -p 输入密码 用户名和密码分别为上图中的user 和password\n1 mysql -udebian-sys-maint -p 新建用户 设置密码 赋权 设置root用户的密码（我的密码设置为123456，根据自己的需求修改命令） 1 2 3 4 use mysql; update user set authentication_string=\u0026#39;\u0026#39; where user=\u0026#39;root\u0026#39;; alter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified with mysql_native_password by \u0026#39;123456\u0026#39;; quit; 使用root用户登入数据库，并新建用户和赋权 下图为查询用户密码（加密过的）的命令 下列命令的意思是： 创建用户rust 并设置rust用户可以访问的位置为%（本地访问和远程访问，仅本地访问为localhost） 复制所有数据库的所有权限给rust用户 刷新权限\n1 2 3 CREATE USER \u0026#39;rust\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;123456\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;rust\u0026#39;@\u0026#39;%\u0026#39; WITH GRANT OPTION; flush privileges; 设置MySQL数据库允许远程访问 默认情况下，MySQL服务器只允许本地连接。\n编辑MySQL配置文件（/etc/mysql/mysql.conf.d/mysqld.cnf）并注释掉以下行（在 bind-address 行前面添加#）： 1 sudo gedit /etc/mysql/mysql.conf.d/mysqld.cnf 保存文件并重启MySQL服务器 1 sudo systemctl restart mysql MySQL执行顺序 参考资料：https://blog.csdn.net/Elsa15/article/details/108544943\n1 2 3 4 5 6 7 8 9 (9) SELECT (10)DISTINCT column,(6) AGG_FUNC(column or expression),... (1）FROM left_table (3）J0IN right_table (2） ON tablename.column = other_tablename.column (4）WHERE constraint_expression(5)GROUP BY column (7)WITH CUBE | ROLLUP (8)HAVING constraint_expression(11)ORDER BY column ASCIDEsc (12)LIMIT count OFFSET count; ","date":"2024-09-04T23:18:48+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E5%AE%89%E8%A3%85mysql8.0.35/","title":"Ubuntu22.04安装MySQL8.0.35"},{"content":"可视化界面的操作简单易上手，主要是基于Gparted,非常适合新手。但是可视化界面的操作也有无法解决的问题，比如因为某些操作（例如编译系统或者下载大小未知的文件）可能会导致系统磁盘空间被占满从而无法下载GParted甚至无法正常开机的状况，这种时候就要使用命令行的扩容方案。\n可视化界面操作方案 VMware给虚拟机扩展空间（不是虚拟机可以直接跳过此步骤） 在虚拟机设置里的磁盘选项点击扩展，选择要扩展到的磁盘大小，我的虚拟机本来就有80G,所以这里就只扩展5G作为演示。 扩容完成后还需要在虚拟机里分区和扩展文件系统。 查看系统占用情况 可视化界面可以在左侧点击文件管理后，点击其他位置，正上方的计算机那里可以看到计算机的使用情况 终端界面，可以输入df -h或者sudo fdisk -l查看系统占用情况 下载分区管理软件GParted 在终端中输入下面的命令下载GParted\n1 sudo apt-get install gparted 对磁盘进行分区 点击左下角的九个点（显示应用程序），找到GParted。因为涉及磁盘数据，所以需要root权限，在弹出界面输入root用户密码即可。 打开软件可以看到目前虚拟机的磁盘情况，灰色的是刚刚在VMware给虚拟机扩容的5G,还没有分配，需要手动分配。 可以看到，/dev/sda3是被挂载到了根目录下面，我们扩容也是要对根目录扩容。右键/dev/sda3，点击调整大小/移动选项，直接滑动条拉到底或者编辑新大小调整空间。然后点击调整大小。 然后左下角会显示一项操作待处理，点击绿色的√后点击应用操作 此时计算机的空间就成功扩容5G 出现无法调整只读文件系统的大小(cannot resize read-only file system)的解决办法 右键/dev/sda3，点击信息，查看挂载点，重新挂载文件夹目录的读写权限。这里系统挂载了几个目录就要重新挂载几个目录的文件读写权限。 这里因为演示的这台机是一台新机，还没有用过火狐浏览器什么的，所以只挂载了根目录。我用另外一台机进行演示 打开终端，重新挂载这几个目录的读写权限\n1 2 sudo mount -o remount -rw / sudo mount -o remount -rw /var/snap/firefox/common/host-hunspell 重新打开GParted或者点击左上角的刷新设备，再次对磁盘进行分区（参照上述教程）\n命令行操作方案 参考资料：https://blog.csdn.net/ynstxx/article/details/129068856\n硬盘占满无法开机 如果磁盘空间还能支持开机，就开机后使用终端； 如果磁盘已经满了不能正常开机（开机时卡在/dev/sda* clean），使用CTRL+ALT+F2进入终端界面，输入用户名和密码进入系统。 下面就可以按照正常的命令行操作步骤进行操作（按照此方法打开的终端会有文本显示错误的问题，如上图中的棱形，这是中文显示错误，不影响实际操作，为了演示美观，我就不用这个演示了）\n查看系统占用情况 扩展虚拟机空间和查看系统占用情况可参照上述教程 输入命令 sudo fdisk -l 用parted -l命令解决爆红部分的分区表问题 使用parted命令将/dev/sda3扩容 1 2 3 4 5 6 7 8 9 10 11 12 13 sudo parted /dev/sda unit s（设置Size单位） p free（查看磁盘详情） resizepart 3（对第3个盘进行扩容） Yes（分区正在使用中，确认是否继续） 188743646s（需要将磁盘扩容到的大小值，此处参考自己的空间） q（退出磁盘分区模式） 使用sudo resize2fs /dev/sda3命令更新磁盘3的容量 1 sudo resize2fs /dev/sda3 再次通过df -h或者sudo fdisk -l命令查看磁盘情况 1 2 df -h sudo fdisk -l 扩容完成，如果是硬盘占满通过CTRL+ALT+F2打开的终端，此时输入命令reboot重启电脑即可正常启动\n","date":"2024-09-04T23:10:48+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E6%89%A9%E5%B1%95%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4/","title":"Ubuntu22.04扩展虚拟机的磁盘空间"},{"content":"安装虚拟机 下载vmvare https://www.vmware.com/products/desktop-hypervisor.html 现在的vmvare安装变得麻烦起来了，还要登陆网站。这里我用的是我之前下载的vmvare16pro。如果官网下载不太容易也可以直接搜索vmvare16安装，浏览器上应该还有留存的资源。\n下载Ubuntu24.04 https://cn.ubuntu.com/download/desktop Ubuntu的镜像文件大了很多，Ubuntu22.04只有3G左右，Ubuntu24.04就已经5G了。自行选择吧，两种都可以。 安装虚拟机 这次设置多少其实都可以，但是建议不要给的太少。尤其是磁盘空间，调整起来有点麻烦。官网给了最少资源。个人建议初始安装内存给4G，磁盘给40G。 这里安装扩展集合也可以，会比默认集合多装一些办公应用、视频播放器之类的东西。 谨慎设置用户名，电脑主机名修改比较容易，但是用户名修改比较困难。密码的话随意设置一个就可以。反正虚拟机是学习使用，太复杂的还是防自己罢了。 这个安装过程可能会很慢，应该会花几分钟。 重启后会提示按enter键。\nUbuntu配置 配置网络 刚安装的虚拟机里没有网，需要手动设置网络。设置网络有两种，一种是修改配置文件，一种是可视化界面（GUI）修改。我个人倾向GUI修改。 配置文件修改的方案参考https://blog.csdn.net/zyw2002/article/details/123486055 的2.2小节 可视化界面修改网络配置信息步骤如下： 点击左上角的编辑，虚拟网络编辑器，VMnet8。记住子网IP。比如我的是192.168.146.0 我的VMnet8子网ip为192.168.146.0。所以这里ip地址我填入了192.168.146.xxx（这里xxx可以填入小于255的数，但是建议不要填10以内的数）。子网掩码填入255.255.255.0即可。网关和DNS参照我的填入即可。 应用配置后重启一下连接就可以正常连接网络。右上角的网络标记变成图中的标志时就表示联网正常。没有可视化界面的可以ping一下外网查看情况。ping baidu.com。按CTRL+C即可终止命令。 安装vmtools 安装vmtools后可以解决屏幕显示比例的问题，可以在虚拟机外复制然后粘贴到虚拟机里，这很重要！ 右键，在终端中打开。安装命令如下\n1 2 3 4 sudo apt update sudo apt install open-vm-tools -y sudo apt install open-vm-tools-desktop -y sudo reboot 配置用户免密 在使用sudo命令时普通用户需要输入密码。每次都需要输入密码很麻烦，设置免密后使用sudo就不需要再输入密码。\n1 2 3 sudo passwd root #给root用户设置一个密码 sudo apt install gedit # gedit是一个文本编辑器，可以让ubuntu上编辑文件像Windows的记事本 sudo apt install vim #vim 是一个纯终端的文本编辑器 1 2 3 4 5 sudo chmod 777 /etc/sudoers # /etc/sudoers是一个只读文件，需要修改权限 su root gedit /etc/sudoers #在文件中添加如下标注部分 chmod 440 /etc/sudoers #将文件权限修改回来，解决报错/etc/sudoers可被任何人写 exit; 安装SSH 1 2 3 sudo apt-get install openssh-server sudo gedit /etc/ssh/sshd_config reboot #重启生效设置 尝试ssh远程登陆 设置主机映射 修改Windows的C:\\Windows\\System32\\drivers\\etc下的hosts文件。这里修改可能会有权限问题，可以复制hosts文件，粘贴到能修改的地方，比如桌面。修改完成后删除C:\\Windows\\System32\\drivers\\etc下的hosts，将修改后的hosts文件粘贴进来。 打开Windows的命令终端，输入命令\n1 ssh rust@bird # rust为ubuntu的用户名，bird为刚才设置的主机映射名 安装Java Ubuntu安装java可以手动安装，也可以命令行安装。手动安装需要设置环境变量，建议使用命令行安装。命令行安装卸载比较方便。\n1 2 sudo apt-get install openjdk-8-jdk java -version #查看安装是否成功 ","date":"2024-09-03T23:14:48+08:00","permalink":"https://rusthx.github.io/p/vmvare%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"VMvare安装配置Ubuntu虚拟机"}]