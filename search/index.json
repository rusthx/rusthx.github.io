[{"content":" 官网链接-ingress概念讲解\nK8s集群搭建以及镜像源配置 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd Ingress原理介绍与对比 Service是“后端+负载均衡”的抽象，解决“怎么找到 Pod”和“在多个Pod间做负载均衡”。\nIngress是“前置网关+路由规则”，在HTTP(S)这一层，做“HTTP路由+TLS+虚拟主机”，根据域名、路径等把流量分到不同的Service。\nGateway API是Ingress的“升级版”，把“网关实例”和“路由规则”拆开，更标准、更强大、更适合生产。\n没有ingress时，部署一套无状态应用 每个应用都需要一个NodePort或者LoadBalancer；同时需要记住\u0026quot;哪个端口对应哪个服务\u0026quot;； 想加域名、HTTPS、路径重写，要自己写反向代理配置（Nginx/HAProxy 等）。\n在学习/小集群场景下完全没问题，但是如果应用多起来（微服务架构），维护成本就变得很高了。\n如果加上一个ingress,内容如下,就可以通过path+pathType(Prefix或者Exact)来区分不同的服务。 对外访问变成了http://demo.example.com/ → svc-a（前端）,http://demo.example.com/api → svc-b（后端）。 而 svc-a / svc-b 可以只是普通 ClusterIP Service，不需要 NodePort/LoadBalancer。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: demo-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: demo.example.com http: paths: - path: / pathType: Prefix backend: service: name: svc-a port: number: 80 - path: /api pathType: Prefix backend: service: name: svc-b port: number: 80 Service与Ingress都有负载均衡的作用，但这两个工作的网络层次不同，面向的对象也不同。\nService工作在L4传输层(IP:port),给一组 Pod 一个统一的VIP(ClusterIP)，把TCP/UDP连接均匀分发到后端Pod。 NodePort/LoadBalancer类型Service，本质上也是在节点上开一个端口（NodePort）或云厂商给你分配一个外部 LB； 再把这个L4流量转发到Service的ClusterIP，最后到 Pod。\nIngress工作在L7应用层(HTTP/HTTPS),它本身是个“HTTP 反向代理”（Nginx、Envoy、Traefik 等）； 根据域名、路径、Header等把请求路由到不同 Service；在同一个 Ingress 内部，它会对后端Service的多个Pod做L7负载均衡（轮询、权重、健康检查等）。\nService：解决“Pod 挂了怎么办、怎么在多个 Pod 间轮询”，是“后端池”。 Ingress：解决“一个入口怎么统一路由给很多后端服务”，是“前端网关”。 Service是怎么通过VIP实现负载均衡的？ 在k8s里，当创建一个type: ClusterIP的Service 时： 控制平面会从某个预先保留的 IP 段（\u0026ndash;service-cluster-ip-range）里分配一个 IP 地址，这就是.spec.clusterIP。 这个 IP 不是一个真正绑定在某个网卡上的 IP，而是一个 “虚拟 IP（Virtual IP）”，只存在于 kube-proxy / CNI 的规则里。\nkube-proxy（或替代组件）在每个节点上运行，监听 Service 和 EndpointSlice。 对每个 ClusterIP:Port，它在节点上配置一套规则（iptables / IPVS / eBPF 等），规则内容类似： “所有发往 10.96.0.1:80的包，都改成一个后端 Pod 的 IP:Port，并做负载均衡轮询”。 当一个 Pod（或集群外通过NodePort进来的包）访问10.96.0.1:80时： 实际上不会真正有“机器”监听在这个 IP 上； 而是经过这些规则，通过iptables/IPVS被DNAT（目标地址转换） 成某个真实 Pod IP:Port。\n反向代理和路由的区别：\n反向代理：是一个“服务器角色”，站在客户端和后端服务器之间，帮服务器收请求、再转发给后端，通常在“应用层”（HTTP 等）做这件事。\n路由：是一种“规则/机制”，用来决定“一个请求（或包）该往哪儿走”，可以在网络层（IP 路由）、也可以在应用层（URL 路由）做。\n关系：反向代理是“做代理的那台机器”，路由是它内部决定怎么转发的“规则”。\n类比就像：反向代理是一个“门卫”，路由是它手里拿的“路线图”，门卫根据路线图把人（请求）带到对应的办公室（后端服务）。\n既然 NodePort 就能用，为什么还要 Ingress？ 类别一下：\nNodePort：像给每栋楼开一个单独的大门，每栋楼自己有门牌号（端口）。 Ingress：像建一个“小区大门”，所有访客先到小区大门，再根据地址（域名/路径）分到具体楼栋 端口与协议限制：官网中明确说：Ingress 不暴露任意端口或协议。把 HTTP/HTTPS 以外的服务暴露到互联网，通常用Service.Type=NodePort或Service.Type=LoadBalancer。（补充：Gateway API支持更多的协议，包括TCP/UDP）\n多服务、多域名场景：如果只有一栋楼（一个应用），那用NodePort确实够了；但是在有一个小区（多个应用）的情况下，再给每栋楼开个对外的门，会造成管理混乱的问题。如果硬要用NodePort，那还需要自己做路由(Nginx)来分发请求。\nTLS、虚拟主机、路径重写等能力：Ingress支持终止TLS（配置证书，统一 HTTPS）；虚拟主机（根据 Host头路由到不同服务）；路径重写、限流、白名单等（通过注解或 Gateway API）。这些如果用 NodePort，就要自己在外面再搭一套反向代理，反而更麻烦。\nIngress和Gateway API区别\n官网说：Kubernetes 项目推荐使用 Gateway 代替 Ingress。Ingress API 已冻结。 也就是说Ingress 不会再有新功能，只做维护；新特性、新标准都会放在 Gateway API 里。\n维度 Ingress Gateway API 目标协议 主要 HTTP/HTTPSkubernetes.io HTTP、HTTPS、gRPC、TCP、UDP 等多协议 API 风格 单一 Ingress 资源，把“网关 + 路由规则”写在一起 拆分为 GatewayClass、Gateway、HTTPRoute 等角色化资源 角色/职责 没有明确区分基础设施和开发者 明确区分：Gateway 归集群管理员，HTTPRoute 归应用开发者 高级路由 仅主机 + 路径，高级功能依赖各控制器注解 原生支持 Header 匹配、流量分割、镜像、超时、重试等 可移植性 各控制器实现差异大，注解各不相同 标准化 API，跨实现更可移植 状态 已冻结，不再演进 活跃开发中，未来会持续增加新能力 无状态应用Ingress改造 前置内容:无状态应用\n改造前客户端通过NodePort直连前端，改造后用ingress统一入口，客户端访问ingress。\n前端 Service：从 type: NodePort 改成 type: ClusterIP。 后端 Service：保持不变（仍然 ClusterIP）。 新增：Ingress Controller（这里用NGINX Ingress）；一个Ingress资源，把hello.example.com/转发到 frontend:80\n安装 NGINX Ingress Controller（bare-metal） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.14.3/deploy/static/provider/baremetal/deploy.yaml # 官方仓库的镜像拉不下来，修改为阿里云的镜像源 sed -i \u0026#39;s|registry.k8s.io/ingress-nginx/controller:v1.14.3@.*|registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.14.3|\u0026#39; deploy.yaml sed -i \u0026#39;s|registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.7@.*|registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.6.7|\u0026#39; deploy.yaml # 部署 kubectl apply -f deploy.yaml rust@k8s1:~/k8s_try$ curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.14.3/deploy/static/provider/baremetal/deploy.yaml % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 16279 100 16279 0 0 22100 0 --:--:-- --:--:-- --:--:-- 22088 rust@k8s1:~/k8s_try$ kubectl apply -f deploy.yaml namespace/ingress-nginx created serviceaccount/ingress-nginx created serviceaccount/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created configmap/ingress-nginx-controller created service/ingress-nginx-controller created service/ingress-nginx-controller-admission created deployment.apps/ingress-nginx-controller created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created ingressclass.networking.k8s.io/nginx created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created 这个yaml将创建一个 ingress-nginx namespace；部署 NGINX Ingress Controller； 创建一个 ingress-nginx-controller Service，类型为 NodePort，在 80/443 端口上暴露。\n应用deloy.yaml创建了好多东西，如果过程中出现了一些问题，比如镜像拉不下来，想重做，用kubectl delete -f deploy.yaml即可删除用这个文件创建的所有资源。\n查看安装状态,如果pod有三个ingress-nginx-admission-create-xx、ingress-nginx-admission-patch-xx、 ingress-nginx-controller-74d74b7998-xx。属于正常情况，creat和patch是两个job，在执行完成后会自动删除。\n1 2 3 4 5 6 7 8 9 10 kubectl get pod -n ingress-nginx kubectl get svc -n ingress-nginx rust@k8s1:~/k8s_try$ kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-controller-65b88c9cd4-vc4qf 1/1 Running 0 8m56s rust@k8s1:~/k8s_try$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.99.59.37 \u0026lt;none\u0026gt; 80:31238/TCP,443:32350/TCP 8m33s ingress-nginx-controller-admission ClusterIP 10.98.81.151 \u0026lt;none\u0026gt; 443/TCP 8m33s HTTP NodePort：比如80:31238/TCP，这里的31238；\nHTTPS NodePort：比如443:32350/TCP，这里的32350。\n访问方式：http://任意节点IP:31238，https://任意节点IP:32350。返回结果目前应该是HTTP ERROR 503。\n把前端 Service 改成 ClusterIP 新建/修改 frontend-service.yaml，去掉 type: NodePort 和 nodePort。\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: frontend spec: selector: app: hello tier: frontend ports: - protocol: TCP port: 80 targetPort: 80 # 不写 type，默认就是 ClusterIP 应用yaml文件。\n1 2 vim frontend-service.yaml kubectl apply -f frontend-service.yaml 后端 Service 保持不变。\n创建 Ingress 资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: frontend-ingress annotations: # 如果后面要用路径重写，可以开这个注解（按需） # nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx # NGINX Ingress Controller 默认使用 nginx 这个 class rules: # - host: hello.example.com # 改成你自己的域名，或者先删掉这行用 IP 访问 - http: paths: - path: / pathType: Prefix backend: service: name: frontend # 对应前端Service 名字 port: number: 80 新建并应用frontend-ingress.yaml。\n1 2 vim frontend-ingress.yaml kubectl apply -f frontend-ingress.yaml 查看ingress状态\n1 2 3 4 5 6 kubectl get ingress kubectl describe ingress frontend-ingress rust@k8s1:~/k8s_try$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE frontend-ingress nginx * 192.168.146.202 80 86s 验证nginx服务状态\n1 2 3 4 curl \u0026lt;Host\u0026gt;:\u0026lt;Port\u0026gt; rust@k8s1:~/k8s_try$ curl k8s1:31238 {\u0026#34;message\u0026#34;:\u0026#34;Hello\u0026#34;} 进阶玩法：把 NGINX Ingress 改成 HostNetwork 把 NGINX Ingress 改成 HostNetwork后有下面的好处：\n让 Ingress Controller 的 Pod 直接监听节点的 80/443； 不再依赖 NodePort Service； 访问方式变成标准的 http(s)://hello.example.com，不用再带 :31234 之类的端口 暂且搁置，以后有时间再研究\n","date":"2026-02-13T09:54:49+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A09_%E6%B5%81%E9%87%8F%E5%85%A5%E5%8F%A3ingress/","title":"K8s实践练习9_流量入口Ingress"},{"content":" 官网链接\n本教程将使用PV部署一个WordPress和MySQL。\nK8s集群搭建以及镜像源配置 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 给本地自建k8s集群添加默认存储类 K8s “产品”本身不自带 StorageClass； 但“常见的集群环境”（云厂商、本地发行版）一般都会预置一个或多个 StorageClass，并把其中一个标为默认。例如：\nminikube：通常有一个 standard StorageClass，并标记为 (default)。 GKE：一般会自带 standard-rwo（或类似名字）默认类。 EKS：AWS EBS CSI 会安装 gp2 / gp3 类并设为默认 kubectl get sc查看集群存储类。有的话可以跳过此节\n在三个节点上创建存储基础目录(xcall是自建的脚本)。 后续动态创建PV时，会按PVC来建子目录，每个PVC会有自己的一块目录。 1 2 xcall sudo mkdir -p /opt/local-path-provisioner xcall sudo chmod 755 /opt/local-path-provisioner 以后想换路径可以修改local-path-config这个ConfigMap里的nodePathMap配置\n安装 local-path-provisioner（自带一个 StorageClass：local-path） 直接用官方的 YAML(以当前稳定版 v0.0.34 为例) 1 kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.34/deploy/local-path-storage.yaml 这个 YAML 里面已经包含：\nNamespace：local-path-storage RBAC（ServiceAccount / Role / ClusterRole / RoleBinding / ClusterRoleBinding） Deployment：local-path-provisioner 一个 StorageClass：名字叫 local-path，provisioner 是 rancher.io/local-path 一个 ConfigMap：local-path-config，里面配置了默认路径 /opt/local-path-provisioner 确认安装成功,检查 Pod 是否 Running,存储类是否已经存在(名字是local-path) 1 2 3 4 5 6 7 8 kubectl get pods -n local-path-storage kubectl describe pod \u0026lt;pod_name\u0026gt; -n local-path-storage kubectl get sc rust@k8s1:~/k8s_try$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path rancher.io/local-path Delete WaitForFirstConsumer false 4m40s 把 local-path 设置为默认StorageClass。 Kubernetes 默认 StorageClass 是通过注解 storageclass.kubernetes.io/is-default-class 来标记的。 local-path-provisioner 的 YAML 里默认没有给它打这个注解， 直接用 kubectl patch 打注解： 1 2 3 4 5 6 7 8 9 10 kubectl patch storageclass local-path \\ -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; rust@k8s1:~/k8s_try$ kubectl patch storageclass local-path \\ -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; storageclass.storage.k8s.io/local-path patched # 打上注解后再查看存储类，发现local-path以标记为默认存储类 rust@k8s1:~/k8s_try$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 9m22s 现在集群就可以自动制取持久卷了。\n创建PV和PVC MySQL和Wordpress 都需要一个 PersistentVolume 来存储数据。它们的 PersistentVolumeClaims将在部署步骤中创建。\n我们刚刚创建了集群的默认存储类，创建PVC时可以动态创建PV。\n创建 kustomization.yaml 创建 Secret 生成器 Secret 是存储诸如密码或密钥之类敏感数据的对象。从 1.14 开始，kubectl 支持使用一个kustomization文件来管理 Kubernetes 对象。可以通过 kustomization.yaml中的生成器创建一个Secret或者ConfigMap。\n通过以下命令在 kustomization.yaml 中添加一个 Secret 生成器。请注意将YOUR_PASSWORD替换为自己要用的密码。\n1 2 3 4 5 6 cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml secretGenerator: - name: mysql-pass literals: - password=YOUR_PASSWORD EOF 补充 MySQL 和 WordPress 的资源配置 注意：MySQL和WordPress的deployment都没有限制资源使用，可能会由于资源使用过量而被驱逐，在生产环境千万不能这么做。\n在kustomization.yaml同级目录创建mysql和wordpress的deployment yaml文件。\n1 vim mysql-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 apiVersion: v1 kind: Service metadata: name: wordpress-mysql labels: app: wordpress spec: ports: - port: 3306 selector: app: wordpress tier: mysql clusterIP: None --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim labels: app: wordpress spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress-mysql labels: app: wordpress spec: selector: matchLabels: app: wordpress tier: mysql strategy: type: Recreate template: metadata: labels: app: wordpress tier: mysql spec: containers: - image: mysql:8.0 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password - name: MYSQL_DATABASE value: wordpress - name: MYSQL_USER value: wordpress - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim 1 vim wordpress-deployment.yaml 注意，由于我的k8s集群是部署在本地的，所以loadBlancer用不了。 下面的yaml文件和官网的不同，区别在Service的type是NodePort。这个问题在k8s实践练习3部署无状态应用的时候也出现了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 apiVersion: v1 kind: Service metadata: name: wordpress labels: app: wordpress spec: ports: - port: 80 targetPort: 80 # 明确指定容器端口（通常是80） nodePort: 30080 # 可选，不指定则系统自动分配一个 30000-32767 之间的端口 selector: app: wordpress tier: frontend type: NodePort --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: wp-pv-claim labels: app: wordpress spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress labels: app: wordpress spec: selector: matchLabels: app: wordpress tier: frontend strategy: type: Recreate template: metadata: labels: app: wordpress tier: frontend spec: containers: - image: wordpress:6.2.1-apache name: wordpress env: - name: WORDPRESS_DB_HOST value: wordpress-mysql - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password - name: WORDPRESS_DB_USER value: wordpress ports: - containerPort: 80 name: wordpress volumeMounts: - name: wordpress-persistent-storage mountPath: /var/www/html volumes: - name: wordpress-persistent-storage persistentVolumeClaim: claimName: wp-pv-claim 将上述内容追加到 kustomization.yaml 文件\n1 2 3 4 5 cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;./kustomization.yaml resources: - mysql-deployment.yaml - wordpress-deployment.yaml EOF 应用和验证 kustomization.yaml包含用于部署 WordPress 网站以及 MySQL 数据库的所有资源。通过以下方式应用目录：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 kubectl apply -k ./ # 验证Secret是否存在 kubectl get secrets # 验证是否已经动态制备PVC kubectl get pvc rust@k8s1:~/k8s_try$ kubectl apply -k ./ secret/mysql-pass-kkcc2b926b created service/wordpress created service/wordpress-mysql created persistentvolumeclaim/mysql-pv-claim created persistentvolumeclaim/wp-pv-claim created deployment.apps/wordpress created deployment.apps/wordpress-mysql created rust@k8s1:~/k8s_try$ kubectl get pods NAME READY STATUS RESTARTS AGE wordpress-7ccff4687c-gbfgk 0/1 ContainerCreating 0 8s wordpress-mysql-5447c445d4-lw47g 1/1 Running 0 8s rust@k8s1:~/k8s_try$ kubectl get secrets NAME TYPE DATA AGE mysql-pass-kkcc2b926b Opaque 1 3m6s rust@k8s1:~/k8s_try$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE mysql-pv-claim Bound pvc-4feb1fcc-f28e-470c-91b1-6f379efffedf 20Gi RWO local-path \u0026lt;unset\u0026gt; 6m5s wp-pv-claim Bound pvc-8d654ca7-9812-475d-953b-cdc4fd9177f8 20Gi RWO local-path \u0026lt;unset\u0026gt; 6m5s mysql和wordpress的镜像都不小，拉镜像可能要两三分钟，制取和绑定PV可能还需要几分钟。过一段时间后查看pod状态\n1 2 3 4 5 6 kubectl get pods -o wide rust@k8s1:~/k8s_try$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES wordpress-7ccff4687c-gbfgk 1/1 Running 0 7m44s 172.16.109.88 k8s2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; wordpress-mysql-5447c445d4-lw47g 1/1 Running 0 7m44s 172.16.109.87 k8s2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 验证service状态，EXTERNAL-IP为none。\n1 2 3 4 5 kubectl get services wordpress rust@k8s1:~/k8s_try$ kubectl get services wordpress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wordpress NodePort 10.103.140.50 \u0026lt;none\u0026gt; 80:30080/TCP 43s 在浏览器中查看wordPress状态（在wordpress的service文件中指定了端口为30080），访问k8s1:30080(改成你的k8s集群上的节点ip:指定的端口) 警告： 不要在此页面上保留 WordPress 安装。如果其他用户找到了它，他们可以在你的实例上建立一个网站并使用它来提供恶意内容。\n通过创建用户名和密码来安装 WordPress 或删除你的实例。\n清理现场 运行下面的命令删除创建的的 Secret、Deployment、Service 和 PersistentVolumeClaims。\n1 kubectl delete -k ./ ","date":"2026-02-12T16:59:38+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A08_%E6%9C%89%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E4%BA%8C%E7%95%AA%E6%88%98%E9%83%A8%E7%BD%B2mysql%E5%92%8Cwordpress/","title":"K8s实践练习8_有状态应用二番战，部署MySQL和WordPress"},{"content":" 官网链接\n本教程将使用StatefulSet部署一个简单的 Web 应用。\n创建 StatefulSet 了解StatefulSet怎样管理它的 Pod 删除 StatefulSet 对StatefulSet 进行扩容/缩容 更新一个StatefulSet的Pod 前置准备 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 给本地自建k8s集群添加默认存储类 K8s “产品”本身不自带 StorageClass； 但“常见的集群环境”（云厂商、本地发行版）一般都会预置一个或多个 StorageClass，并把其中一个标为默认。例如：\nminikube：通常有一个 standard StorageClass，并标记为 (default)。 GKE：一般会自带 standard-rwo（或类似名字）默认类。 EKS：AWS EBS CSI 会安装 gp2 / gp3 类并设为默认 kubectl get sc查看集群存储类。有的话可以跳过此节\n在三个节点上创建存储基础目录(xcall是自建的脚本)。 后续动态创建PV时，会按PVC来建子目录，每个PVC会有自己的一块目录。 1 2 xcall sudo mkdir -p /opt/local-path-provisioner xcall sudo chmod 755 /opt/local-path-provisioner 以后想换路径可以修改local-path-config这个ConfigMap里的nodePathMap配置\n安装 local-path-provisioner（自带一个 StorageClass：local-path） 直接用官方的 YAML(以当前稳定版 v0.0.34 为例) 1 kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.34/deploy/local-path-storage.yaml 这个 YAML 里面已经包含：\nNamespace：local-path-storage RBAC（ServiceAccount / Role / ClusterRole / RoleBinding / ClusterRoleBinding） Deployment：local-path-provisioner 一个 StorageClass：名字叫 local-path，provisioner 是 rancher.io/local-path 一个 ConfigMap：local-path-config，里面配置了默认路径 /opt/local-path-provisioner 确认安装成功,检查 Pod 是否 Running,存储类是否已经存在(名字是local-path) 1 2 3 4 5 6 7 8 kubectl get pods -n local-path-storage kubectl describe pod \u0026lt;pod_name\u0026gt; -n local-path-storage kubectl get sc rust@k8s1:~/k8s_try$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path rancher.io/local-path Delete WaitForFirstConsumer false 4m40s 把 local-path 设置为默认StorageClass。 Kubernetes 默认 StorageClass 是通过注解 storageclass.kubernetes.io/is-default-class 来标记的。 local-path-provisioner 的 YAML 里默认没有给它打这个注解， 直接用 kubectl patch 打注解： 1 2 3 4 5 6 7 8 9 10 kubectl patch storageclass local-path \\ -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; rust@k8s1:~/k8s_try$ kubectl patch storageclass local-path \\ -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; storageclass.storage.k8s.io/local-path patched # 打上注解后再查看存储类，发现local-path以标记为默认存储类 rust@k8s1:~/k8s_try$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 9m22s 现在集群就可以自动制取持久卷了。\n创建 StatefulSet 创建一个Headless Service nginx用来发布StatefulSet web中的Pod的IP地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx # 官方文档里用的瘦身版，拉不下来就用普通的nginx就行 image: nginx:1.25 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi 开启两个终端，一个终端用来监控StatefulSet状态，另一个用来创建StatefulSet。\n1 2 3 4 5 6 # 在第一个终端监控StatefulSet状态 kubectl get pods --watch -l app=nginx # 在第二个终端创建StatefulSet vim web.yaml kubectl apply -f web.yaml web.yaml创建了两个 Pod，每个都运行了一个 NGINX Web 服务器。查看pod、service、statefulset运行状态：\n1 2 3 4 5 6 7 8 9 10 11 rust@k8s1:~/k8s_try$ kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 90s web-1 1/1 Running 0 90s rust@k8s1:~/k8s_try$ kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None \u0026lt;none\u0026gt; 80/TCP 26m rust@k8s1:~/k8s_try$ kubectl get statefulset web NAME READY AGE web 2/2 26m StatefulSet 默认以严格的顺序创建其 Pod。\n对于一个拥有 n 个副本的 StatefulSet，Pod 被部署时是按照 {0..n-1} 的序号顺序创建的。 从刚刚开启的监控终端可以看到statefulSet创建过程中直到第一个pod(web-0)处于Running并ready状态时，第二个pod才开始创建。\n1 2 3 4 5 6 7 8 9 NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 19s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 18s StatefulSet 中的 Pod 检查 Pod 的顺序索引 StatefulSet中的每个Pod拥有一个唯一的顺序索引和稳定的网络身份标识。\nStatefulSet 中的每个Pod拥有一个具有黏性的、独一无二的身份标志。这个标志基于StatefulSet控制器分配给每个 Pod 的唯一顺序索引。 Pod 名称的格式为 \u0026lt;statefulset 名称\u0026gt;-\u0026lt;序号索引\u0026gt;。web StatefulSet拥有两个副本，所以它创建了两个Pod：web-0 和 web-1。\n每个 Pod 都拥有一个基于其顺序索引的稳定的主机名。使用kubectl exec在每个 Pod 中查看hostname\n1 2 3 4 5 6 7 rust@k8s1:~$ kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 1 (9m28s ago) 13h web-1 1/1 Running 1 (9m23s ago) 13h rust@k8s1:~$ for i in 0 1; do kubectl exec \u0026#34;web-$i\u0026#34; -- sh -c \u0026#39;hostname\u0026#39;; done web-0 web-1 使用稳定的网络身份标识 使用 kubectl run 运行一个提供 nslookup 命令的容器，该命令来自于 dnsutils 包。 通过对 Pod 的主机名执行 nslookup，检查这些主机名在集群内部的 DNS 地址：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 打开dns-test容器终端 kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm # 在终端中查看dns,可以看到两个pod的dns server地址相同 nslookup web-0.nginx nslookup web-1.nginx rust@k8s1:~$ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm If you don\u0026#39;t see a command prompt, try pressing enter. / # nslookup web-0.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 172.16.109.124 web-0.nginx.default.svc.cluster.local / # nslookup web-1.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 172.16.219.51 web-1.nginx.default.svc.cluster.local 删除现有的pod，等待statefulSet控制器重建启动这些pod。再次查看这些pod的信息，会发现Pod 的序号、主机名、SRV 条目和记录名称没有改变，但和 Pod 相关联的 IP 地址可能发生了改变。 在本教程中使用的集群中它们就改变了。这就是为什么不要在其他应用中使用 StatefulSet 中特定 Pod 的 IP 地址进行连接，这点很重要（可以通过解析 Pod 的主机名来连接到 Pod）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # 先开一个终端监控删除后statefulSet重建pod的过程 kubectl get pod --watch -l app=nginx # 在第二个终端删除pod kubectl delete pod -l app=nginx # 删除完成后再查看pod，可以看见还是有两个pod在运行，但是age只有几秒 kubectl get pods rust@k8s1:~$ kubectl delete pod -l app=nginx pod \u0026#34;web-0\u0026#34; deleted pod \u0026#34;web-1\u0026#34; deleted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 5s web-1 1/1 Running 0 4s # 第二个终端监控 rust@k8s1:~$ kubectl get pod --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 29s web-1 1/1 Running 0 28s web-0 1/1 Terminating 0 32s web-1 1/1 Terminating 0 31s web-1 1/1 Terminating 0 31s web-0 1/1 Terminating 0 32s web-1 0/1 Terminating 0 32s web-0 0/1 Terminating 0 33s web-1 0/1 Terminating 0 32s web-1 0/1 Terminating 0 32s web-0 0/1 Terminating 0 33s web-0 0/1 Terminating 0 33s web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 2s web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 1s web-1 1/1 Running 0 2s 两个pod在删除重建后，再次查看主机名和集群内部的DNS表项。 Pod 的序号、主机名、SRV 条目和记录名称没有改变，但和 Pod 相关联的 IP 地址可能发生了改变\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 rust@k8s1:~$ for i in 0 1; do kubectl exec web-$i -- sh -c \u0026#39;hostname\u0026#39;; done web-0 web-1 rust@k8s1:~$ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm If you don\u0026#39;t see a command prompt, try pressing enter. / # nslookup web-0.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 172.16.109.127 web-0.nginx.default.svc.cluster.local / # nslookup web-1.nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 172.16.219.53 web-1.nginx.default.svc.cluster.local / # exit pod \u0026#34;dns-test\u0026#34; deleted rust@k8s1:~$ 发现 StatefulSet 中特定的 Pod 相关概念：无头服务、Pod稳定主机名、SRV记录\nHeadless Service（无头服务）就是 clusterIP: None 的 Service，比如示例里的： 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Service metadata: name: nginx spec: clusterIP: None # 关键：Headless selector: app: nginx ports: - port: 80 name: web 对应statefulSet的：\n1 2 3 4 5 6 7 8 apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; # 关联到这个 Headless Service replicas: 2 ... Headless Service不会有一个统一的 ClusterIP，而是给每个 Pod 生成一条 DNS 记录\nPod 的稳定主机名 StatefulSet 里的 Pod 名字固定、有序：web-0，web-1，web-2…… 每个Pod 有一个“稳定的主机名”：web-0.nginx.default.svc.cluster.local、web-1.nginx.default.svc.cluster.local 即使 Pod 删除重建，名字不变，IP 可能变，但主机名对应关系不变。 SRV 记录： DNS 里除了 A 记录（域名-\u0026gt;IP），还有 SRV 记录，用来表示“某个服务有哪些后端实例（主机:端口）”。 对 Headless Service，K8s 会给每个 Running+Ready 的 Pod 生成一条 SRV 记录 同一个 StatefulSet，有三种不同抽象层级的“服务发现方式”，分别对应“要列表 / 要固定 / 要简单”三种需求。\n查找需求 DNS 查询的内容 返回值 自动健康检查 典型场景 当前所有健康成员列表 Headless Service 的 CNAME → SRV(nginx.default.svc.cluster.local) 多个 Pod 的主机名+端口（仅 Running+Ready） 是，由 SRV 过滤 数据库集群、副本集，要成员列表 固定连某个编号的 Pod 某个 Pod 的 SRV(web-0.nginx.default.svc.cluster.local、web-1.nginx.default.svc.cluster.local) 单个 Pod 的 IP+端口 不负责，靠应用自己探活 主从、分片，需要固定连某个序号 随便连一个健康的 Pod 普通ClusterIP Service的 DNS 或 IP 随机一个后端 Pod 的 IP 是，Service 层负责 无状态服务、简单客户端 写入稳定的存储 查看web-0和web-1的PersistentVolumeClaims：\n1 2 3 4 5 6 kubectl get pvc -l app=nginx rust@k8s1:~$ kubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE www-web-0 Bound pvc-29d78e3e-a93b-4db3-880e-3a121d9af83b 1Gi RWO local-path \u0026lt;unset\u0026gt; 18h www-web-1 Bound pvc-0b32d304-d17d-4f7d-9c38-a8d6fb62a709 1Gi RWO local-path \u0026lt;unset\u0026gt; 18h StatefulSet 控制器创建了两个 PersistentVolumeClaims， 绑定到两个 PersistentVolumes。 前面配置了默认存储类，所以PV是动态创建和绑定的。\nNginX Web 服务器默认会加载位于 /usr/share/nginx/html/index.html 的 index 文件。 StatefulSet spec 中的 volumeMounts 字段保证了 /usr/share/nginx/html 文件夹由一个 PersistentVolume 卷支持。\n将 Pod 的主机名写入它们的 index.html 文件并验证 NginX Web 服务器使用该主机名提供服务：\n1 2 3 4 5 6 7 for i in 0 1; do kubectl exec \u0026#34;web-$i\u0026#34; -- sh -c \u0026#39;echo \u0026#34;$(hostname)\u0026#34; \u0026gt; /usr/share/nginx/html/index.html\u0026#39;; done for i in 0 1; do kubectl exec -i -t \u0026#34;web-$i\u0026#34; -- curl http://localhost/; done rust@k8s1:~$ for i in 0 1; do kubectl exec \u0026#34;web-$i\u0026#34; -- sh -c \u0026#39;echo \u0026#34;$(hostname)\u0026#34; \u0026gt; /usr/share/nginx/html/index.html\u0026#39;; done rust@k8s1:~$ for i in 0 1; do kubectl exec -i -t \u0026#34;web-$i\u0026#34; -- curl http://localhost/; done web-0 web-1 在一个终端监视 StatefulSet 的 Pod,在另一个终端删除Pod，删除后再次验证所有Web 服务器在继续使用它们的主机名提供服务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 监控终端 kubectl get pod -w -l app=nginx # 操作终端，删除pod后检查pod状态以及提供服务情况 kubectl delete pod web-0 web-1 kubectl get pods for i in 0 1; do kubectl exec -i -t \u0026#34;web-$i\u0026#34; -- curl http://localhost/; done rust@k8s1:~$ kubectl delete pod web-0 web-1 pod \u0026#34;web-0\u0026#34; deleted pod \u0026#34;web-1\u0026#34; deleted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 40s web-1 1/1 Running 0 39s rust@k8s1:~$ for i in 0 1; do kubectl exec -i -t \u0026#34;web-$i\u0026#34; -- curl http://localhost/; done web-0 web-1 虽然 web-0 和 web-1 被重新调度了，但它们仍然继续监听各自的主机名，因为和它们的 PersistentVolumeClaim 相关联的 PersistentVolume 卷被重新挂载到了各自的 volumeMount 上。 不管 web-0 和 web-1 被调度到了哪个节点上，它们的 PersistentVolume 卷将会被挂载到合适的挂载点上。\n扩容/缩容 StatefulSet 扩容/缩容 StatefulSet 指增加或减少它的副本数。这通过更新 replicas 字段完成（水平缩放）。 可以使用 kubectl scale 或者 kubectl patch 来扩容/缩容一个 StatefulSet。\n扩容 在一个终端窗口监视 StatefulSet 的 Pod，在另一个终端窗口使用 kubectl scale 扩展副本数为 5。\n1 2 3 4 # 监控pod状态，当 StatefulSet 有 5 个健康的 Pod 时结束此 watch kubectl get pods --watch -l app=nginx kubectl scale sts web --replicas=5 StatefulSet 控制器扩展了副本的数量。StatefulSet 按序号索引顺序创建各个 Pod，并且会等待前一个 Pod 变为 Running 和 Ready 才会启动下一个 Pod。\n缩容 依旧在一个终端中监控，另一个终端中操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 监控到有3个pod在运行时就可以停了 kubectl get pods --watch -l app=nginx # 操作终端，缩容pod副本数为3个 kubectl patch sts web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3}}\u0026#39; rust@k8s1:~$ kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 17m web-1 1/1 Running 0 17m web-2 1/1 Running 0 3m24s web-3 1/1 Running 0 3m18s web-4 1/1 Running 0 3m13s web-4 1/1 Terminating 0 3m17s web-4 1/1 Terminating 0 3m17s web-4 0/1 Terminating 0 3m17s web-4 0/1 Terminating 0 3m17s web-4 0/1 Terminating 0 3m17s web-3 1/1 Terminating 0 3m22s web-3 1/1 Terminating 0 3m22s web-3 0/1 Terminating 0 3m22s web-3 0/1 Terminating 0 3m23s web-3 0/1 Terminating 0 3m23s rust@k8s1:~$ kubectl patch sts web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3}}\u0026#39; statefulset.apps/web patched rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 18m web-1 1/1 Running 0 18m web-2 1/1 Running 0 3m54s 控制器会按照与 Pod 序号索引相反的顺序每次删除一个 Pod。在删除下一个 Pod 前会等待上一个被完全关闭。也就是后开启的先关闭（后进先出）。\n获取 StatefulSet 的 PersistentVolumeClaims,PVC的生命周期和Pod是隔离的， 即使Pod被关闭或者删除（不管是手动删除还是缩容导致的删除），PVC也不会回收。\n1 2 3 4 5 6 7 8 9 kubectl get pvc -l app=nginx rust@k8s1:~$ kubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE www-web-0 Bound pvc-29d78e3e-a93b-4db3-880e-3a121d9af83b 1Gi RWO local-path \u0026lt;unset\u0026gt; 18h www-web-1 Bound pvc-0b32d304-d17d-4f7d-9c38-a8d6fb62a709 1Gi RWO local-path \u0026lt;unset\u0026gt; 18h www-web-2 Bound pvc-1d4e4d92-8153-4bd2-b1d0-45e3de8b477a 1Gi RWO local-path \u0026lt;unset\u0026gt; 5m46s www-web-3 Bound pvc-cc082b06-9eec-4831-bc9a-1efa3f6c4eb7 1Gi RWO local-path \u0026lt;unset\u0026gt; 5m40s www-web-4 Bound pvc-fbad49f8-f1b3-4869-9dba-67e1d2200324 1Gi RWO local-path \u0026lt;unset\u0026gt; 5m35s 更新StatefulSet StatefulSet 控制器支持自动更新。 更新策略由 StatefulSet API 对象的 spec.updateStrategy 字段决定。这个特性能够用来更新一个 StatefulSet中Pod的容器镜像(image)、资源请求和限制(request、limit)、标签和注解(label、annotation)。\n有两个有效的更新策略：RollingUpdate（默认）和 OnDelete。\n滚动更新 RollingUpdate 更新策略会更新一个 StatefulSet 中的所有 Pod，采用与序号索引相反的顺序并遵循 StatefulSet 的保证。\n可以通过指定.spec.updateStrategy.rollingUpdate.partition将使用RollingUpdate策略的StatefulSet的更新拆分为多个分区。\n尝试一个简单的滚动更新，更新nginx镜像版本为1.14.2。\n1 2 3 4 #在另外一个终端监控pod滚动更新 kubectl get pod -l app=nginx --watch kubectl patch statefulset web --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;nginx:1.14.2\u0026#34;}]\u0026#39; StatefulSet 里的 Pod 采用和序号相反的顺序更新。在更新下一个 Pod 前，StatefulSet控制器会终止最新的Pod并等待它们变成 Running 和 Ready。\n请注意，虽然在顺序后继者变成 Running 和 Ready 之前 StatefulSet 控制器不会更新下一个 Pod，但它仍然会重建任何在更新过程中发生故障的 Pod，使用的是它们现有的版本。\n此外，我在实验中发现假如滚动更新过程中重建pod失败，例如更新后的镜像是一个拉不下来的镜像（k8s官方镜像源的镜像），那么web-2将卡住另外两个pod的更新。这时候我再执行更新命令，更新nginx镜像为可拉取的镜像，web-2并不会执行更新而是一直堵塞在上一次更新，并且导致其他两个pod也不更新（因为最新的Pod web-2还没有完成更新）。这时候删除web-2，StatefulSet控制器会根据当前的StatefulSet配置文件创建web-2,然后再滚动更新web-1和web-0。\n查看更新后的镜像，确认滚动更新是否以及完成。\n1 2 3 4 5 6 for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done rust@k8s1:~$ for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done nginx:1.14.2 nginx:1.14.2 nginx:1.14.2 分段更新 指定 .spec.updateStrategy.rollingUpdate.partition 将使用 RollingUpdate 策略的 StatefulSet 的更新拆分为多个分区。\n如果声明了一个分区，当 StatefulSet 的 .spec.template 被更新时，所有序号大于等于该分区序号的 Pod 都会被更新。\n所有序号小于该分区序号的 Pod 都不会被更新，并且，即使它们被删除也会依据之前的版本进行重建。 如果 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 大于它的 .spec.replicas，则对它的 .spec.template 的更新将不会传递到它的 Pod。\n分段更新适用于阶段更新、执行金丝雀或执行分阶段上线。\n金丝雀发布 金丝雀部署是一种部署策略，开始时有两个环境：一个有实时流量，另一个包含没有实时流量的更新代码。 流量逐渐从应用程序的原始版本转移到更新版本。 它可以从移动 1% 的实时流量开始，然后是 10%，25%，以此类推，直到所有流量都通过更新的版本运行。 企业可以在生产中测试新版本的软件，获得反馈，诊断错误，并在必要时快速回滚到稳定版本。\n“金丝雀” 一词是指 “煤矿中的金丝雀” 的做法，即把金丝雀带入煤矿以保证矿工的安全。 如果出现无味的有害气体，鸟就会死亡，而矿工们知道他们必须迅速撤离。 同样，如果更新后的代码出了问题，现场交通就会被 “疏散” 回原来的版本\n对 web StatefulSet 执行 Patch 操作，为 updateStrategy 字段添加一个分区,再次 Patch StatefulSet 来改变此 StatefulSet 使用的容器镜像，这次将nginx版本1.14.2-\u0026gt;1.16.1。再删除Pod web-2，等待替代的 Pod 变成 Running 和 Ready后查看pod的镜像版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 kubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;OnDelete\u0026#34;, \u0026#34;rollingUpdate\u0026#34;: null}}}\u0026#39; kubectl patch statefulset web --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;nginx:1.16.1\u0026#34;}]\u0026#39; kubectl delete pod web-2 kubectl get pod web-2 --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39; rust@k8s1:~$ kubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;OnDelete\u0026#34;, \u0026#34;rollingUpdate\u0026#34;: null}}}\u0026#39; statefulset.apps/web patched rust@k8s1:~$ kubectl patch statefulset web --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;nginx:1.16.1\u0026#34;}]\u0026#39; statefulset.apps/web patched rust@k8s1:~$ kubectl delete pod web-2 pod \u0026#34;web-2\u0026#34; deleted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 25m web-1 1/1 Running 0 25m web-2 1/1 Running 0 39s rust@k8s1:~$ kubectl get pod web-2 --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39; nginx:1.16.1rust@k8s1:~$ 监控更新的终端里显示只有web-2被更新（终止后重启），web-1和web-0没变化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 rust@k8s1:~$ kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 24m web-1 1/1 Running 0 24m web-2 1/1 Running 0 24m web-2 1/1 Terminating 0 24m web-2 1/1 Terminating 0 24m web-2 0/1 Terminating 0 24m web-2 0/1 Terminating 0 24m web-2 0/1 Terminating 0 24m web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 1s 在第一个终端查看各个pod的nginx镜像版本，确实只有web-2更新了。\n1 2 3 4 5 6 7 for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done rust@k8s1:~$ for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done nginx:1.14.2 nginx:1.14.2 nginx:1.16.1 金丝雀发布 现在尝试对分段的变更进行金丝雀发布。\n可以通过减少上文指定的 partition 来进行金丝雀发布，以测试修改后的模板。\n控制平面会触发 web-2 的替换（先优雅地删除现有 Pod，然后在删除完成后创建一个新的 Pod）。 等待新的 web-2 Pod 变成 Running 和 Ready，再查看容器的镜像版本。\n改 template + 大 partition，再把 partition 减小，这样web-2由于分区比partition小，将在当前template将下被重建\n1 2 3 4 5 6 7 8 9 # 开一个监控终端监控pod变化 kubectl get pods --watch -l app=nginx # “partition” 的值应与 StatefulSet 现有的最高序号相匹配 kubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;RollingUpdate\u0026#34;,\u0026#34;rollingUpdate\u0026#34;:{\u0026#34;partition\u0026#34;:2}}}}\u0026#39; # 再改一次镜像（或其他 template 字段） kubectl patch statefulset web --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;nginx:1.25\u0026#34;}]\u0026#39; 更改template字段后，监控终端将看到web-2这个pod终止后重启，而另外两个pod由于partition比设定的partition小，将不会更新。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 rust@k8s1:~$ for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done nginx:1.14.2 nginx:1.14.2 nginx:1.16.1 rust@k8s1:~$ kubectl patch statefulset web --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;nginx:1.25\u0026#34;}]\u0026#39; statefulset.apps/web patched rust@k8s1:~$ for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done nginx:1.14.2 nginx:1.14.2 nginx:1.25 rust@k8s1:~$ kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 71m web-1 1/1 Running 0 21m web-2 1/1 Running 0 46m web-2 1/1 Terminating 0 46m web-2 1/1 Terminating 0 46m web-2 0/1 Terminating 0 46m web-2 0/1 Terminating 0 46m web-2 0/1 Terminating 0 46m web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 0/1 ContainerCreating 0 1s web-2 1/1 Running 0 1s 这时候再删除web-1,由于web-1的Pod的序号小于分区，web-1重建将依据原来的template文件，也即是nginx版本还是1.14.2。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 rust@k8s1:~$ kubectl delete pod web-1 pod \u0026#34;web-1\u0026#34; deleted rust@k8s1:~$ for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done nginx:1.14.2 nginx:1.14.2 nginx:1.25 rust@k8s1:~$ kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 71m web-1 1/1 Running 0 21m web-2 1/1 Running 0 46m web-2 1/1 Terminating 0 46m web-2 1/1 Terminating 0 46m web-2 0/1 Terminating 0 46m web-2 0/1 Terminating 0 46m web-2 0/1 Terminating 0 46m web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 0/1 ContainerCreating 0 1s web-2 1/1 Running 0 1s web-1 1/1 Terminating 0 24m web-1 1/1 Terminating 0 24m web-1 0/1 Terminating 0 24m web-1 0/1 Terminating 0 24m web-1 0/1 Terminating 0 24m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 1s 分阶段的发布 可以使用类似金丝雀发布的方法执行一次分阶段的发布 （例如一次线性的、等比的或者指数形式的发布）。\n把分区设置为0，则web-1和web-0都将更新。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 kubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;RollingUpdate\u0026#34;,\u0026#34;rollingUpdate\u0026#34;:{\u0026#34;partition\u0026#34;:0}}}}\u0026#39; for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done rust@k8s1:~$ kubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;RollingUpdate\u0026#34;,\u0026#34;rollingUpdate\u0026#34;:{\u0026#34;partition\u0026#34;:0}}}}\u0026#39; statefulset.apps/web patched rust@k8s1:~$ for p in 0 1 2; do kubectl get pod \u0026#34;web-$p\u0026#34; --template \u0026#39;{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}\u0026#39;; echo; done nginx:1.25 nginx:1.25 nginx:1.25 rust@k8s1:~$ kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 78m web-1 1/1 Running 0 3m8s web-2 1/1 Running 0 6m34s web-1 1/1 Terminating 0 3m12s web-1 1/1 Terminating 0 3m12s web-1 0/1 Terminating 0 3m12s web-1 0/1 Terminating 0 3m12s web-1 0/1 Terminating 0 3m12s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 1s web-0 1/1 Terminating 0 78m web-0 1/1 Terminating 0 78m web-0 0/1 Terminating 0 78m web-0 0/1 Terminating 0 78m web-0 0/1 Terminating 0 78m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 1s OnDelete 策略 将.spec.template.updateStrategy.type设置为OnDelete，以使用OnDelete更新策略。\n对 web StatefulSet 执行 patch 操作\n1 kubectl patch statefulset web -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;updateStrategy\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;OnDelete\u0026#34;, \u0026#34;rollingUpdate\u0026#34;: null}}}\u0026#39; 当选择这个更新策略并修改 StatefulSet 的 .spec.template 字段时，StatefulSet 控制器将不会自动更新 Pod。 需要自己手动管理发布，或使用单独的自动化工具来管理发布。\n删除StatefulSet StatefulSet 同时支持非级联和级联删除。使用非级联方式删除 StatefulSet 时，StatefulSet 的 Pod 不会被删除。使用级联删除时，StatefulSet 和它的 Pod 都会被删除。\n非级联删除 在删除statefulSet时加上参数--cascade=orphan即可非级联删除。非级联删除虽然不会删除Pod，但是也没有StatefulSet管理pod，如果pod被删除或者因为异常停止退出，也不会重启。\n1 2 3 4 5 6 7 8 9 10 11 12 13 rust@k8s1:~$ kubectl delete statefulset web --cascade=orphan statefulset.apps \u0026#34;web\u0026#34; deleted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 11m web-1 1/1 Running 0 11m web-2 1/1 Running 0 18m rust@k8s1:~$ kubectl delete pod web-0 pod \u0026#34;web-0\u0026#34; deleted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 13m web-2 1/1 Running 0 20m 重新应用StatefulSet的yaml文件，因为接下来测试级联删除还需要用(最早的yaml文件里设置的副本数为2，只是扩容缩容后变成了3)。\n1 2 3 4 5 6 7 rust@k8s1:~/k8s_try$ kubectl apply -f web.yaml service/nginx unchanged statefulset.apps/web created rust@k8s1:~/k8s_try$ kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 5s web-1 1/1 Running 0 15m 当重新创建 web StatefulSet 时，web-0 被第一个重新启动。 由于 web-1 已经处于 Running 和 Ready 状态，当 web-0 变成 Running 和 Ready 时， StatefulSet 会接收这个 Pod。由于重新创建的 StatefulSet 的 replicas 等于 2， 一旦 web-0 被重新创建并且 web-1 被认为已经处于 Running 和 Ready 状态时，web-2 将会被终止。\n由于巧合，web-1的nginx版本被改回了1.25版本，也就是最早的web.yaml里的版本。如果刚才改的版本与web.yaml版本不一致，再查看pod的nginx版本，会发现web-1并没有修改为web.yaml里指定的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 rust@k8s1:~$ kubectl get pods --watch -l app=nginx web-0 1/1 Terminating 0 13m web-0 1/1 Terminating 0 13m web-0 0/1 Terminating 0 13m web-0 0/1 Terminating 0 13m web-0 0/1 Terminating 0 13m web-2 1/1 Running 0 22m web-1 1/1 Running 0 15m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 1s web-2 1/1 Terminating 0 22m web-2 1/1 Terminating 0 22m web-2 0/1 Terminating 0 22m web-2 0/1 Terminating 0 22m web-2 0/1 Terminating 0 22m 现在再看看被 Pod 的 Web 服务器加载的 index.html 的内容：\n1 2 3 4 5 for i in 0 1; do kubectl exec -i -t \u0026#34;web-$i\u0026#34; -- curl http://localhost/; done rust@k8s1:~/k8s_try$ for i in 0 1; do kubectl exec -i -t \u0026#34;web-$i\u0026#34; -- curl http://localhost/; done web-0 web-1 尽管同时删除了 StatefulSet 和 web-0 Pod，但它仍然使用最初写入 index.html 文件的主机名进行服务。这是因为StatefulSet不会删除PV。当你重建这个 StatefulSet并且重新启动了 web-0 时，它原本的 PersistentVolume 卷会被重新挂载。\n级联删除 默认的删除模式就是级联删除（不加参数）。不过级联删除也只删除pod，不会删除关联的service。这是因为pod是由StatefulSet控制器创建的，所以删除StatefulSet时会级联删除也能级联删除Pod，但Service是由Service控制器创建的，StatefulSet不能删除Service。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 rust@k8s1:~/k8s_try$ kubectl delete statefulset web statefulset.apps \u0026#34;web\u0026#34; deleted rust@k8s1:~$ kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 7m17s web-1 1/1 Running 0 22m web-1 1/1 Terminating 0 27m web-0 1/1 Terminating 0 12m web-1 1/1 Terminating 0 27m web-0 1/1 Terminating 0 12m web-1 0/1 Terminating 0 27m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-1 0/1 Terminating 0 27m web-1 0/1 Terminating 0 27m 这时候再重新应用web.yaml，再查看web-0和web-1的index.yaml，会发现挂载的还是原本的PVC,输出的还是更改过的主机名。 （不知是第几次call back了，这也是持久卷为什么叫持久卷。这里就不再演示了）\nPod管理策略 对于某些分布式系统来说，StatefulSet 的顺序性保证是不必要和/或者不应该的。这些系统仅仅要求唯一性和身份标志。\n可以指定Pod管理策略以避免这个严格的顺序；OrderedReady（默认）或 Parallel\nOrderedReady Pod管理策略 OrderedReady Pod 管理策略是 StatefulSet 的默认选项。它告诉 StatefulSet 控制器遵循上文展示的顺序性保证。\n顺序就是前文中创建和删除的顺序。后进先出。严格按序\nParallel Pod管理策略 Parallel Pod管理策略告诉StatefulSet控制器并行的终止所有Pod，在启动或终止另一个 Pod 前，不必等待这些 Pod 变成 Running 和 Ready 或者完全终止状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; # 关键：指定Pod管理策略为并行 podManagementPolicy: \u0026#34;Parallel\u0026#34; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.25 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 开一个终端监控pod状态 kubectl get pods --watch -l app=nginx # 操作终端创建编辑应用yaml文件 vim web-parallel.yaml kubectl apply -f web-parallel.yaml rust@k8s1:~$ kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 1s web-1 1/1 Running 0 1s 清理现场 1 2 3 4 5 6 7 # 删除StatefulSet,sts 是 statefulset 的缩写 kubectl delete sts web # 删除Sevice kubectl delete svc nginx # 删除PVC。 kubectl delete pvc www-web-0 www-web-1 www-web-2 www-web-3 www-web-4 由于动态制备的 PV 默认回收策略是Delete(reclaimPolicy: Delete)，所以删除PVC之后绑定的PV也会自动删除。 如果是Retain 策略则PV 变 Released，保留数据\n而直接删PV不会立刻成功，PV会进入 Terminating 状态，finalizer里会有kubernetes.io/pv-protection；要等到PV不再被任何PVC绑定，才会真正删除\n","date":"2026-02-11T17:35:23+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A07_%E6%9C%89%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8/","title":"K8s实践练习7_有状态应用"},{"content":" 官网链接\n本文将介绍如何配置Pod使用PersistentVolumeClaim(持久卷申领)作为存储\n作为集群管理员创建由物理存储支持的不与任何Pod关联的PersistentVolume(持久卷)。\n以开发人员或者集群用户的角色创建一个PersistentVolumeClaim，它将自动绑定到合适的PersistentVolume。\n创建一个使用刚创建的PersistentVolumeClaim作为存储的Pod。\n前置准备 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 创建index.html 1 2 3 4 5 6 # 创建文件夹 sudo mkdir /mnt/data # 创建index.html并写入数据 sudo sh -c \u0026#34;echo \u0026#39;Hello from Kubernetes storage\u0026#39; \u0026gt; /mnt/data/index.html\u0026#34; # 验证查看数据 cat /mnt/data/index.html 创建PersistentVolume 本练习将创建一个 hostPath 类型的 PersistentVolume。 Kubernetes 支持用于在单节点集群上开发和测试的 hostPath 类型的 PersistentVolume。 hostPath 类型的 PersistentVolume 使用节点上的文件或目录来模拟网络附加存储。\nhostPath类型的PV会无视调度，直接绑定到主机上。如果 Pod 漂移到了另一个节点，它依然会尝试去挂载那个路径。 即使 Pod 被删除，Node 上的文件依然存在（除非手动清理）。这有时会导致“幽灵数据”问题（新 Pod 跑过来读取到了旧 Pod 的残留数据）。 风险较高。Pod 可以直接访问 Node 的系统目录（如 /var/run/docker.sock），如果 Pod 被攻破，黑客可以直接控制宿主机。\n生产环境一般禁用或者限制使用hostPath的PV。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce # 与local卷的关键区别，local类型的没有hostPath关键字。而是用nodeAffinity(节点亲和性)绑定节点 hostPath: path: \u0026#34;/mnt/data\u0026#34; # 宿主机上的物理路径 创建本地持久卷。\n1 2 vim pv-volume.yaml kubectl apply -f pv-volume.yaml 此配置文件指定卷位于集群节点上的/mnt/data路径。其配置还指定了卷的容量大小为 10 GB，访问模式为ReadWriteOnce， 这意味着该卷可以被单个节点以读写方式挂载。此配置文件还在 PersistentVolume 中定义了StorageClass的名称为 manual。 它将用于将PersistentVolumeClaim的请求绑定到此 PersistentVolume。\n说明：为了简化，本示例采用了ReadWriteOnce访问模式。然而对于生产环境，Kubernetes 项目建议改用 ReadWriteOncePod 访问模式。\nK8s Access Mode(访问模式)\n名称 简称 介绍 适用场景 ReadWriteOnce RWO 卷可以被单个节点上的单个Pod以读写方式挂载 数据库、单实例有状态应用 ReadWriteOncePod RWOP V1.29+稳定，卷可以被整个集群上的单个Pod以读写方式挂载 需要强数据一致性保障、不支持并发读写的有状态应用,如云盘 ReadOnlyMany ROX 卷可以被多个节点以只读方式挂载 共享配置文件、静态Web内容、只读数据集 ReadWriteMany RWX 卷可以被多个节点以读写方式挂载 多副本Web应用共享上传目录、分布式缓存共享存储、日志聚合 查看 PersistentVolume 的信息，该持久卷状态为可用，代表还未与PVC(持久卷申领)绑定。\n1 2 3 4 5 kubectl get pv task-pv-volume rust@k8s1:~$ kubectl get pv task-pv-volume NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS VOLUMEATTRIBUTESCLASS REASON AGE task-pv-volume 10Gi RWO Retain Available manual \u0026lt;unset\u0026gt; 7s 创建 PersistentVolumeClaim Pod 使用 PersistentVolumeClaim来请求物理存储。 下面创建一个 PersistentVolumeClaim，它请求至少 3 GB 容量的卷，该卷一次最多可以为一个节点提供读写访问。\n1 2 vim pv-claim.yaml kubectl apply -f pv-claim.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 3Gi 创建 PersistentVolumeClaim 之后，Kubernetes 控制平面将查找满足申领要求的 PersistentVolume。 如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume， 则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上。\n1 2 3 4 5 6 rust@k8s1:~$ kubectl get pv task-pv-volume NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS VOLUMEATTRIBUTESCLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual \u0026lt;unset\u0026gt; 6m18s rust@k8s1:~$ kubectl get pvc task-pv-claim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual \u0026lt;unset\u0026gt; 82s 创建Pod 创建一个使用刚创建的PersistentVolumeClaim作为存储卷的Pod。对 Pod 而言，PersistentVolumeClaim 就是一个存储卷。普通用户一般没有PV的权限，只有PVC的使用权限。而PVC绑定的PV的制备细节（动态或者静态制备，制备参数）对用户透明。\nk8s创建pod会先进行调度，控制平面上有控制平面的污点（.spec.taints），所以创建的普通pod不会调度到控制平面上执行。 这也是为什么官网教程里用的是单节点集群。解决方法有两种,这里选择方案2，更安全更精细。\n移除k8s1控制平面上的NoSchedule污点（仅学习测试环境，生产环境不建议,因为控制平面节点上有很多关键组件,如apiserver、etcd、controller-manager 等，资源压力大会影响集群稳定性） 让pod\u0026quot;不普通\u0026quot;,给pod添加上容忍度(tolerations)和节点选择器(nodeSelector) 1 2 vim pv-pod.yaml kubectl apply -f pv-pod.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: # 1. 让这个 Pod 能容忍控制平面节点的 NoSchedule 污点 tolerations: - key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule # 2. 强制把 Pod 调度到 k8s1 这个节点 nodeSelector: kubernetes.io/hostname: k8s1 volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: - name: task-pv-container image: nginx:1.25 ports: - containerPort: 80 name: \u0026#34;http-server\u0026#34; volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: task-pv-storage 验证查看pod状态：\n1 2 3 4 5 6 7 8 9 kubectl get pod task-pv-pod # 打开nginx容器的终端，验证 Nginx 是否正在从 hostPath 卷提供 index.html kubectl exec -it task-pv-pod -- curl http://localhost/ rust@k8s1:~$ kubectl get pod task-pv-pod NAME READY STATUS RESTARTS AGE task-pv-pod 1/1 Running 1 (2m25s ago) 112m rust@k8s1:~$ kubectl exec -it task-pv-pod -- curl http://localhost/ Hello from Kubernetes storage 清理现场，删除pod\n1 kubectl delete pod task-pv-pod 在两个位置挂载一个卷 同一个 volumes.name（同一个卷对象）； 被两个 volumeMounts 各自引用，指定不同的 mountPath 和 subPath。\n更直白地讲就是PVC对应的PV的文件夹挂载到容器里的两个目录或文件。\n创建/mnt/data/nginx.conf和/mnt/data/html。将index.html重复利用一下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 sudo mkdir /mnt/data/html sudo mv /mnt/data/index.html /mnt/data/html sudo tee /mnt/data/nginx.conf \u0026gt; /dev/null \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } } EOF 创建一个 Pod，使用已有的 PersistentVolume 和 PersistentVolumeClaim。 不过，这个 Pod 只将特定的文件 nginx.conf 和目录 html 挂载到容器中。\n1 2 vim pv-duplicate.yaml kubectl apply -f pv-duplicate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: v1 kind: Pod metadata: name: test spec: # 1. 让这个 Pod 能容忍控制平面节点的 NoSchedule 污点 tolerations: - key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule # 2. 强制把 Pod 调度到 k8s1 这个节点 nodeSelector: kubernetes.io/hostname: k8s1 containers: - name: test image: nginx:1.25 volumeMounts: # 网站数据挂载 - name: config mountPath: /usr/share/nginx/html subPath: html # Nginx 配置挂载 - name: config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf volumes: - name: config persistentVolumeClaim: claimName: task-pv-claim subPath：此字段允许将挂载的 PersistentVolume 中的特定文件或目录暴露到容器内的不同位置。在本例中： subPath: html挂载 html 目录。 subPath: nginx.conf挂载一个特定文件 nginx.conf。\nnginx 容器中会挂载两个路径：\n/usr/share/nginx/html：用于静态网站 /etc/nginx/nginx.conf：用于默认配置 验证nginx容器运行情况,验证 nginx 是否从 hostPath 卷中加载了 nginx.conf 文件(keepalived_timeout改为了65)\n1 2 3 4 5 6 7 kubectl exec -it test -- curl http://localhost/ kubectl exec -it test -- cat /etc/nginx/nginx.conf | grep keepalive_timeout rust@k8s1:~$ kubectl exec -it test -- curl http://localhost/ Hello from Kubernetes storage rust@k8s1:~$ kubectl exec -it test -- cat /etc/nginx/nginx.conf | grep keepalive_timeout keepalive_timeout 65; 清理现场\n1 2 3 4 5 kubectl delete pod test kubectl delete pvc task-pv-claim kubectl delete pv task-pv-volume sudo rm -rf /mnt ","date":"2026-02-11T09:14:22+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A06_%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/","title":"K8s实践练习6_持久化存储"},{"content":" 官网链接\n很多应用在其初始化或运行期间要依赖一些配置信息。但是我们不太想要配置参数被硬编码进容器,而是希望这些配置参数是可调节的。 ConfigMap 是 Kubernetes 的一种机制，可让你将配置数据注入到应用的Pod内部。\nConfigMap支持将配置清单与镜像内容分离，以保持容器化的应用程序的可移植性。 例如，可以下载并运行相同的容器镜像来启动容器， 用于本地开发、系统测试或运行实时终端用户工作负载。\n本页提供了一系列使用示例，这些示例演示了如何创建 ConfigMap 以及配置 Pod 使用存储在 ConfigMap 中的数据。\n前置准备 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 创建ConfigMap k8s支持使用kubectl create configmap或者在kustomization.yaml中的 ConfigMap 生成器来创建 ConfigMap。\n使用 kubectl create configmap 创建 ConfigMap 可以使用 kubectl create configmap 命令基于目录、 文件或者字面值来创建 ConfigMap： kubectl create configmap \u0026lt;configmap_name\u0026gt; \u0026lt;数据源\u0026gt;\n其中，\u0026lt;configmap_name\u0026gt; 是为 ConfigMap 指定的名称，\u0026lt;数据源\u0026gt; 是要从中提取数据的目录、 文件或者字面值。ConfigMap 对象的名称必须是合法的 DNS 子域名。\n在基于文件来创建ConfigMap时，\u0026lt;数据源\u0026gt; 中的键名默认取自文件的基本名， 而对应的值则默认为文件的内容。\n可以使用kubectl describe configmaps \u0026lt;config_map_name\u0026gt;或者kubectl get configmaps获取有关 ConfigMap 的信息。\n基于目录创建ConfigMap 可以使用 kubectl create configmap 基于同一目录中的多个文件创建 ConfigMap。 当基于目录来创建 ConfigMap 时，kubectl 识别目录下文件名可以作为合法键名的文件， 并将这些文件打包到新的 ConfigMap 中。普通文件之外的所有目录项都会被忽略 （例如：子目录、符号链接、设备、管道等等）。\n说明： 用于创建 ConfigMap 的每个文件名必须由可接受的字符组成，即：字母（A 到 Z 和 a 到 z）、数字（0 到 9）、\u0026rsquo;-\u0026rsquo;、\u0026rsquo;_\u0026rsquo; 或 \u0026lsquo;.\u0026rsquo;。 如果在一个目录中使用 kubectl create configmap，而其中任一文件名包含不可接受的字符， 则 kubectl 命令可能会失败。\nkubectl 命令在遇到不合法的文件名时不会打印错误。\n创建本地目录：\n1 mkdir -p configure-pod-container/configmap/ 现在，下载示例的配置并创建 ConfigMap：\n1 2 3 4 5 6 # 将示例文件下载到 `configure-pod-container/configmap/` 目录 wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties # 创建 ConfigMap kubectl create configmap game-config --from-file=configure-pod-container/configmap/ 以上命令将 configure-pod-container/configmap 目录下的所有文件，也就是 game.properties 和 ui.properties 打包到 game-config ConfigMap 中。\n显示 ConfigMap 的详细信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 kubectl describe configmaps game-config Name: game-config Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: ---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: ---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice BinaryData ==== Events: \u0026lt;none\u0026gt; configure-pod-container/configmap/ 目录中的 game.properties 和 ui.properties 文件出现在 ConfigMap 的 data 部分。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 kubectl get configmaps game-config -o yaml rust@k8s1:~$ kubectl get configmaps game-config -o yaml apiVersion: v1 data: game.properties: |- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice kind: ConfigMap metadata: creationTimestamp: \u0026#34;2026-02-10T06:51:06Z\u0026#34; name: game-config namespace: default resourceVersion: \u0026#34;169466\u0026#34; uid: b379ed2c-575b-492e-8485-68ac4c385062 基于单个或多个文件创建ConfigMap, \u0026ndash;from-file指定单个文件即可基于单个文件创建ConfigMap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties rust@k8s1:~$ kubectl get configmaps game-config-2 -o yaml apiVersion: v1 data: game.properties: |- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 kind: ConfigMap metadata: creationTimestamp: \u0026#34;2026-02-10T07:00:06Z\u0026#34; name: game-config-2 namespace: default resourceVersion: \u0026#34;170295\u0026#34; uid: a5a08086-7e39-4763-b3aa-1285d96c28e9 多个\u0026ndash;from-file，每个指定单个文件即可实现基于多个文件创建ConfigMap\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties --from-file=configure-pod-container/configmap/ui.properties rust@k8s1:~$ kubectl delete configmap game-config-2 configmap \u0026#34;game-config-2\u0026#34; deleted rust@k8s1:~$ kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties --from-file=configure-pod-container/configmap/ui.properties configmap/game-config-2 created rust@k8s1:~$ kubectl get configmaps game-config-2 -o yaml apiVersion: v1 data: game.properties: |- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice kind: ConfigMap metadata: creationTimestamp: \u0026#34;2026-02-10T07:02:57Z\u0026#34; name: game-config-2 namespace: default resourceVersion: \u0026#34;170560\u0026#34; uid: c0db714a-11ee-4125-8dee-1f64b2986f6a 使用 \u0026ndash;from-env-file 选项基于 env 文件创建 ConfigMap 1 2 3 4 wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties kubectl create configmap game-config-env-file \\ --from-env-file=configure-pod-container/configmap/game-env-file.properties Env 文件包含环境变量列表。其中适用以下语法规则:\nEnv 文件中的每一行必须为 VAR=VAL 格式。 以＃开头的行（即注释）将被忽略。 空行将被忽略。 引号不会被特殊处理（即它们将成为 ConfigMap 值的一部分）。 1 2 3 4 5 enemies=aliens lives=3 allowed=\u0026#34;true\u0026#34; # 此注释和上方的空行将被忽略 使用env创建ConfigMap与基于普通文件创建语法类似，同样支持基于单个文件或多个文件创建ConfigMap。 不同的是创建出的ConfigMap的data里的配置参数格式不同。不再是文件名：配置参数信息的格式，而是直接提取文件内的参数信息，变成了参数名称：值\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2019-12-27T18:36:28Z name: game-config-env-file namespace: default resourceVersion: \u0026#34;809965\u0026#34; uid: d9d1ca5b-eb34-11e7-887b-42010a8002b8 data: allowed: \u0026#39;\u0026#34;true\u0026#34;\u0026#39; enemies: aliens lives: \u0026#34;3\u0026#34; 在使用 \u0026ndash;from-file 参数时，可以定义在 ConfigMap 的 data 部分出现键名， 而不是按默认行为使用文件名：\n1 2 3 kubectl create configmap game-config-3 --from-file=\u0026lt;我的键名\u0026gt;=\u0026lt;文件路径\u0026gt; kubectl create configmap game-config-3 --from-file=game-special-key=configure-pod-container/configmap/game.properties 基于字面值创建configmap, 用\u0026ndash;from-literal传入单个或多个字面值也可以创建configmap 1 kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm 得到了类似从env文件创建的config。\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2022-02-18T19:14:38Z name: special-config namespace: default resourceVersion: \u0026#34;651\u0026#34; uid: dadce046-d673-11e5-8cd0-68f728db1985 data: special.how: very special.type: charm 基于生成器创建configmap 可以基于生成器（Generators）创建 ConfigMap，然后将其应用于集群的 API 服务器上创建对象。 生成器应在目录内的kustomization.yaml中指定。\n基于kustomization.yaml生成器文件创建configmap\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 rust@k8s1:~$ cat \u0026lt;\u0026lt;EOF \u0026gt;./kustomization.yaml./kustomization.yaml configMapGenerator: - name: game-config-4 options: labels: game-config: config-4 files: - configure-pod-container/configmap/game.properties EOF rust@k8s1:~$ kubectl apply -k . configmap/game-config-4-tbg7c4gc77 created rust@k8s1:~$ kubectl get configmap NAME DATA AGE game-config 2 38m game-config-2 2 26m game-config-4-tbg7c4gc77 1 8s kube-root-ca.crt 1 8d rust@k8s1:~$ kubectl describe configmap game-config-4-tbg7c4gc77 Name: game-config-4-tbg7c4gc77 Namespace: default Labels: game-config=config-4 Annotations: \u0026lt;none\u0026gt; Data ==== game.properties: ---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 BinaryData ==== Events: \u0026lt;none\u0026gt; 在生成器文件的file字段，可以像kubectl create configmap那样指定字段值而不是用默认的文件名\n1 2 3 4 5 files: - configure-pod-container/configmap/game.properties files: - game-special-key=configure-pod-container/configmap/game.properties 生成器也能用字面值创建configmap,只需要在生成器文件里用literals,就像这样：\n1 2 3 4 5 configMapGenerator: - name: special-config-2 literals: - special.how=very - special.type=charm 使用配置文件创建configmap 第三种创建configmap的方式就是创建configmap类型的yaml文件，yaml文件类似上面两种方式创建的configmap输出的yaml格式的文件。\n1 2 3 4 5 6 7 apiVersion: v1 kind: ConfigMap metadata: name: env-config namespace: default data: log_level: INFO 再用kubectl apply -f \u0026lt;configmap_yaml\u0026gt;应用配置文件即可创建configmap。\n使用ConfigMap 使用configmap的数据定义容器环境变量 创建configmap。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very --- apiVersion: v1 kind: ConfigMap metadata: name: env-config namespace: default data: log_level: INFO 1 2 vim configmaps.yaml kubectl apply -f configmaps.yaml 在pod规约(spec)中通过env.valueFrom使用configmap:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: busybox:1.27.2 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] env: # 定义容器的环境变量 - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # ConfigMap 包含你要赋给 SPECIAL_LEVEL_KEY 的值 name: special-config # 指定与取值相关的键名 key: special.how # 多个环境变量对应多个值 - name: LOG_LEVEL valueFrom: configMapKeyRef: name: env-config key: log_level restartPolicy: Never pod启动后在pod详情里可以看到环境变量，使用了configmap。\n1 2 3 Environment: SPECIAL_LEVEL_KEY: \u0026lt;set to the key \u0026#39;special.how\u0026#39; of config map \u0026#39;special-config\u0026#39;\u0026gt; Optional: false LOG_LEVEL: \u0026lt;set to the key \u0026#39;log_level\u0026#39; of config map \u0026#39;env-config\u0026#39;\u0026gt; Optional: false 将ConfigMap中的所有键值对配置为容器环境变量 创建configmap,yaml文件如下：\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm 1 2 vim configmap-multikeys.yaml kubectl create -f configmap-multikeys.yaml 使用 envFrom 将所有 ConfigMap 的数据定义为容器环境变量，ConfigMap 中的键成为 Pod 中的环境变量名称。\n1 2 vim pod-configmap-envFrom.yaml kubectl apply -f pod-configmap-envFrom.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: busybox:1.27.2 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] envFrom: - configMapRef: name: special-config restartPolicy: Never 查看pod详情，发现容器的环境变量全部来自于configmap\n1 2 3 4 5 kubectl describe pod dapi-test-pod Environment Variables from: special-config ConfigMap Optional: false Environment: \u0026lt;none\u0026gt; 在 Pod 命令中使用 ConfigMap 定义的环境变量 可以使用 $(VAR_NAME)Kubernetes替换语法在容器的command和args属性中使用 ConfigMap 定义的环境变量。\n例如下面的pod配置文件里，在容器的command里就用$(SPECIAL_LEVEL_KEY) 使用了环境变量。也即是，通过环境变量使用configmap，再在命令里使用替换语法使用环境变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: busybox:1.27.2 command: [ \u0026#34;/bin/echo\u0026#34;, \u0026#34;$(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\u0026#34; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never 将 ConfigMap 数据添加到一个卷中 k8s支持用kubectl create configmap来从文件中创建configmap,也支持把configmap作为文件挂载到容器内部，就好像容器里有configmap的文件一样。\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm 创建configmap：\n1 2 vim configmap-multikeys.yaml kubectl apply -f configmap-multikeys.yaml 在 Pod 规约的 volumes 部分下添加 ConfigMap 名称。 这会将 ConfigMap 数据添加到 volumeMounts.mountPath 所指定的目录 （在本例中为 /etc/config）。\ncommand 部分列出了名称与 ConfigMap 中的键匹配的目录文件，查看pod日志（command执行情况），将会看到configmap作为文件挂到到容器内，并被ll命令展示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: busybox:1.27.2 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls /etc/config/\u0026#34; ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: # 提供包含要添加到容器中的文件的 ConfigMap 的名称 name: special-config restartPolicy: Never 1 2 3 4 5 6 7 8 9 10 vim pod-configmap-volume.yaml kubectl apply -f pod-configmap-volume.yaml rust@k8s1:~$ vim pod-configmap-volume.yaml rust@k8s1:~$ kubectl apply -f pod-configmap-volume.yaml pod/dapi-test-pod created rust@k8s1:~$ kubectl logs dapi-test-pod SPECIAL_LEVEL SPECIAL_TYPE 如果将容器的命令改为查看configmap文件(cat /etc/config/SPECIAL_LEVEL),日志中将输出对应文件的内容，也即是configmap中对应key的值。\n将 ConfigMap 数据添加到卷中的特定路径 上面的默认挂在configmap的key同名文件里。可以通过path修改数据挂载到卷中的路径。 例如下面这个pod就把SPECIAL_LEVEL数据挂载到了keys文件(/etc/config/keys)里。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: busybox:1.27.2 command: [ \u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cat /etc/config/keys\u0026#34; ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: special-config items: - key: SPECIAL_LEVEL path: keys restartPolicy: Never 1 2 3 4 5 6 7 8 9 10 11 12 kubectl delete pod dapi-test-pod vim pod-configmap-volume-specific-key.yaml kubectl apply -f pod-configmap-volume-specific-key.yaml rust@k8s1kubectl delete pod dapi-test-pod pod \u0026#34;dapi-test-pod\u0026#34; deleted rust@k8s1:~$ vim pod-configmap-volume-specific-key.yaml rust@k8s1:~$ kubectl apply -f pod-configmap-volume-specific-key.yaml pod/dapi-test-pod created rust@k8s1:~$ kubectl logs dapi-test-pod veryrust@k8s1:~$ ConfigMap原理介绍 说明：ConfigMap 应该引用属性文件，而不是替换它们。可以将 ConfigMap 理解为类似于 Linux /etc 目录及其内容的东西。例如，如果你基于 ConfigMap 创建 Kubernetes 卷，则 ConfigMap 中的每个数据项都由该数据卷中的某个独立的文件表示\n可以在 Pod 规约中将对ConfigMap的引用(env.valueFrom.configMapKeyRef)标记为可选（optional）。 如果 ConfigMap 不存在，则挂载的卷将为空。 如果 ConfigMap 存在，但引用的键不存在，则挂载点下的路径将不存在。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: busybox:1.27.2 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34;] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: a-config key: akey optional: true # 将环境变量标记为可选 restartPolicy: Never 也可以在 Pod 规约中将ConfigMap提供的卷和文件(volumes.configMap)标记为可选。 此时 Kubernetes 将总是为卷创建挂载路径，即使引用的 ConfigMap 或键不存在。 例如，以下 Pod 规约将所引用的ConfigMap 的卷标记为可选：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: busybox:1.27.2 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls /etc/config\u0026#34;] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: no-config optional: true # 将引用的 ConfigMap 的卷标记为可选 restartPolicy: Never 当已挂载的 ConfigMap 被更新时，所投射的内容最终也会被更新。 这适用于 Pod 启动后可选引用的 ConfigMap 重新出现的情况。\nKubelet 在每次定期同步时都会检查所挂载的 ConfigMap 是否是最新的。 然而，它使用其基于 TTL 机制的本地缓存来获取 ConfigMap 的当前值。 因此，从ConfigMap更新到新键映射到Pod的总延迟可能与 kubelet 同步周期（默认为 1 分钟）+ kubelet 中 ConfigMap 缓存的 TTL（默认为 1 分钟）一样长。 你可以通过更新 Pod 的一个注解来触发立即刷新。\n说明：使用 ConfigMap 作为 subPath 卷的容器将不会收到 ConfigMap 更新。\nconfigMap使用具有如下限制：\n在 Pod 规约中引用某个 ConfigMap 之前，必须先创建这个对象， 或者在 Pod 规约中将 ConfigMap 标记为 optional。 如果所引用的 ConfigMap 不存在，并且没有将应用标记为 optional 则 Pod 将无法启动。 同样，引用ConfigMap中不存在的主键也会令 Pod 无法启动，除非你将 Configmap 标记为 optional。 如果使用envFrom 来基于ConfigMap 定义环境变量，那么无效的键将被忽略。 Pod可以被启动，但无效名称将被记录在事件日志中（InvalidVariableNames）。 日志消息列出了每个被跳过的键。例如： 1 2 3 4 kubectl get events #输出与此类似： LASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 0s 0s 1 dapi-test-pod Pod Warning InvalidEnvironmentVariableNames {kubelet, 127.0.0.1} Keys [1badkey, 2alsobad] from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names. ConfigMap位于确定的名字空间中。每个ConfigMap只能被同一名字空间中的 Pod 引用。 Kubernetes不支持将ConfigMap用于静态 Pod（由特定节点上的kubelet守护进程直接管理的Pod） ","date":"2026-02-10T14:30:19+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A05_%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/","title":"K8s实践练习5_配置管理"},{"content":" 官网链接\n前置准备 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 创建命名空间 创建一个命名空间，以便将本练习中创建的资源与集群的其余部分隔离。\n1 kubectl create namespace mem-example 指定内存请求和限制 要为容器指定内存请求，需要在容器资源清单中包含resources: requests字段。同理，要指定内存限制，需包含resources: limits\n在本练习中，将创建一个拥有一个容器的 Pod。 容器将会请求100 MiB内存，并且内存会被限制在200 MiB以内。 这是Pod的配置文件(stress镜像从k8s官网拉不下来，daocloud上也没有，好在问ai发现华为云有)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: memory-demo namespace: mem-example spec: containers: - name: memory-demo-ctr image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/polinux/stress:latest resources: requests: memory: \u0026#34;100Mi\u0026#34; limits: memory: \u0026#34;200Mi\u0026#34; command: [\u0026#34;stress\u0026#34;] args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;150M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] 创建memory-request-limit.yaml文件，并应用。\n1 2 vim memory-request-limit.yaml kubectl apply -f memory-request-limit.yaml 配置文件的 args 部分提供了容器启动时的参数。 \u0026ldquo;\u0026ndash;vm-bytes\u0026rdquo;, \u0026ldquo;150M\u0026rdquo; 参数告知容器尝试分配 150 MiB 内存。\n1 2 3 4 5 6 7 # 查看pod是否已在运行 rust@k8s1:~$ kubectl get pod memory-demo --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo 1/1 Running 0 4m17s # 查看pod详细信息 kubectl get pod memory-demo --namespace=mem-example --output=yaml 结果显示pod申请了100MiB内存，限制最多使用200MiB内存。\n获取pod指标数据 下载metric-server官方yaml文件，修改metric-server镜像为国内镜像源，在 args 下面添加：\u0026ndash;kubelet-insecure-tls（解决连接 kubelet 的证书问题，自建集群里很常见）。\n1 2 3 4 wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml # 编辑components.yaml文件 vim components.yaml 大概是在140行左右，改成下面的内容：\n1 2 3 4 5 6 7 8 9 containers: - args: - --cert-dir=/tmp - --secure-port=10250 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.8.1 应用修改后的components.yaml，查看运行状态：\n1 2 3 4 5 6 7 8 9 10 11 kubectl apply -f components.yaml # 过几十秒后查看 kubectl get pod -n kube-system | grep metrics-server rust@k8s1:~$ kubectl get pod -n kube-system | grep metrics-server metrics-server-6468f846d6-9tggg 1/1 Running 0 11m # 如果pod没在运行，查看pod事件，定位故障原因 kubectl describe pod \u0026lt;metrics-server_name\u0026gt; -n kube-system 运行 kubectl top 命令，获取该 Pod 的指标数据：\n1 2 3 4 5 kubectl top pod memory-demo --namespace=mem-example rust@k8s1:~$ kubectl top pod memory-demo --namespace=mem-example NAME CPU(cores) MEMORY(bytes) memory-demo 79m 150Mi 删除pod\n1 kubectl delete pod memory-demo --namespace=mem-example 超出容器限制的内存 当节点拥有足够的可用内存时，容器可以使用其请求的内存。 但是，容器不允许使用超过其限制的内存。 如果容器分配的内存超过其限制，该容器会成为被终止的候选容器。 如果容器继续消耗超出其限制的内存，则终止容器。如果终止的容器可以被重启，则 kubelet 会重新启动它，就像其他任何类型的运行时失败一样。\n在本练习中，将创建一个 Pod，尝试分配超出其限制的内存。\n这是一个 Pod 的配置文件，其拥有一个容器，该容器的内存请求为 50 MiB，内存限制为 100 MiB：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: memory-demo-2 namespace: mem-example spec: containers: - name: memory-demo-2-ctr image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/polinux/stress:latest imagePullPolicy: IfNotPresent resources: requests: memory: \u0026#34;50Mi\u0026#34; limits: memory: \u0026#34;100Mi\u0026#34; command: [\u0026#34;stress\u0026#34;] args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;250M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] 创建编辑配置文件，应用配置文件。\n1 2 vim memory-request-limit-2.yaml kubectl apply -f memory-request-limit-2.yaml 查看 Pod详细信息,发现pod由于使用内存超出限制被杀掉\n1 2 3 rust@k8s1:~$ kubectl get pod memory-demo-2 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-2 0/1 OOMKilled 3 (36s ago) 52s 以yaml格式输出详细信息，输出结果显示由于内存溢出，pod已被杀死\n1 2 3 4 5 6 7 8 9 10 kubectl get pod memory-demo-2 --output=yaml --namespace=mem-example lastState: terminated: containerID: containerd://376e96fffa7a75aa31aef9da861cfd5a25bc87d17a67c02a507a3897e2a77840 exitCode: 137 finishedAt: \u0026#34;2026-02-10T03:11:25Z\u0026#34; reason: OOMKilled startedAt: \u0026#34;2026-02-10T03:11:25Z\u0026#34; 查看pod事件，可以发现pod不断被重启：\n1 2 3 4 5 6 7 8 9 10 kubectl describe pod memory-demo-2 --namespace=mem-example Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m53s default-scheduler Successfully assigned mem-example/memory-demo-2 to k8s2 Normal Pulled 3m18s (x5 over 4m52s) kubelet Container image \u0026#34;swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/polinux/stress:latest\u0026#34; already present on machine Normal Created 3m18s (x5 over 4m52s) kubelet Created container: memory-demo-2-ctr Normal Started 3m18s (x5 over 4m52s) kubelet Started container memory-demo-2-ctr Warning BackOff 3m3s (x10 over 4m51s) kubelet Back-off restarting failed container memory-demo-2-ctr in pod memory-demo-2_mem-example(5d9cf5d7-2d00-45f4-af71-bd64f4002eab) 删除 Pod\n1 kubectl delete pod memory-demo-2 --namespace=mem-example 超过整个节点容量的内存 内存请求和限制是与容器关联的，而Pod也有内存请求和限制。Pod的内存请求是Pod中所有容器的内存请求之和，Pod 的内存限制是Pod中所有容器的内存限制之和。\nPod的调度基于请求。只有当节点拥有足够满足Pod内存请求的内存时，才会将Pod调度至节点上运行。\n在本练习中，将创建一个 Pod，其内存请求超过了集群中的任意一个节点所拥有的内存。\n这是该 Pod 的配置文件，其拥有一个请求 1000 GiB 内存的容器，这超过了集群中任何节点的容量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: memory-demo-3 namespace: mem-example spec: containers: - name: memory-demo-3-ctr image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/polinux/stress:latest imagePullPolicy: IfNotPresent resources: requests: memory: \u0026#34;1000Gi\u0026#34; limits: memory: \u0026#34;1000Gi\u0026#34; command: [\u0026#34;stress\u0026#34;] args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;150M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] 创建填入上面的内容并应用配置文件。\n1 2 vim memory-request-limit-3.yaml kubectl apply -f memory-request-limit-3.yaml 查看pod状态：\n1 2 3 rust@k8s1:~$ kubectl get pod memory-demo-3 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-3 0/1 Pending 0 13s 输出结果显示：Pod 处于 PENDING 状态。 这意味着，该 Pod 没有被调度至任何节点上运行，并且它会无限期的保持该状态\n查看Pod事件,输出结果显示由于节点内存不足，该容器无法被调度。\n1 2 3 4 5 6 7 kubectl describe pod memory-demo-3 -n mem-example Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 2m42s default-scheduler 0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient memory. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod. 删除 Pod：\n1 kubectl delete pod memory-demo-3 --namespace=mem-example 原理介绍与补充 如果没有为一个容器指定内存限制，则自动遵循以下情况之一：\n容器可无限制地使用内存。容器可以使用其所在节点所有的可用内存， 进而可能导致该节点调用 OOM Killer。 此外，如果发生 OOM Kill，没有资源限制的容器更可能被杀掉。\n运行的容器所在命名空间有默认的内存限制，那么该容器会被自动分配默认限制。 集群管理员可用使用LimitRange来指定默认的内存限制。\n通过为集群中运行的容器配置内存请求和限制，可以有效利用集群节点上可用的内存资源。 通过将 Pod 的内存请求保持在较低水平，可以更好地安排 Pod 调度。 通过让内存限制大于内存请求，可以实现：\nPod 可以进行一些突发活动，从而更好的利用可用内存。 Pod 在突发活动期间，可使用的内存被限制为合理的数量 CPU的申请与限制和内存相似。需要注意的是，CPU和内存的单位：\n内存资源的基本单位是字节(byte)，可以使用这些后缀将内存表示为纯整数或定点整数：E、P、T、G、M、K、Ei、Pi、Ti、Gi、Mi、Ki CPU资源的基本单位是毫核(millicore)，1000m =1核。CPU并不直接对应物理CPU核心数量，而是表示调度器分配给容器的CPU时间比例。容器进程可能被调度到服务器的任意CPU上执行（优先NUMA节点）。只有当节点开启cpu-manager-policy=static，Pod 的request == limit且为整核，才会获得独占 CPU（pin 到特定核）。 底层实现基于Linux的CFS(Completely Fair Scheduler)+cgroup v1/v2来限制CPU配额(quota)和周期(period)。 查看k8s节点cpu总量：kubectl describe node \u0026lt;node_name\u0026gt; | grep -A 5 Allocatable\n补充：容器的本质就是Linux namespace+Linux cgroup。namespace实现隔离，让进程“看不见”其他进程、网络文件系统等，好像给进程制造一个独立房间。cgroup实现限制，控制进程能用多少资源，好像给房间装电表水表，超过限额就断。\ncgroup有以下功能：\n资源限制(limit):限制一个进程组最多能用多少CPU、内存等。例如某个容器最多只能用512MB内存 优先级分配(prioritization):给不同组分配不同的CPU或磁盘I/O优先级 资源统计(accounting):记录某个组用了多少资源，用于计费或监控 进程控制(control):可以冻结/恢复/杀死整个组的进程 cgroup有v1和v2两个版本，Linux4.5支持v2。k8s从v1.25默认v2，v1.30+仅支持v2。\ncgroupv1:每种资源（CPU、内存等）由独立的子系统（subsystem）管理。配置分散在不同目录，管理复杂 cgroupv2:统一层级结构，所有资源在一个树形结构中管理，更安全，更一致，避免了v1的一些竟态问题\nNUMA(NON-Uniform Memory Access 非统一内存访问)补充介绍：\n传统的多处理器系统中，所有CPU共享同一块内存总线，这会导致“内存墙问题”————CPU越多，争抢内存带宽就越严重。 而NUMA架构将系统划分为多个节点(Node)。每个NUMA节点通常包含：\n一个或多个CPU核心（或一个CPU插槽） 与之直接相连的本地内存（Local Memory） 可能还有自己的I/O总线 MUMA特点：访问本地内存块，当一个CPU核心访问自己所在NUMA节点内的内存时，速度最快。 访问远程内存慢，当它需要访问其他NUMA节点的内存时，必须通过节点间的连接总线，如Intel的QPI或AMD的Infinity Fabric。\n因此，为了获得最佳性能，操作系统和应用程序会尽量将进程调度到其数据所在的NUMA节点上运行，这被称为NUMA节点亲和性。\nSMT(Simultaneous Multithreading) 同时多线程介绍： 一种CPU微架构技术，允许一个物理核心同时\u0026quot;假装\u0026quot;成两个（或更多）逻辑核心。 这个物理核心会同时从两个分发到它上的线程中取出指令。 最常见的的SMT实现就是Intel的超线程技术(Hyper-Threading)。这也是为什么笔记本常见四核八线程，八核十六线程而不是四核四线程。\n清理现场 删除命名空间。下面的命令会删除根据这个任务创建的所有 Pod：\n1 kubectl delete namespace mem-example ","date":"2026-02-10T09:39:00+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A04_%E5%86%85%E5%AD%98%E8%B5%84%E6%BA%90%E7%94%B3%E8%AF%B7%E4%B8%8E%E9%99%90%E5%88%B6/","title":"K8s实践练习4_内存资源申请与限制"},{"content":" 转载自黑马程序员的资料\n由于每台服务器的运行环境不同，写好的安装流程、部署脚本并不一定在每个服务器都能正常运行。Docker能避免部署对服务器环境的依赖，减少复杂的部署流程，解决程序“在我这可以运行，但是在你那不行”的问题。\n快速上手 使用docker部署一个MySQL\n1 2 3 4 5 6 docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123 \\ mysql MySQL安装完毕！通过任意客户端工具即可连接到MySQL.\n当执行命令后，Docker做的第一件事情，是去自动搜索并下载了MySQL，然后会自动运行MySQL，完全不用插手，非常方便。\n而且，这种安装方式你完全不用考虑运行的操作系统环境，它不仅仅在CentOS系统是这样，在Ubuntu系统、macOS系统、甚至是装了WSL的Windows下，都可以使用这条命令来安装MySQL。 要知道，不同操作系统下其安装包、运行环境是都不相同的！如果是手动安装，必须手动解决安装包不同、环境不同的、配置不同的问题！\n而使用Docker，这些完全不用考虑。就是因为Docker会自动搜索并下载MySQL。注意：这里下载的不是安装包，而是镜像。镜像中不仅包含了MySQL本身，还包含了其运行所需要的环境、配置、系统级函数库。因此它在运行时就有自己独立的环境，就可以跨系统运行，也不需要手动再次配置环境了。这套独立运行的隔离环境我们称为容器。\n说明：\n镜像：英文是image 容器：英文是container 因此，Docker安装软件的过程，就是自动搜索下载镜像，然后创建并运行容器的过程。\nDocker会根据命令中的镜像名称自动搜索并下载镜像，那么问题来了，它是去哪里搜索和下载镜像的呢？这些镜像又是谁制作的呢？\nDocker官方提供了一个专门管理、存储镜像的网站，并对外开放了镜像上传、下载的权利。Docker官方提供了一些基础镜像，然后各大软件公司又在基础镜像基础上，制作了自家软件的镜像，全部都存放在这个网站。这个网站就成了Docker镜像交流的社区： https://hub.docker.com/ 基本上我们常用的各种软件都能在这个网站上找到，我们甚至可以自己制作镜像上传上去。\n像这种提供存储、管理Docker镜像的服务器，被称为DockerRegistry，可以翻译为镜像仓库。DockerHub网站是官方仓库，阿里云、华为云会提供一些第三方仓库，我们也可以自己搭建私有的镜像仓库。 官方仓库在国外，下载速度较慢，一般我们都会使用第三方仓库提供的镜像加速功能，提高下载速度。而企业内部的机密项目，往往会采用私有镜像仓库。\n总之，镜像的来源有两种：\n基于官方基础镜像自己制作 直接去DockerRegistry下载 总结一下： Docker本身包含一个后台服务，我们可以利用Docker命令告诉Docker服务，帮助我们快速部署指定的应用。Docker服务部署应用时，首先要去搜索并下载应用对应的镜像，然后根据镜像创建并允许容器，应用就部署完成了。\n命令解读：\ndocker run -d ：创建并运行一个容器，-d则是让容器以后台进程运行 \u0026ndash;name mysql : 给容器起个名字叫mysql，你可以叫别的 -p 3306:3306 : 设置端口映射。 容器是隔离环境，外界不可访问。但是可以将宿主机端口映射容器内到端口，当访问宿主机指定端口时，就是在访问容器内的端口了。 容器内端口往往是由容器内的进程决定，例如MySQL进程默认端口是3306，因此容器内端口一定是3306；而宿主机端口则可以任意指定，一般与容器内保持一致。 格式： -p 宿主机端口:容器内端口，示例中就是将宿主机的3306映射到容器内的3306端口 -e TZ=Asia/Shanghai : 配置容器内进程运行时的一些参数 格式：-e KEY=VALUE，KEY和VALUE都由容器内进程决定 案例中，TZ=Asia/Shanghai是设置时区；MYSQL_ROOT_PASSWORD=123是设置MySQL默认密码 mysql : 设置镜像名称，Docker会根据这个名字搜索并下载镜像 格式：REPOSITORY:TAG，例如mysql:8.0，其中REPOSITORY可以理解为镜像名，TAG是版本号 在未指定TAG的情况下，默认是最新版本，也就是mysql:latest 镜像的名称不是随意的，而是要到DockerRegistry中寻找，镜像运行时的配置也不是随意的，要参考镜像的帮助文档，这些在DockerHub网站或者软件的官方网站中都能找到。\n如果我们要安装其它软件，也可以到DockerRegistry中寻找对应的镜像名称和版本，阅读相关配置即可。\n基础命令 Docker官方文档\n常见命令\n命令 说明 文档地址 docker pull 拉取镜像 docker pull docker push 推送镜像到DockerRegistry docker push docker images 查看本地镜像 docker images docker rmi 删除本地镜像 docker rmi docker run 创建并运行容器（不能重复创建） docker run docker stop 停止指定容器 docker stop docker start 启动指定容器 docker start docker restart 重新启动容器 docker restart docker rm 删除指定容器 docker rm docker ps 查看容器 docker ps docker logs 查看容器运行日志 docker logs docker exec 进入容器 docker exec docker save 保存镜像到本地压缩文件 docker save docker load 加载本地压缩文件到镜像 docker load docker inspect 查看容器详细信息 docker inspect 这些命令的关系如下图所示： 默认情况下，每次重启虚拟机我们都需要手动启动Docker和Docker中的容器。通过命令可以实现开机自启：\n1 2 3 4 5 # Docker开机自启 systemctl enable docker # Docker容器开机自启 docker update --restart=always [容器名/容器id] 以nginx为例演示上面的命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 第1步，去DockerHub查看nginx镜像仓库及相关信息 # 第2步，拉取Nginx镜像 docker pull nginx # 第3步，查看镜像 docker images # 结果如下： REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest 605c77e624dd 16 months ago 141MB mysql latest 3218b38490ce 17 months ago 516MB # 第4步，创建并允许Nginx容器 docker run -d --name nginx -p 80:80 nginx # 第5步，查看运行中容器 docker ps # 也可以加格式化方式访问，格式会更加清爽 docker ps --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34; # 第6步，访问网页，地址：http://虚拟机地址 # 第7步，停止容器 docker stop nginx # 第8步，查看所有容器 docker ps -a --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34; # 第9步，再次启动nginx容器 docker start nginx # 第10步，再次查看容器 docker ps --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34; # 第11步，查看容器详细信息 docker inspect nginx # 第12步，进入容器,查看容器内目录 docker exec -it nginx bash # 或者，可以进入MySQL docker exec -it mysql mysql -uroot -p # 第13步，删除容器 docker rm nginx # 发现无法删除，因为容器运行中，强制删除容器 docker rm -f nginx linux里可以用alias命令给常用Docker命令起别名，方便我们访问：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 修改/root/.bashrc文件 vi /root/.bashrc 内容如下： # .bashrc # User specific aliases and functions alias rm=\u0026#39;rm -i\u0026#39; alias cp=\u0026#39;cp -i\u0026#39; alias mv=\u0026#39;mv -i\u0026#39; alias dps=\u0026#39;docker ps --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34;\u0026#39; alias dis=\u0026#39;docker images\u0026#39; # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi 数据卷 容器是隔离环境，容器内程序的文件、配置、运行时产生的容器都在容器内部，要读写容器内的文件非常不方便。下面有几个问题：\n如果要升级MySQL版本，需要销毁旧容器，那么数据岂不是跟着被销毁了？ MySQL、Nginx容器运行后，如果我要修改其中的某些配置该怎么办？ 我想要让Nginx代理我的静态资源怎么办？ 因此，容器提供程序的运行环境，但是程序运行产生的数据、程序运行依赖的配置都应该与容器解耦。\n数据卷（volume）是一个虚拟目录，是容器内目录与宿主机目录之间映射的桥梁。 以Nginx为例，我们知道Nginx中有两个关键的目录：\nhtml：放置一些静态资源 conf：放置配置文件 如果我们要让Nginx代理我们的静态资源，最好是放到html目录；如果我们要修改Nginx的配置，最好是找到conf下的nginx.conf文件。\n但遗憾的是，容器运行的Nginx所有的文件都在容器内部。所以我们必须利用数据卷将两个目录与宿主机目录关联，方便我们操作。如图： 在上图中：\n我们创建了两个数据卷：conf、html Nginx容器内部的conf目录和html目录分别与两个数据卷关联。 而数据卷conf和html分别指向了宿主机的/var/lib/docker/volumes/conf/_data目录和/var/lib/docker/volumes/html/_data目录 这样，容器内的conf和html目录就与宿主机的conf和html目录关联起来，这被称为挂载。此时，我们操作宿主机的/var/lib/docker/volumes/html/_data就是在操作容器内的/usr/share/nginx/html/_data目录。只要我们将静态资源放入宿主机对应目录，就可以被Nginx代理了\n小提示： /var/lib/docker/volumes这个目录就是默认的存放所有容器数据卷的目录，其下再根据数据卷名称创建新目录，格式为/数据卷名/_data。\n为什么不让容器目录直接指向宿主机目录呢？\n因为直接指向宿主机目录就与宿主机强耦合了，如果切换了环境，宿主机目录就可能发生改变了。由于容器一旦创建，目录挂载就无法修改，这样容器就无法正常工作了。 但是容器指向数据卷，一个逻辑名称，而数据卷再指向宿主机目录，就不存在强耦合。如果宿主机目录发生改变，只要改变数据卷与宿主机目录之间的映射关系即可。 不过，我们通过由于数据卷目录比较深，不好寻找，通常我们也允许让容器直接与宿主机目录挂载而不使用数据卷，具体参考后文。\n数据卷常用命令：\n命令 说明 文档地址 docker volume create 创建数据卷 docker volume create docker volume ls 查看所有数据卷 docker volume ls docker volume rm 删除指定数据卷 docker volume rm docker volume inspect 查看某个数据卷的详情 docker volume inspect docker volume prune 清除数据卷 docker volume prune 注意：容器与数据卷的挂载要在创建容器时配置，对于创建好的容器，是不能设置数据卷的。而且创建容器的过程中，数据卷会自动创建。\n教学演示环节：演示一下nginx的html目录挂载。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # 1.首先创建容器并指定数据卷，注意通过 -v 参数来指定数据卷 docker run -d --name nginx -p 80:80 -v html:/usr/share/nginx/html nginx # 2.然后查看数据卷 docker volume ls # 结果 DRIVER VOLUME NAME local 29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f local html # 3.查看数据卷详情 docker volume inspect html # 结果 [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2024-05-17T19:57:08+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/html/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;html\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] # 4.查看/var/lib/docker/volumes/html/_data目录 ll /var/lib/docker/volumes/html/_data # 可以看到与nginx的html目录内容一样，结果如下： 总用量 8 -rw-r--r--. 1 root root 497 12月 28 2021 50x.html -rw-r--r--. 1 root root 615 12月 28 2021 index.html # 5.进入该目录，并随意修改index.html内容 cd /var/lib/docker/volumes/html/_data vi index.html # 6.打开页面，查看效果 # 7.进入容器内部，查看/usr/share/nginx/html目录内的文件是否变化 docker exec -it nginx bash 教学演示环节：演示一下MySQL的匿名数据卷\n1 2 3 # 查看MySQL容器详细信息 docker inspect mysql # 关注其中.Config.Volumes部分和.Mounts部分 可以发现.Config.Volumes里这个容器声明了一个本地目录，需要挂载数据卷，但是数据卷未定义。这就是匿名卷。\n1 2 3 4 5 6 7 8 9 { \u0026#34;Config\u0026#34;: { // ... 略 \u0026#34;Volumes\u0026#34;: { \u0026#34;/var/lib/mysql\u0026#34;: {} } // ... 略 } } 然后，再看结果中的.Mounts部分\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/var/lib/mysql\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, } ] } 可以发现，其中有几个关键属性：\nName：数据卷名称。由于定义容器未设置容器名，这里的就是匿名卷自动生成的名字，一串hash值。 Source：宿主机目录 Destination : 容器内的目录 上述配置是将容器内的/var/lib/mysql这个目录，与数据卷29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f挂载。于是在宿主机中就有了/var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data这个目录。这就是匿名数据卷对应的目录，其使用方式与普通数据卷没有差别。\n接下来，可以查看该目录下的MySQL的data文件：\nls -l /var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data\n注意：每一个不同的镜像，将来创建容器后内部有哪些目录可以挂载，可以参考DockerHub对应的页面\n可以发现，数据卷的目录结构较深，如果我们去操作数据卷目录会不太方便。在很多情况下，我们会直接将容器目录与宿主机指定目录挂载。挂载语法与数据卷类似：\n1 2 3 4 # 挂载本地目录 -v 本地目录:容器内目录 # 挂载本地文件 -v 本地文件:容器内文件 注意：本地目录或文件必须以 / 或 ./开头，如果直接以名字开头，会被识别为数据卷名而非本地目录名。\n例如：\n1 2 -v mysql:/var/lib/mysql # 会被识别为一个数据卷叫mysql，运行时会自动创建这个数据卷 -v ./mysql:/var/lib/mysql # 会被识别为当前目录下的mysql目录，运行时如果不存在会创建目录 镜像 前面我们一直在使用别人准备好的镜像，那如果我要部署一个Java项目，把它打包为一个镜像该怎么做呢？\n要想自己构建镜像，必须先了解镜像的结构。\n镜像之所以能让我们快速跨操作系统部署应用而忽略其运行环境、配置，就是因为镜像中包含了程序运行需要的系统函数库、环境、配置、依赖。\n因此，自定义镜像本质就是依次准备好程序运行的基础环境、依赖、应用本身、运行配置等文件，并且打包而成。\n举个例子，我们要从0部署一个Java应用，大概流程是这样：\n准备一个linux服务（CentOS或者Ubuntu均可） 安装并配置JDK 上传Jar包 运行jar包 那因此，我们打包镜像也是分成这么几步：\n准备Linux运行环境（java项目并不需要完整的操作系统，仅仅是基础运行环境即可） 安装并配置JDK 拷贝jar包 配置启动脚本 上述步骤中的每一次操作其实都是在生产一些文件（系统运行环境、函数库、配置最终都是磁盘文件），所以镜像就是一堆文件的集合。\n但需要注意的是，镜像文件不是随意堆放的，而是按照操作的步骤分层叠加而成，每一层形成的文件都会单独打包并标记一个唯一id，称为Layer（层）。这样，如果我们构建时用到的某些层其他人已经制作过，就可以直接拷贝使用这些层，而不用重复制作。\n例如，第一步中需要的Linux运行环境，通用性就很强，所以Docker官方就制作了这样的只包含Linux运行环境的镜像。我们在制作java镜像时，就无需重复制作，直接使用Docker官方提供的CentOS或Ubuntu镜像作为基础镜像。然后再搭建其它层即可，这样逐层搭建，最终整个Java项目的镜像结构如图所示：\n由于制作镜像的过程中，需要逐层处理和打包，比较复杂，所以Docker就提供了自动打包镜像的功能。我们只需要将打包的过程，每一层要做的事情用固定的语法写下来，交给Docker去执行即可。\n而这种记录镜像结构的文件就称为Dockerfile，其对应的语法可以参考官方文档\n指令 说明 示例 FROM 指定基础镜像 FROM centos:6 ENV 设置环境变量，可在后面指令使用 ENV key value COPY 拷贝本地文件到镜像的指定目录 COPY ./xx.jar /tmp/app.jar RUN 执行Linux的shell命令，一般是安装过程的命令 RUN yum install gcc EXPOSE 指定容器运行时监听的端口，是给镜像使用者看的 EXPOSE 8080 ENTRYPOINT 镜像中应用的启动命令，容器运行时调用 ENTRYPOINT java -jar xx.jar 例如，要基于Ubuntu镜像来构建一个Java应用，其Dockerfile内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 指定基础镜像 FROM ubuntu:16.04 # 配置环境变量，JDK的安装目录、容器内时区 ENV JAVA_DIR=/usr/local ENV TZ=Asia/Shanghai # 拷贝jdk和java项目的包 COPY ./jdk8.tar.gz $JAVA_DIR/ COPY ./docker-demo.jar /tmp/app.jar # 设定时区 RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone # 安装JDK RUN cd $JAVA_DIR \\ \u0026amp;\u0026amp; tar -xf ./jdk8.tar.gz \\ \u0026amp;\u0026amp; mv ./jdk1.8.0_144 ./java8 # 配置环境变量 ENV JAVA_HOME=$JAVA_DIR/java8 ENV PATH=$PATH:$JAVA_HOME/bin # 指定项目监听的端口 EXPOSE 8080 # 入口，java项目的启动命令 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/app.jar\u0026#34;] 学习工作中会有很多java项目需要打包为镜像，他们都需要Linux系统环境、JDK环境这两层，只有上面的3层不同（因为jar包不同）。如果每次制作java镜像都重复制作前两层镜像，会很麻烦。\n所以，就有人提供了基础的系统加JDK环境，我们在此基础上制作java镜像，就可以省去JDK的配置了：\n1 2 3 4 5 6 7 8 9 # 基础镜像 FROM openjdk:11.0-jre-buster # 设定时区 ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone # 拷贝jar包 COPY docker-demo.jar /app.jar # 入口 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/app.jar\u0026#34;] 准备好jar包和dockerfile之后，就可以利用命令来构建镜像了\n1 2 3 4 # 进入镜像目录 cd /root/demo # 开始构建 docker build -t docker-demo:1.0 . 命令说明：\ndocker build : 就是构建一个docker镜像 -t docker-demo:1.0 ：-t参数是指定镜像的名称（repository和tag） . : 最后的点是指构建时Dockerfile所在路径，.代表当前目录，也可以直接指定Dockerfile目录： docker build -t docker-demo:1.0 /root/demo 查看镜像列表：\n1 2 3 4 5 6 7 # 查看镜像列表： docker images # 结果 REPOSITORY TAG IMAGE ID CREATED SIZE docker-demo 1.0 d6ab0b9e64b9 27 minutes ago 327MB nginx latest 605c77e624dd 16 months ago 141MB mysql latest 3218b38490ce 17 months ago 516MB 尝试运行镜像：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 1.创建并运行容器 docker run -d --name dd -p 8080:8080 docker-demo:1.0 # 2.查看容器 dps # 结果 CONTAINER ID IMAGE PORTS STATUS NAMES 78a000447b49 docker-demo:1.0 0.0.0.0:8080-\u0026gt;8080/tcp, :::8090-\u0026gt;8090/tcp Up 2 seconds dd f63cfead8502 mysql 0.0.0.0:3306-\u0026gt;3306/tcp, :::3306-\u0026gt;3306/tcp, 33060/tcp Up 2 hours mysql # 3.访问 curl localhost:8080/hello/count # 结果： \u0026lt;h5\u0026gt;欢迎访问黑马商城, 这是您第1次访问\u0026lt;h5\u0026gt; 网络 创建容器后，容器之间能否互相访问呢？（Java项目往往需要访问其它各种中间件，例如MySQL、Redis等）\n查看下MySQL容器的详细信息，重点关注其中的网络IP地址：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 1.用基本命令，寻找Networks.bridge.IPAddress属性 docker inspect mysql # 也可以使用format过滤结果 docker inspect --format=\u0026#39;{{range .NetworkSettings.Networks}}{{println .IPAddress}}{{end}}\u0026#39; mysql # 得到IP地址如下： 172.17.0.2 # 2.然后通过命令进入dd容器 docker exec -it dd bash # 3.在容器内，通过ping命令测试网络 ping 172.17.0.2 # 结果 PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data. 64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.053 ms 64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.059 ms 64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.058 ms 发现可以互联，没有问题。\n但是，容器的网络IP其实是一个虚拟的IP，其值并不固定与某一个容器绑定，如果我们在开发时写死某个IP，而在部署时很可能MySQL容器的IP会发生变化，连接会失败。\n所以，必须借助于docker的网络功能来解决这个问题，官方文档\n命令 说明 文档地址 docker network create 创建一个网络 docker network create docker network ls 查看所有网络 docker network ls docker network rm 删除指定网络 docker network rm docker network prune 清除未使用的网络 docker network prune docker network connect 使指定容器连接加入某网络 docker network connect docker network disconnect 使指定容器连接离开某网络 docker network disconnect docker network inspect 查看网络详细信息 docker network inspect 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 1.首先通过命令创建一个网络 docker network create hmall # 2.然后查看网络 docker network ls # 结果： NETWORK ID NAME DRIVER SCOPE 639bc44d0a87 bridge bridge local 403f16ec62a2 hmall bridge local 0dc0f72a0fbb host host local cd8d3e8df47b none null local # 其中，除了hmall以外，其它都是默认的网络 # 3.让dd和mysql都加入该网络，注意，在加入网络时可以通过--alias给容器起别名 # 这样该网络内的其它容器可以用别名互相访问！ # 3.1.mysql容器，指定别名为db，另外每一个容器都有一个别名是容器名 docker network connect hmall mysql --alias db # 3.2.db容器，也就是我们的java项目 docker network connect hmall dd # 4.进入dd容器，尝试利用别名访问db # 4.1.进入容器 docker exec -it dd bash # 4.2.用db别名访问 ping db # 结果 PING db (172.18.0.2) 56(84) bytes of data. 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=1 ttl=64 time=0.070 ms 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=2 ttl=64 time=0.056 ms # 4.3.用容器名访问 ping mysql # 结果： PING mysql (172.18.0.2) 56(84) bytes of data. 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=1 ttl=64 time=0.044 ms 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=2 ttl=64 time=0.054 ms 总结：\n在自定义网络中，可以给容器起多个别名，默认的别名是容器名本身 在同一个自定义网络中的容器，可以通过别名互相访问 DockerCompose 部署一个简单的java项目，其中包含3个容器：\nMySQL Nginx Java项目 而稍微复杂的项目，其中还会有各种各样的其它中间件，需要部署的东西远不止3个。如果还像之前那样手动的逐一部署，就太麻烦了。 而Docker Compose就可以实现多个相互关联的Docker容器的快速部署。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器。\ndocker-compose.yml文件的基本语法可以参考官方文档\ndocker-compose文件中可以定义多个相互关联的应用容器，每一个应用容器被称为一个服务（service）。由于service就是在定义某个应用的运行时参数，因此与docker run参数非常相似。 举例来说，用docker run部署MySQL的命令如下：\n1 2 3 4 5 6 7 8 9 10 docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123 \\ -v ./mysql/data:/var/lib/mysql \\ -v ./mysql/conf:/etc/mysql/conf.d \\ -v ./mysql/init:/docker-entrypoint-initdb.d \\ --network hmall mysql 如果用docker-compose.yml文件来定义，就是这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 version: \u0026#34;3.8\u0026#34; services: mysql: image: mysql container_name: mysql ports: - \u0026#34;3306:3306\u0026#34; environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123 volumes: - \u0026#34;./mysql/conf:/etc/mysql/conf.d\u0026#34; - \u0026#34;./mysql/data:/var/lib/mysql\u0026#34; networks: - new networks: new: name: hmall 对比如下：\ndocker run 参数 docker compose 指令 说明 \u0026ndash;name container_name 容器名称 -p ports 端口映射 -e environment 环境变量 -v volumes 数据卷配置 \u0026ndash;network networks 网络 编写好docker-compose.yml文件，就可以部署项目了。常见的命令\ndocker compose [OPTIONS] [COMMAND]\n其中，OPTIONS和COMMAND都是可选参数，比较常见的有：\n类型 参数或指令 说明 Options -f 指定compose文件的路径和名称 -p 指定project名称。project就是当前compose文件中设置的多个service的集合，是逻辑概念 Commands up 创建并启动所有service容器 down 停止并移除所有容器、网络 ps 列出所有启动的容器 logs 查看指定容器的日志 stop 停止容器 start 启动容器 restart 重启容器 top 查看运行的进程 exec 在指定的运行中容器中执行命令 ","date":"2026-02-07T15:59:20+08:00","permalink":"https://rusthx.github.io/p/docker%E5%8E%9F%E7%90%86%E4%B8%8E%E5%91%BD%E4%BB%A4/","title":"Docker原理与命令"},{"content":" 官网链接\n本教程会描述如何创建前端（Frontend）微服务和后端（Backend）微服务。后端微服务是一个 hello 欢迎程序。 前端通过 nginx 和一个 Kubernetes 服务（service）暴露后端所提供的服务。\n教程目标 使用部署对象（Deployment object）创建并运行一个 hello 后端微服务 使用一个 Service 对象将请求流量发送到后端微服务的多个副本 同样使用一个 Deployment 对象创建并运行一个 nginx 前端微服务 配置前端微服务将请求流量发送到后端微服务 使用 type=NodePort 的 Service 对象将前端微服务暴露到集群外部\n注意：在大多数本地/自建 K8s 环境（如 minikube、kubeadm、裸机等）里，集群没有云厂商的负载均衡器控制器来分配外部 IP，LoadBalancer 类型 Service的EXTERNAL-IP会长期 。我的环境是本地建的三台Ubuntu虚拟机里的k8s集群，所以前端service的type改成了NodePort。本地想用loadBalancer需要部署 MetalLB（让它变成“云式”LoadBalancer）或者进阶使用Ingress(集群只需一个公网/NodePort 入口)\n前置准备 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 准备镜像 镜像下载 官网演示里用的后端镜像和前端镜像阿里云并没有同步，好在问ai发现docker仓库里有个用户上传了镜像。\n这里拉镜像有两种方式，一种就是找个能科学上网的机器，把k8s上的镜像拉下来（不用阿里云镜像源）；另一种就是从docker上把镜像拉下来再导到k8s集群里。我用的第二种方式（由于镜像文件太大，放不进蓝奏云里，放百度网盘下载又很慢，而且最近百度云盘才被爆出后台扫盘，所以不建议用百度云盘）。文中会给出具体命令，读者就八仙过海，各显神通吧。\n1 2 3 4 5 6 7 8 9 10 # 拉取前后端的镜像 docker/crictl(containerd的) # sudo crictl pull docker.io/vaikulkaz/hello-go-gke:1.0 # sudo crictl pull docker.io/vaikulkaz/hello-frontend:1.0 docker pull docker.io/vaikulkaz/hello-go-gke:1.0 docker pull docker.io/vaikulkaz/hello-frontend:1.0 # 保存镜像 docker save -o hello-go-gke-1.0.tar vaikulkaz/hello-go-gke:1.0 docker save -o hello-frontend-1.0.tar vaikulkaz/hello-frontend:1.0 上传保存的镜像tar包到k8s集群上的每一个节点。建议上传到一台上，然后用脚本分发。因为后面还要在每个节点上导入镜像。\n文件分发和命令传输的脚本 在用户目录创建一个bin目录，创建xcall(命令传输，用于在三台虚拟机上依次执行相同命令)、xsync(文件分发，用于把文件分发到三台虚拟机上)。\n1 2 3 4 5 6 7 cd mkdir bin cd bin vim xsync sudo chmod 777 xsync vim xcall sudo chmod 777 xcall xsync内容如下,注意我的三台虚拟机叫k8s1、k8s2、k8s3。如果你的虚拟机不叫这个名字，记得修改成自己的主机名\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash #校验参数是否合法 if(($#==0)) then echo 请输入要分发的文件! exit; fi #获取分发文件的绝对路径 dirpath=$(cd `dirname $1`; pwd -P) filename=`basename $1` echo 要分发的文件的路径是:$dirpath/$filename #循环执行rsync分发文件到集群的每条机器 for((i=1;i\u0026lt;=3;i++)) do echo ---------------------k8s$i--------------------- rsync -rvlt $dirpath/$filename k8s$i:$dirpath done #此脚本用于虚拟机之间通过scp传送文件 xcall内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash #在集群的所有机器上批量执行同一个命令 if(($#==0)) then echo 请输入要操作的命令! exit; fi echo 要执行的命令是$* #循环执行此命令 for((i=1;i\u0026lt;=3;i++)) do echo --------------------k8s$i-------------------- ssh k8s$i $* done #此脚本用于对所有声明的虚拟机进行命令的传输 分发导入镜像 分发导入的命令如下：\n1 2 3 4 5 6 # 分发镜像文件 xsync hello-frontend-1.0.tar hello-go-gke-1.0.tar # 脚本批处理，在k8s节点上导入镜像 xcall sudo ctr -n k8s.io images import hello-frontend-1.0.tar xcall sudo ctr -n k8s.io images import hello-go-gke-1.0.tar 创建前后端 后端部署 创建后端deploymentbackend-deployment.yaml,内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: apps/v1 kind: Deployment metadata: name: backend spec: selector: matchLabels: app: hello tier: backend track: stable replicas: 3 template: metadata: labels: app: hello tier: backend track: stable spec: containers: - name: hello image: vaikulkaz/hello-go-gke:1.0 ports: - name: http containerPort: 80 创建后端servicebackend-service.yaml，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: hello spec: selector: app: hello tier: backend ports: - protocol: TCP port: 80 targetPort: http 创建后端的pod和service：\n1 2 kubectl apply -f backend-deployment.yaml kubectl apply -f backend-service.yaml 前端部署 创建前端deploymentfrontend-deployment.yaml,内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: selector: matchLabels: app: hello tier: frontend track: stable replicas: 1 template: metadata: labels: app: hello tier: frontend track: stable spec: containers: - name: nginx image: vaikulkaz/hello-frontend:1.0 lifecycle: preStop: exec: command: [\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;] 创建前端servicefrontend-service.yaml，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: name: frontend spec: selector: app: hello tier: frontend ports: - protocol: \u0026#34;TCP\u0026#34; port: 80 targetPort: 80 nodePort: 30001 # 可选，不指定则系统自动分配 type: NodePort 创建前端的pod和service：\n1 2 kubectl apply -f frontend-deployment.yaml kubectl apply -f frontend-service.yaml 状态检查 查看前后端的pod状态，应该都是running；\n1 2 3 4 5 6 7 8 9 10 rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE backend-5fd65cb786-dv265 1/1 Running 1 (128m ago) 4d15h backend-5fd65cb786-hdzvz 1/1 Running 1 (128m ago) 4d15h backend-5fd65cb786-zdjqb 1/1 Running 1 (128m ago) 4d15h frontend-5c6cf9bbfc-5mln7 1/1 Running 10 (121m ago) 4d15h liveness-exec 0/1 CrashLoopBackOff 136 (2m30s ago) 4d21h liveness-grpc 1/1 Running 1 (128m ago) 4d19h liveness-http 0/1 CrashLoopBackOff 144 (4m29s ago) 4d21h liveness-tcp 0/1 CrashLoopBackOff 139 (4m47s ago) 4d21h 查看前后端的service状态，后端service类型是ClusterIP,前端service类型是NodePort。\n1 2 3 4 5 rust@k8s1:~$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend NodePort 10.101.209.199 \u0026lt;none\u0026gt; 80:30001/TCP 99m backend ClusterIP 10.111.212.50 \u0026lt;none\u0026gt; 80/TCP 119m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 7d 原理介绍 将请求从前端发送到后端的关键是后端 Service。Service 创建一个固定 IP 和 DNS 解析名入口， 使得后端微服务总是可达。Service使用选择算符(.selector/hello .selector/tier)来寻找目标 Pod。\n后端配置文件中，可以看到名为hello的Service将流量路由到包含app: hello和tier: backend标签的 Pod,也就是后端的pod上。后端pod的作用就是返回网页。\n前端使用被赋予后端 Service的DNS名称将请求发送到后端工作 Pods。这一 DNS 名称为 hello，也就是backend-service.yaml配置 文件中 name 字段的取值。\n前端 Deployment 中的 Pods 运行一个 nginx 镜像，这个已经配置好的镜像会将请求转发给后端的 hello Service。 下面是 nginx 的配置文件:\n1 2 3 4 5 6 7 8 9 10 11 12 # Backend 是 nginx 的内部标识符，用于命名以下特定的 upstream upstream Backend { # hello 是 Kubernetes 中的后端服务所使用的内部 DNS 名称 server hello; } server { listen 80; location / { # 以下语句将流量通过代理方式转发到名为 Backend 的上游 proxy_pass http://Backend; } } 说明：这个 nginx 配置文件是被打包在容器镜像里的。更好的方法是使用ConfigMap，这样解耦了配置参数和应用镜像，可以更轻易地更改配置。\nk8s Service介绍 Service按API type字段分有4种type：ClusterIP（默认）、NodePort、LoadBalancer和ExternalName。 此外，ClusterIP还有一种特殊用法Headless Service（spec.clusterIP: None）。\nClusterIP（默认）：在集群内部分配一个 ClusterIP（虚拟 IP），只在集群内部可达。 后端服务只给集群内部其他服务调用；前端Service在用Ingress之前，会首先尝试使用ClusterIP。 NodePort：在每个Node上开一个静态端口（NodePort），: 会被转发到Service的ClusterIP。 本质上是ClusterIP + 每个节点上都开一个端口。用于从集群外访问服务，尤其是本地环境、测试环境，例如使用nodePort: 30001暴露前端。 LoadBalancer：依赖云厂商的 外部负载均衡器（如 AWS ELB、GCE LB、阿里云 SLB 等），对应云厂商会给你分配一个 外部 IP，然后转发到 NodePort 或直接转发到 Pod。适用于公有云上需要公网 IP对外暴露服务。 ExternalName：不会分配 ClusterIP，也不会创建 Endpoints/EndpointSlices； 只是在集群DNS里做一个 CNAME 映射：.ns.svc.cluster.local映射到externalName指定的DNS名字。 适用于在集群内部解耦外部服务，例如在集群内部要访问外部服务（比如外部数据库、第三方 API），但希望用内部 Service 名访问； 方便以后把外部服务迁到集群里时，只改 Service 定义，不用改应用代码。 Headless：type: ClusterIP（或者不写 type，默认就是 ClusterIP）再加上 spec.clusterIP: None。K8s不会分配 ClusterIP； kube-proxy 不会给它做负载均衡和转发；集群 DNS 会返回每个Pod的IP（A/AAAA 记录），而不是一个VIP。适用于客户端自己选择连哪个 Pod（比如某些数据库客户端、服务发现场景）；StatefulSet + Headless Service，每个 Pod 有稳定的网络标识（DNS 名）。 hello项目测试 前端和后端已经完成连接了。可以使用 curl 命令通过你的前端 Service 的外部 IP 访问服务端点。\n为了确定服务的连通性，我在k8s2上进行测试。由于我配置了host映射，所以这里可以用k8s1:port。没配置的要换成自己的ip(集群上哪个节点的ip都可以)。\n1 2 rust@k8s2:~$ curl k8s1:30001 {\u0026#34;message\u0026#34;:\u0026#34;Hello\u0026#34;} 测试完成，练习结束。不想保存这个练习的服务和pod可用如下命令清除：\n1 2 kubectl delete services frontend backend kubectl delete deployment frontend backend ","date":"2026-02-04T16:49:00+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A03_%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B8%8E%E7%BD%91%E7%BB%9C/","title":"K8s实践练习3_服务发现与网络"},{"content":" 官网链接：配置存活、就绪和启动探针\n前置准备 搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 探针原理简介 k8s用各种类型的探针（存活态探针、就绪态探针、启动态探针）来检测容器状态，根据容器的状态执行对应的策略，例如重启(restartPolicy)、从EndpointSlice中删除pod的ip地址（使该pod不再对外提供服务）。\nk8s探针有如下四种检测方式：\nexec:在容器内执行指定命令。如果命令推出时返回码为0，则认为诊断成功 grpc:使用grpc执行一个远程调用。目标应该实现grpc健康检查。如果响应状态是serving，则认为诊断成功。 httpGet:对容器的IP地址上指定端口和路径执行HTTP Get请求。如果响应的状态码处于[200,400)区间，则认为诊断是成功的 tcpSocket:对容器的IP地址上的指定端口执行TCP检查。如果端口打开，则认为诊断成功。如果远程系统（容器）在打开连接后立刻将其关闭，也算作是健康的。 k8s探针根据检测状态或者时机可以分为三种类型：\nlivenessProbe(存活态探针):指示容器是否正在运行。如果存活态探针检测失败，则kubelet会杀死容器，并且容器将根据其重启策略决定未来。 readinessProbe(就绪态探针):指示容器是否准备好为请求提供服务。如果就绪态探测失败，EndpointSlice控制器将从与该Pod匹配的所有Service的EndpointSlice中删除该Pod的IP地址。初始延迟之前的就绪态的状态默认为Failure。 startupProbe(启动态探针):指示容器中的应用是否已经启动。如果提供了启动探针，则直到此探针成功为止所有其他探针都会被禁用。如果启动探测失败，kubelet将杀死容器，而容器将依照其重启策略进行重启。 关于探针的配置字段：\ninitialDelaySeconds:容器启动后，等待多少秒才开始第一次探针检查。给应用流出初始化时间（比如加载配置、连接数据库等），避免在还没准备好时就被探针误判为失败。 periodSeconds:每隔多少秒进行一次探针检查，默认10s。 failureThreshold:探针连续失败多少次后，才将容器标记为不健康。对存活态探针来说会触发重启，对就绪态探针来说会从Service移除。第一次失败就算一次，不是“容忍失败次数”，默认为3。 timeoutSeconds：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。 successThreshold：探针在失败后，被视为成功的最小连续成功数。默认值是 1。 存活和启动探测的这个值必须是 1。最小值是 1。 terminationGracePeriodSeconds：为 kubelet 配置从为失败的容器触发终止操作到强制容器运行时停止该容器之前等待的宽限时长。 默认值是继承 Pod 级别的 terminationGracePeriodSeconds 值（如果不设置则为 30 秒），最小值为 1。 补充：initialDelaySeconds+failureThreshold*periodSeconds:从**容器启动到被判断为“不健康”**所需的最短时间。（在探针连续失败的情况下） 存活探针练习 exec 下面的yaml定义一个执行命令的存活态探针。pod里只有一个容器，periodSeconds 字段指定了 kubelet 应该每 5 秒执行一次存活探测。 initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒。 kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测。 如果命令执行成功并且返回值为 0，kubelet 就会认为这个容器是健康存活的。 如果这个命令返回非 0 值，kubelet 会杀死这个容器并重新启动它。\n当容器启动时，容器执行命令：/bin/sh -c \u0026quot;touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600\u0026quot;。\n这个容器生命的前 30 秒，/tmp/healthy 文件是存在的。 所以在这最开始的 30 秒内，执行命令 cat /tmp/healthy 会返回成功代码。 30 秒之后，执行命令 cat /tmp/healthy 就会返回失败代码。存活态探针检测失败，就会触发重启。\n1 2 vim exec-liveness.yaml kubectl apply -f exec-liveness.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: busybox:1.27.2 args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 查看这个pod的信息，发现前30s内正常，35s时存活态探针检测到不健康，将重启容器。再查看pod,发现liveness-exec已经重启过（restarts次数不为0，失败的容器恢复为运行状态，RESTARTS 计数器就会增加 1）。\n1 2 3 4 5 6 7 8 9 10 11 12 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 63s default-scheduler Successfully assigned default/liveness-exec to k8s2 Normal Pulled 63s kubelet Container image \u0026#34;busybox:1.27.2\u0026#34; already present on machine Normal Created 63s kubelet Created container: liveness Normal Started 63s kubelet Started container liveness Warning Unhealthy 18s (x3 over 28s) kubelet Liveness probe failed: cat: can\u0026#39;t open \u0026#39;/tmp/healthy\u0026#39;: No such file or directory Normal Killing 18s kubelet Container liveness failed liveness probe, will be restarted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 2 (45s ago) 3m16s http k8s官网的agnhost镜像（特别是 e2e-test-images 这个仓库）在阿里云等国内镜像源上的同步存在问题。这里使用nginx镜像和一段启停脚本来模拟。\n0-15 秒,脚本在后台启动了 Nginx，index.html 存在，HTTP 探测返回 200。状态为 Running，RESTARTS 为 0。 15 秒左右,脚本执行了rm命令，删除了网页文件。kubelet每3秒探测一次，发现 /index.html 返回 404。kubelet 连续失败几次后，判定容器不健康，杀死容器。最终RESTARTS 变成了 1，状态变回 Running，然后循环往复\n1 2 vim http-liveness.yaml kubectl apply -f http-liveness.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: nginx:1.16.1 # 这里我们覆盖 Nginx 的默认启动命令，用脚本来模拟 \u0026#34;先健康，后故障\u0026#34; 的过程 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - | # 1. 启动 Nginx (放在后台运行) nginx -g \u0026#34;daemon off;\u0026#34; \u0026amp; # 2. 等待 10 秒 (这期间 /index.html 是正常的，探测会成功) echo \u0026#34;App is running healthy...\u0026#34; sleep 10 # 3. 10 秒后，删除被探测的文件，制造 HTTP 404 错误 echo \u0026#34;Simulating failure now...\u0026#34; rm /usr/share/nginx/html/index.html # 4. 保持容器主进程不退出 (挂起)，否则 k8s 会认为容器 Crash 了，而不是因为 Liveness 失败 # 我们只需要等待 Kubelet 的探针发现 404 并杀死我们即可 sleep infinity livenessProbe: httpGet: path: /index.html port: 80 initialDelaySeconds: 3 periodSeconds: 3 kubectl describe pod liveness-http,查看事件，能看见探针检测失败，容器重启。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m44s default-scheduler Successfully assigned default/liveness-http to k8s3 Normal Killing 50s (x3 over 2m26s) kubelet Container liveness failed liveness probe, will be restarted Normal Pulled 20s (x4 over 2m44s) kubelet Container image \u0026#34;nginx:1.16.1\u0026#34; already present on machine Normal Created 20s (x4 over 2m44s) kubelet Created container: liveness Normal Started 20s (x4 over 2m44s) kubelet Started container liveness Warning Unhealthy 8s (x10 over 2m32s) kubelet Liveness probe failed: HTTP probe failed with statuscode: 404 rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE liveness-exec 0/1 CrashLoopBackOff 11 (4m8s ago) 30m liveness-http 1/1 Running 4 (11s ago) 3m23s TCP k8s官网的goproxy国内拉不下来，依旧用nginx模拟。TCP探针的原理是kubelet 尝试连接容器的指定端口。如果连接成功，健康；如果连接被拒绝（端口关闭），不健康。\n模拟逻辑：启动 Nginx（打开 80 端口） -\u0026gt; 10秒后停止 Nginx（关闭 80 端口） -\u0026gt; 探测失败 -\u0026gt; 重启。\n1 2 vim tcp-liveness.yaml kubectl apply -f tcp-liveness.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: v1 kind: Pod metadata: name: liveness-tcp labels: test: liveness spec: containers: - name: liveness image: nginx:1.16.1 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - | # 1. 启动 Nginx (端口 80 打开) nginx -g \u0026#34;daemon off;\u0026#34; \u0026amp; # 2. 等待 10 秒，此时 tcpSocket 探测会成功 sleep 10 # 3. 停止 Nginx (端口 80 关闭)，模拟服务挂掉 nginx -s stop # 4. 保持容器主进程存活，等待 Kubelet 判定失败并重启我们 sleep infinity livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 periodSeconds: 3 1 2 3 4 5 6 7 8 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 15s default-scheduler Successfully assigned default/liveness-tcp to k8s2 Normal Pulled 15s kubelet Container image \u0026#34;nginx:1.16.1\u0026#34; already present on machine Normal Created 15s kubelet Created container: liveness Normal Started 14s kubelet Started container liveness Warning Unhealthy 3s kubelet Liveness probe failed: dial tcp 172.16.109.84:80: connect: connection refused grpc 创建grpc-liveness.yaml，发起一个远程调用，调用成功，状态正常。\n1 2 vim grpc-liveness.yaml kubectl apply -f grpc-liveness.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: liveness-grpc spec: containers: - name: etcd image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0 command: [ \u0026#34;/usr/local/bin/etcd\u0026#34;, \u0026#34;--data-dir\u0026#34;, \u0026#34;/var/lib/etcd\u0026#34;, \u0026#34;--listen-client-urls\u0026#34;, \u0026#34;http://0.0.0.0:2379\u0026#34;, \u0026#34;--advertise-client-urls\u0026#34;, \u0026#34;http://127.0.0.1:2379\u0026#34;, \u0026#34;--log-level\u0026#34;, \u0026#34;debug\u0026#34;] ports: - containerPort: 2379 livenessProbe: grpc: port: 2379 initialDelaySeconds: 10 grpc不像前面三种那样容易制造错误，不过还是能从pod事件中看出探针运行情况。查看pod情况，在liveness一栏能看见：\n1 2 rust@k8s1:~$ kubectl describe pod liveness-grpc|grep Liveness: Liveness: grpc \u0026lt;pod\u0026gt;:2379 delay=10s timeout=1s period=10s #success=1 #failure=3 补充 对于 HTTP 和 TCP 存活检测可以使用命名的 port （gRPC 探针不支持使用命名端口）。\n例如：\n1 2 3 4 5 6 7 8 ports: - name: liveness-port containerPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port 有时候，会有一些现有的应用在启动时需要较长的初始化时间。可以将 failureThreshold * periodSeconds 参数设置为足够长的时间来应对最坏情况下的启动时间。\n这样，前面的例子就变成了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ports: - name: liveness-port containerPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 1 periodSeconds: 10 startupProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 30 periodSeconds: 10 就绪探针练习 就绪探针的配置和存活探针的配置相似。 唯一区别就是要使用 readinessProbe 字段，而不是 livenessProbe 字段。\n就绪探针在容器的整个生命周期中保持运行状态。\n存活探针与就绪性探针相互间不等待对方成功。 如果要在执行就绪性探针之前等待，应该使用 initialDelaySeconds 或 startupProbe。\n","date":"2026-02-04T11:18:32+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A02_%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E7%9A%84%E6%8E%A2%E9%92%88/","title":"K8s实践练习2_健康检查的探针"},{"content":" 官网链接：使用 Deployment 运行一个无状态应用\n前置准备 前置要求：搭建好一套k8s集群，可以参考我写的这篇教程：搭建k8s集群\nk8s官方的镜像站在国内是拉不下来的，有几种方法解决：\n在拉取镜像的虚拟机/服务器上科学上网 配置k8s的镜像源，目前国内只有阿里云支持改版后的k8s镜像源(registry.k8s.io)。 需要拉取镜像的时候，指定拉取策略为本地拉取(imagePullPolicy:Never),每次需要拉取镜像前都手动拉取/上传一份镜像到服务器上再导入镜像 这里给出阿里云镜像源的配置教程： 旧版的k8s直接修改/etc/containerd/config.toml里的mirror信息，添加上阿里云的镜像站就行。但是新版的不支持inline或者说暂时兼容，未来不支持。所以这里就只给出新版k8s镜像源配置教程。\n修改/etc/containerd/config.yaml,填入下列信息（如果你已经有了config.yaml且这个配置文件是从containerd默认配置里生成的，那直接备份，然后使用下面的内容）。sudo vim /etc/containerd/config.yaml\n1 2 3 4 5 6 7 8 9 10 version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 到这里，镜像源就配置好了，如果不出意外，文件目录应该是下面这样：\n1 2 3 4 5 6 7 rust@k8s1:/etc/containerd$ ll 总计 28 drwxr-xr-x 3 root root 4096 2月 4 10:31 ./ drwxr-xr-x 144 root root 12288 2月 2 17:01 ../ drwxr-xr-x 4 root root 4096 2月 2 16:44 certs.d/ -rw-r--r-- 1 root root 423 2月 2 19:02 config.toml -rw-r--r-- 1 root root 886 12月 19 02:48 config.toml.dpkg-dist 修改完配置文件后需要重启containerd：\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 创建一个 nginx Deployment k8s里创建pod有多种方式，可以创建Pod/Deployment/StatefulSet等等配置文件，然后用kubectl应用配置文件。k8s里建议用Deployment部署无状态应用并管理副本，如nginx，用StatefulSet部署有状态应用，如MySQL、Zookeeper。\n下面这个YAML文件描述了一个运行 nginx:1.14.2镜像的 Deployment。在k8s集群上创建这个文件。并应用它。\n1 2 vim deployment.yaml kubectl apply -f deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # 告知 Deployment 运行 2 个与该模板匹配的 Pod template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 查看pod信息,类似如下信息。默认部署的pod在default命令空间（namespace）下，不指定命名空间也是查看default命名空间下的pod。\n1 2 3 4 rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-595dff4fdb-2v5x7 1/1 Running 1 (97m ago) 38h nginx-deployment-595dff4fdb-klfnb 1/1 Running 1 (97m ago) 38h 上面的yaml文件里设置了标签(label)app为nginx，所以也可以用这个命令查看刚刚创建的pod:kubectl get pods -l app=nginx\n需要展示某一个pod的信息可以用describe命令：kubectl describe pod \u0026lt;pod-name\u0026gt;\nk8s里pod创建失败、状态异常（ImagePullBackOff、CrashLoopBackOff ）等情况建议先用describe查看pod的状态信息，而不是直接查看日志log。\n更新 nginx Deployment 刚刚创建了一个1.14.2版本双副本的nginx，接下来将更新nginx版本为1.16.1。\n创建一个新的deployment。vim deployment-update.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16.1 # 将 nginx 版本从 1.14.2 更新为 1.16.1 ports: - containerPort: 80 应用新的deployment，查看pod状态，你应该会看见pod滚动更新。（一个新的pod创建好之后会删除一个老pod,然后再创建新的，直到两个pod都完成升级）\n1 2 kubectl apply -f deployment-update.yaml kubectl get pods 通过修改Deployment副本数来扩缩容。创建deployment-scale.yaml文件。应用新的配置文件\n1 2 vim deployment-scale.yaml kubectl apply -f deployment-scale.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 4 # 将副本数从 2 更新为 4 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16.1 ports: - containerPort: 80 验证查看pod,应该能看见有四个nginx pod。\n1 2 3 4 5 6 rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-595dff4fdb-2v5x7 1/1 Running 1 (115m ago) 39h nginx-deployment-595dff4fdb-klfnb 1/1 Running 1 (115m ago) 39h nginx-deployment-595dff4fdb-ls2x8 1/1 Running 0 6s nginx-deployment-595dff4fdb-tmvwf 1/1 Running 0 5s 基于名称删除deployment，你也可以先尝试基于名称删除pod,删除后k8s的deployment Controller会自动重建pod。\n1 2 3 4 5 6 7 8 9 10 11 12 rust@k8s1:~$ kubectl delete pod nginx-deployment-595dff4fdb-2v5x7 pod \u0026#34;nginx-deployment-595dff4fdb-2v5x7\u0026#34; deleted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-595dff4fdb-fgt8n 1/1 Running 0 5s nginx-deployment-595dff4fdb-klfnb 1/1 Running 1 (118m ago) 39h nginx-deployment-595dff4fdb-ls2x8 1/1 Running 0 2m38s nginx-deployment-595dff4fdb-tmvwf 1/1 Running 0 2m37s rust@k8s1:~$ kubectl delete deployment nginx-deployment deployment.apps \u0026#34;nginx-deployment\u0026#34; deleted rust@k8s1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE ","date":"2026-02-04T10:13:03+08:00","permalink":"https://rusthx.github.io/p/k8s%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A01_%E4%BD%BF%E7%94%A8deployment%E8%BF%90%E8%A1%8C%E6%97%A0%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E5%B9%B6%E6%9B%B4%E6%96%B0/","title":"K8s实践练习1_使用Deployment运行无状态应用并更新"},{"content":"Kafka3依赖Zookeeper进行元数据管理，分为Scala2.12和Scala2.13编写的两个版本。 而Kafka4则移除了对Zookeeper的依赖，采用KRaft协议确保一致性。 同时需要注意的是Kafka3的运行时环境（JRE）是Java8,而Kafka4的运行时环境是Java17。\n参考资料：尚硅谷的B站课程\nZookeeper下载链接、 Kafka下载链接\nZookeeper搭建 下载Zookeeper安装包后解压缩，修改名字为zookeeper。上传到虚拟机或者服务器，准备安装（我上传到了/usr/local/）。 创建数据存储目录 在zookeeper路径下创建zkData文件夹\n1 2 cd /usr/local/zookeeper mkdir zkData 创建serverid指定文件 在zkData目录下创建一个myid文件,填入一个与server对应的数字。这里注意三台机的数字一定要不同，我准备在1号机填入2，二号机填入3，3号机填入4。事实上填入1、2、3应该也是可以。这里先填入数字，后面会分发zookeeper，后续步骤会进行修改。\n1 2 cd /usr/local/zookeeper/zkData vim myid 修改zoo.cfg文件的配置信息 1 2 3 4 5 6 7 8 cd /usr/local/zookeeper/conf mv zoo_sample.cfg zoo.cfg vim zoo.cfg # 填入如下内容 dataDir=/usr/local/zookeeper/zkData server.2=hadoop1:2888:3888 server.3=hadoop2:2888:3888 server.4=hadoop3:2888:3888 注意上面配置信息里的dataDir就是zookeeper存储数据的文件夹。要修改为自己的路径。server.2就是1号机的myid里的数字，也要修改为自己填的数字。\n分发zookeeper 分发zookeeper。xsync是一个文件分发脚本。如果没有这个脚本，可以参考我的Hadoop部署教程。 zookeeper分发完成后一定记得修改2号机和3号机的myid，填入serverid。\n1 2 cd /usr/local/ xsync zookeeper 启动zookeeper 在三台机上分别执行如下zookeeper启动命令。然后查看zookeeper启动状态。 1 2 /usr/local/zookeeper/bin/zkServer.sh start /usr/local/zookeeper/bin/zkServer.sh status 正确的状态应该如下：\n1 2 3 4 5 rust@hadoop1:/usr/local/zookeeper$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg Client port found: 2181. Client address: localhost. Client SSL: false. Mode: follower 在1号机上编写启停脚本。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 vim ~/bin/zk.sh # 填入如下内容，注意把主机名改成自己的 #!/bin/bash case $1 in \u0026#34;start\u0026#34;){ for i in hadoop1 hadoop2 hadoop3 do echo ---------- zookeeper $i 启动 ------------ ssh $i \u0026#34;/usr/local/zookeeper/bin/zkServer.sh start\u0026#34; done };; \u0026#34;stop\u0026#34;){ for i in hadoop1 hadoop2 hadoop3 do echo ---------- zookeeper $i 停止 ------------ ssh $i \u0026#34;/usr/local/zookeeper/bin/zkServer.sh stop\u0026#34; done };; \u0026#34;status\u0026#34;){ for i in hadoop1 hadoop2 hadoop3 do echo ---------- zookeeper $i 状态 ------------ ssh $i \u0026#34;/usr/local/zookeeper/bin/zkServer.sh status\u0026#34; done };; esac 增加脚本执行权限 1 2 3 4 5 6 7 sudp chmod 777 ~/bin/zk.sh # zookeeper集群启动命令 zk.sh start # zookeeper集群关闭命令 zk.sh stop # zookeeper集群查看状态 zk.sh status Kafka3搭建 解压下载的Kafka3，并改名为kafka，并上传到虚拟机或服务器的/usr/local目录下。\n修改配置文件 1 2 3 4 5 6 7 cd /usr/local/config vim server.properties # 填入如下内容 broker.id=0 advertised.listeners=PLAINTEXT://hadoop1:9092 log.dirs=/usr/local/kafka/datas zookeeper.connect=hadoop1:2181,hadoop2:2181,hadoop3:2181/kafka 分发Kafka，修改另外两台机的broker.id(broker.id不能重复，2号机改成1，3号机改成2)和advertised.listeners。\n配置环境变量 1 2 3 4 5 6 sudo vim ~/.bashrc # 填入如下内容 export KAFKA_HOME=/usr/local/kafka export PATH=$PATH:$KAFKA_HOME/bin xsync ~/.bashrc 启动Kafka集群 启动Kafka3集群前一定要先启动zookeeper集群，关闭则要先关闭Kafka集群再关闭zookeeper集群。 不然zookeeper关闭后Kafka就不能正常关闭，只能手动杀进程了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 zk.sh start # 依次在三台机执行启动命令 /usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties # 查看进程 xcall jps # 成功搭建后应如下 rust@hadoop1:/usr/local/kafka$ xcall jps 要执行的命令是jps --------------------hadoop1-------------------- 8388 Jps 8299 Kafka 6828 QuorumPeerMain --------------------hadoop2-------------------- 4464 Kafka 3721 QuorumPeerMain 4543 Jps --------------------hadoop3-------------------- 4491 QuorumPeerMain 5212 Jps 5132 Kafka # 手动关闭Kafka集群，在三台机上分别执行如下命令 /usr/local/kafka//usr/local/kafka/ 集群启停脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 sudo vim ~/bin/kf.sh # 填入如下内容 #!/bin/bash case $1 in \u0026#34;start\u0026#34;) for i in hadoop1 hadoop2 hadoop3 do echo \u0026#34;---启动 $i kafka---\u0026#34; ssh $i \u0026#34;/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties\u0026#34; done ;; \u0026#34;stop\u0026#34;) for i in hadoop1 hadoop2 hadoop3 do echo \u0026#34;---停止 $i kafka---\u0026#34; ssh $i \u0026#34;/usr/local/kafka/bin/kafka-server-stop.sh\u0026#34; done ;; esac # 添加执行权限 sudo chmod 777 ~/bin/kf.sh # kakfa集群启动 kf.sh strat # kafka集群关闭 kf.sh stop # kafka集群状态查看 kf.sh status Kafka终端命令操作 主题（topic）相关命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 1）查看topic命令参数 /usr/local/bin/kafka-topics.sh 参数\t描述 --bootstrap-server \u0026lt;String: server toconnect to\u0026gt;\t连接的Kafka Broker主机名称和端口号。 --topic \u0026lt;String: topic\u0026gt;\t操作的topic名称。 --create\t创建主题。 --delete\t删除主题。 --alter\t修改主题。 --list\t查看所有主题。 --describe\t查看主题详细描述。 --partitions \u0026lt;Integer: # of partitions\u0026gt;\t设置分区数。 --replication-factor\u0026lt;Integer: replication factor\u0026gt;\t设置分区副本。 --config \u0026lt;String: name=value\u0026gt;\t更新系统默认的配置。 # 2）查看当前服务器中的所有topic /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server hadoop1:9092 --list # 3）创建名字为first的topic /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server hadoop1:9092 --create --partitions 1 --replication-factor 3 --topic first # --topic 定义topic名 # --replication-factor 定义副本数 # --partitions 定义分区数 # 4）查看first主题的详情 /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server hadoop1:9092 --describe --topic first # 5）修改分区数（注意：分区数只能增加，不能减少） /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server hadoop1:9092 --alter --topic first --partitions 3 # 6）再次查看first主题的详情 /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server hadoop1:9092 --describe --topic first # 7）删除topic /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server hadoop1:9092 --delete --topic first 生产者（producer）相关命令 1 2 3 4 5 6 7 8 9 10 11 # 1）查看操作生产者命令参数 /usr/local/kafka/bin/kafka-console-producer.sh # 参数\t描述 # --bootstrap-server \u0026lt;String: server toconnect to\u0026gt;\t连接的Kafka Broker主机名称和端口号。 # --topic \u0026lt;String: topic\u0026gt;\t操作的topic名称。 # 2）发送消息 /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server hadoop1:9092 --topic first \u0026gt;hello world \u0026gt;atguigu atguigu 消费者（consumer）相关命令 1 2 3 4 5 6 7 8 9 10 11 12 13 # 1）查看操作消费者命令参数 /usr/local/kafka/bin/kafka-console-consumer.sh # 参数\t描述 # --bootstrap-server \u0026lt;String: server toconnect to\u0026gt;\t连接的Kafka Broker主机名称和端口号。 # --topic \u0026lt;String: topic\u0026gt;\t操作的topic名称。 # --from-beginning\t从头开始消费。 # --group \u0026lt;String: consumer group id\u0026gt;\t指定消费者组名称。 # 2）消费消息 # （1）消费first主题中的数据 /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server hadoop1:9092 --topic first # （2）把主题中所有的数据都读取出来（包括历史数据） /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server hadoop1:9092 --from-beginning --topic first ","date":"2025-06-13T21:47:27+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.4%E6%90%AD%E5%BB%BAkafka3%E5%8F%8Azookeeper/","title":"Ubuntu22.4搭建Kafka3及Zookeeper"},{"content":" 参考：尚硅谷Hadoop课程。 CSDN相关教程\n在搭建Hadoop集群前需要先搭建好Ubuntu虚拟机，具体可参考下面的教程。\n前置虚拟机搭建\n本文搭建了三台虚拟机，其中hadoop1是主节点，主机名与ip对应关系如下。 如果你想抄我的配置，不想在部署的时候修改ip,你得先在vmvare里点击左上角编辑，虚拟网络编辑器。改成如下图所示。 192.168.146.161 hadoop1 192.168.146.162 hadoop2 192.168.146.163 hadoop3\n下载Hadoop 推荐在Hadoop官网下载。https://hadoop.apache.org/releases.html\n可以在官网下载好后再用winscp/xshell等工具把Hadoop的二进制包上传到虚拟机上，也可以直接在虚拟机的下载路径(下载文件夹没有权限问题)直接下载Hadoop二进制包。命令如下：\n1 2 cd ~/下载 wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz 可以在这里直接右键提取到此处,然后右键改名为hadoop，也可以用命令解压后改名：\n1 2 tar xzf hadoop-3.4.1.tar.gz mv hadoop-3.4.1 hadoop 修改Hadoop配置文件 Hadoop的配置文件在Hadoop文件夹下的/etc/hadoop/。包含五个：\n1 2 3 4 5 core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml workers 照样可以右键修改，也可以用命令；\n1 2 3 4 5 6 cd hadoop/etc/hadoop vim core-site.xml vim hdfs-site.xml vim mapred-site.xml vim yarn-site.xml vim workers 依次添加配置信息(注意把用户名改成自己的用户名，我的用户名是rust)。core-site.xml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;!-- Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file. --\u0026gt; \u0026lt;!-- Put site-specific property overrides in this file. --\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.default.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;HDFS的URI，文件系统://namenode标识:端口号\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/hadoop/tmp\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;namenode上本地的hadoop临时文件夹\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.http.staticuser.user\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;rust\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.rust.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.rust.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; hdfs-site.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;!-- Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file. --\u0026gt; \u0026lt;!-- Put site-specific property overrides in this file. --\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/hadoop/hdfs/name\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;namenode上存储hdfs名字空间元数据 \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/hadoop/hdfs/data\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;datanode上数据块的物理存储位置\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;3\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.http.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1:9870\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 2nn web端访问地址--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.secondary.http-address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop2:9868\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.client.block.write.replace-datanode-on-failure.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.client.block.write.replace-datanode-on-failure.policy\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;NEVER\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.webhdfs.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.permissions.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; mapred-site.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;!-- Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file. --\u0026gt; \u0026lt;!-- Put site-specific property overrides in this file. --\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.map.memory.mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2048\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.jobhistory.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1:10020\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.jobhistory.webapp.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1:19888\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.app.mapreduce.am.env\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;HADOOP_MAPRED_HOME=/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.map.env\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;HADOOP_MAPRED_HOME=/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.reduce.env\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;HADOOP_MAPRED_HOME=/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; yarn-site.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;!-- Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file. --\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定ResourceManager的地址--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.log-aggregation-enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 环境变量的继承 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.env-whitelist\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.pmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.vmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认 是 true --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.pmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认 是 true --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.vmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; workers\n1 2 3 hadoop1 hadoop2 hadoop3 修改hadoop-env.sh,添加java环境变量，这是用命令行apt安装的Java路径，如果自己的Java版本或者路径不对，请修改成自己的版本。\n1 2 3 vim hadoop-env.sh #添加如下内容 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 修改后移动到配置文件中的位置（/usr/local/hadoop）。其他位置也可以，但是我习惯放在/usr/local下。如果想改到其他的目录记得修改上面的五个配置文件设计到的地方。\n1 2 cd ~/下载 sudo mv hadoop /usr/local/ 添加环境变量 修改环境变量，添加如下内容：\n1 sudo vim ~/.bashrc 1 2 export HADOOP_HOME=/usr/local/hadoop export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 添加后使用hadoop version命令查看，如果出现Hadoop版本，那环境变量就修改成功了。如果没有，关闭当前的终端，新开一个终端试试。 如果还不行，复查一下前面的步骤哪里做的不对。 文件分发和命令传输的脚本 在用户目录创建一个bin目录，创建xcall(命令传输，用于在三台虚拟机上依次执行相同命令)、xsync(文件分发，用于把文件分发到三台虚拟机上)。\n1 2 3 4 5 6 7 cd mkdir bin cd bin vim xsync sudo chmod 777 xsync vim xcall sudo chmod 777 xcall xsync内容如下,注意我的三台虚拟机叫hadoop1、hadoop2、hadoop3。如果你的虚拟机不叫这个名字，记得修改成自己的主机名\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash #校验参数是否合法 if(($#==0)) then echo 请输入要分发的文件! exit; fi #获取分发文件的绝对路径 dirpath=$(cd `dirname $1`; pwd -P) filename=`basename $1` echo 要分发的文件的路径是:$dirpath/$filename #循环执行rsync分发文件到集群的每条机器 for((i=1;i\u0026lt;=3;i++)) do echo ---------------------hadoop$i--------------------- rsync -rvlt $dirpath/$filename hadoop$i:$dirpath done #此脚本用于虚拟机之间通过scp传送文件 xcall内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash #在集群的所有机器上批量执行同一个命令 if(($#==0)) then echo 请输入要操作的命令! exit; fi echo 要执行的命令是$* #循环执行此命令 for((i=1;i\u0026lt;=3;i++)) do echo --------------------hadoop$i-------------------- ssh hadoop$i $* done #此脚本用于对所有声明的虚拟机进行命令的传输 配置hosts映射 修改/etc/hosts。添加映射\n1 sudo vim /etc/hosts 添加如下内容：记得把ip改成自己的ip，虚拟机看ip和改ip的教程在前置教程安装虚拟机里有写，也可以自行必应查询或者问大模型。\n1 2 3 192.168.146.161 hadoop1 192.168.146.162 hadoop2 192.168.146.163 hadoop3 补充：配置Windows到虚拟机的hosts映射：（可选） 修改C盘下的hosts文件，路径为C:\\Windows\\System32\\drivers\\etc 克隆虚拟机 在终端命令中关闭虚拟机或者直接在关掉vmvare再启动。\n1 shutdown -h now 按照上面的步骤再克隆一台hadoop3。克隆完成后点击vmvare左上角的文件，打开。 修改另外两台机主机名和ip 在另外两台机上分别执行sudo vim /etc/hostname,分别改成hadoop2和hadopp3。 修改网络，注意如果你之前改ip是用可视化界面改的就一定要用可视化界面改，如果之前是用命令改的那现在也要用命令改，不然会出问题，将无法访问外网。\n这里的ip改成之前hosts映射里写的ip。\n配置ssh免密登录 配置免密登录后就不用每次文件分发和命令传输就输一次密码。 在hadoop1上执行如下密钥生成命令：\n1 ssh-keygen -t rsa 一直回车就行，然后再执行如下密钥拷贝命令：\n1 2 3 ssh-copy-id hadoop1 ssh-copy-id hadoop2 ssh-copy-id hadoop3 如此hadoop1就可以免密登录另外两台机，如果想三台机相互可以免密登录，那分别在hadoop2和hadoop3上执行密钥拷贝命令即可。 测试免密情况，在hadoop1上执行如下命令：\n1 xcall jps 初始化Hadoop并启动 Hadoop初始化命令如下，在hadoop1上执行如下命令：\n1 2 hadoop namenode -format start-all.sh Hadoop启动后注意看jps进程，一定要有图中的进程，少一个就是安装失败了，请仔细核对一下自己前面的步骤哪里做的不对，尤其是Hadoop的五个配置文件和hadoop-env.sh。\n查看webui 访问192.168.146.161:9870,看见有三个节点即是配置成功。在Windows配置hosts映射后也可以访问hadoop1:9870 补充：Ubuntu修改主机名后怎么办 修改hosts:sudo vim /etc/hosts\n修改hostname:sudo vim /etc/hostname\n修改 xsync xcall,把脚本里的旧的用户名改成新的用户名。比如主机名从hadoop01-\u0026gt;hadoo1，那就要把脚本里的hadoop0$i改成hadoop$i。\n修改ssh,直接删除旧的ssh密钥，重新生成拷贝,在主机执行如下命令（记得把主机名改成自己的）。\n1 2 3 4 ssh-keygen -t rsa ssh-copy-id hadoop1 ssh-copy-id hadoop2 ssh-copy-id hadoop3 5.修改Hadoop的配置文件，包括core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、worker\n删除hdfs的数据（/usr/local/hadoop/hdfs/）,主机（namenode所在节点）会多一个name，从节点只有data,都删除，把tmp文件夹直接删掉。\n备忘录：如果安装了hive和spark，那还需要修改如下文件\nhive-site.xml\n修改spark/conf/worker\nspark-env.sh\nspark-defaults.conf\n","date":"2025-06-12T21:19:57+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.4%E6%90%AD%E5%BB%BAhadoop3%E9%9B%86%E7%BE%A4/","title":"Ubuntu22.4搭建Hadoop3集群"},{"content":"sqoop功能稳定后就没再更新了，最新版本就是1.4.7。而最新版的sqoop又分为支持Hadoop2的版本和纯净的版本。 我们需要把两个包都下下来，提取部分sqoop_hadoop2.6.0版本的jar包放到纯净版sqoop的lib目录下，在sqoop配置文件中加入获取当前环境中的hive及hadoop的lib库来使用.\n下载sqoop 下载sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 和sqoop-1.4.7.tar.gz 这两个包。 从支持Hadoop2的sqoop的lib目录下复制下图所示的三个包到纯净版中， mysql开头的那个jar包是MySQL的jdbc连接包，如果使用sqoop同步数据的过程中需要从MySQL导出数据或者从导入数据到MySQL，那还需要放这个包进去。 同理，如果需要使用hbase，那也一样放jar包进去。（连接hive不需要再放jar包,hive数据是存在hdfs上的）\n上传sqoop到服务器并配置 把修改好的sqoop上传到服务器上,放在喜欢的位置（我放在了/usr/local下），在sqoop的conf目录的sqoop-env.sh文件添加如下内容,引入Hadoop和hive的lib库,注意路径要改成自己的安装路径：\n1 2 3 4 5 6 export HADOOP_COMMON_HOME=/usr/local/hadoop export HADOOP_MAPRED_HOME=/usr/local/hadoop export HADOOP_HOME=/usr/local/hadoop export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/* export HIVE_HOME=/usr/local/hive 在服务器上添加sqoop的环境变量，修改~/.bashrc。添加如下内容：\n1 2 export SQOOP_HOME=/usr/local/sqoop export PATH=$PATH:$SQOOP_HOME/bin:$SQOOP_HOME/bin 至此，sqoop安装完成。sqoop是离线应用，即用即开，和DataX相同。\n","date":"2025-06-10T17:31:20+08:00","permalink":"https://rusthx.github.io/p/hadoop3%E9%85%8D%E7%BD%AEsqoop/","title":"Hadoop3配置sqoop"},{"content":"前置条件 需要部署好Hadoop集群和zookeeper集群\n上传软件包 下载hbase包，上传到虚拟机或者服务器中。放在适合的位置。我放在了/usr/local/下\n下载链接：https://archive.apache.org/dist/hbase/2.6.2/hbase-2.6.2-bin.tar.gz\n修改配置文件 修改hbase/conf下的hbase-env.sh和habse-site.xml\n在hbase-env.sh末尾添加export HBASE_MANAGES_ZK=false。\nhbase-site.xml修改为如下内容，记得将主机名修改为自己的机器名。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1,hadoop2,hadoop3\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;The directory shared by RegionServers. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/hbase\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;The directory shared by RegionServers. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.wal.provider\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;filesystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 修改环境变量 1 2 3 4 5 6 sudo vim ~/.bashrc # 添加如下内容，记得将路径改为自己的hbase路径 #HBASE_HOME export HBASE_HOME=/usr/local/hbase export PATH=$PATH:$HBASE_HOME/bin 解决hbase和Hadoop的log4j兼容问题 修改HBase的jar包，使用Hadoop的jar包\n1 mv /usr/local/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar /usr/local/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar.bak 分发hbase 创建分发脚本xsync\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cd mkdir bin cd bin sudo vim xsync # 添加如下内容 #!/bin/bash #校验参数是否合法 if(($#==0)) then echo 请输入要分发的文件! exit; fi #获取分发文件的绝对路径 dirpath=$(cd `dirname $1`; pwd -P) filename=`basename $1` echo 要分发的文件的路径是:$dirpath/$filename #循环执行rsync分发文件到集群的每条机器 for((i=1;i\u0026lt;=3;i++)) do echo ---------------------hadoop$i--------------------- rsync -rvlt $dirpath/$filename hadoop$i:$dirpath done #此脚本用于虚拟机之间通过scp传送文件 添加执行权限，分发hbase\n1 2 sudo chmod 777 xsync xsync /usr/local/hbase 启动hbase 启动zookeeper成功之后再启动habse。 单点启动：\n1 2 bin/hbase-daemon.sh start master bin/hbase-daemon.sh start regionserver 群启：bin/start-habse.sh\n访问webui http://hadoop1:16010/\n能正常访问即成功。 ","date":"2025-06-09T21:25:02+08:00","permalink":"https://rusthx.github.io/p/ubuntu%E9%85%8D%E7%BD%AEhbase/","title":"Ubuntu配置hbase"},{"content":"点击新建项目，选择Java Maven项目 点击文件-设置-\u0026gt;插件。搜索安装Scala插件 点击左上角文件，项目结构 点击全局库，点击+新建全局库，点击添加Scala SDK，可以在IDEA里下载，也可以自己在Scala官网手动下载后手动导入。\nscala官网：https://www.scala-lang.org/download/\n注意：已经创建过Scala项目的这里会有全局库，只需要点击-删除旧的全局库，再添加即可。\n在main下的java目录里，即可创建Scala类。如果想实现一个蓝色的scala文件夹，可以新建目录，命名为scala,然后右键scala目录，选择标记为源代码目录。 ","date":"2025-06-09T13:32:25+08:00","permalink":"https://rusthx.github.io/p/idea%E6%96%B0%E5%BB%BAscala%E9%A1%B9%E7%9B%AE/","title":"Idea新建Scala项目"},{"content":"下载apache服务器\n1 2 sudo apt-get update sudo apt-get install apache2 可以用apache2 -v查看版本 此时可以在浏览器中输入Ubuntu的ip地址访问apache服务器 修改默认网页。有两种方式，apache服务器的默认网页是/var/www/html/index.html。 可以直接修改或替换这个html文件来修改默认网页。 另外一种方式是新建一个文件夹存放网页资源，然后修改apache的配置文件，配置文件为/etc/apache2/sites-available/000-default.conf。\n我这里使用第二种方式，这样更加灵活。\n新建目录存放网络资源文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mkdir /var/www/mysite gedit /var/www/mysite/get_data.xml #终端操作可以使用vim编辑文件 # vim ~/mysite/get_data.xml \u0026lt;apps\u0026gt; \u0026lt;app\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Google Maps\u0026lt;/name\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;/app\u0026gt; \u0026lt;app\u0026gt; \u0026lt;id\u0026gt;2\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Chrome\u0026lt;/name\u0026gt; \u0026lt;version\u0026gt;2.1\u0026lt;/version\u0026gt; \u0026lt;/app\u0026gt; \u0026lt;app\u0026gt; \u0026lt;id\u0026gt;3\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Google Play\u0026lt;/name\u0026gt; \u0026lt;version\u0026gt;2.3\u0026lt;/version\u0026gt; \u0026lt;/app\u0026gt; \u0026lt;/apps\u0026gt; 2. 编辑默认虚拟主机配置,修改DocumentRoot指定目录和目录权限。 1 2 3 4 5 6 7 8 sudo gedit /etc/apache2/sites-available/000-default.conf DocumentRoot /var/www/mysite \u0026lt;Directory /var/www/mysite\u0026gt; Options Indexes FollowSymLinks AllowOverride None Require all granted \u0026lt;/Directory\u0026gt; 重启apache服务器 1 sudo systemctl restart apache2 查看数据 ","date":"2025-04-10T14:49:48+08:00","permalink":"https://rusthx.github.io/p/ubuntu%E9%85%8D%E7%BD%AEapache%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"Ubuntu配置Apache服务器"},{"content":"修改app/builde.gradle.kts和gadle/libs.versions.toml如下即可引入依赖（以RecylceView）为例\n我的Java版本是Java17,注意修改为自己的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 plugins { alias(libs.plugins.android.application) alias(libs.plugins.jetbrains.kotlin.android) } android { namespace = \u0026#34;com.example.uibestpractice\u0026#34; compileSdk = 34 defaultConfig { applicationId = \u0026#34;com.example.uibestpractice\u0026#34; minSdk = 30 targetSdk = 34 versionCode = 1 versionName = \u0026#34;1.0\u0026#34; testInstrumentationRunner = \u0026#34;androidx.test.runner.AndroidJUnitRunner\u0026#34; } buildTypes { release { isMinifyEnabled = false proguardFiles( getDefaultProguardFile(\u0026#34;proguard-android-optimize.txt\u0026#34;), \u0026#34;proguard-rules.pro\u0026#34; ) } } compileOptions { sourceCompatibility = JavaVersion.VERSION_17 targetCompatibility = JavaVersion.VERSION_17 } kotlinOptions { jvmTarget = \u0026#34;17\u0026#34; } } dependencies { implementation(libs.androidx.core.ktx) implementation(libs.androidx.appcompat) implementation(libs.material) implementation(libs.androidx.activity) implementation(libs.androidx.constraintlayout) //增加这一行引入RecycleView依赖 implementation(libs.androidx.recyclerview) testImplementation(libs.junit) androidTestImplementation(libs.androidx.junit) androidTestImplementation(libs.androidx.espresso.core) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [versions] agp = \u0026#34;8.5.2\u0026#34; kotlin = \u0026#34;1.9.0\u0026#34; coreKtx = \u0026#34;1.10.1\u0026#34; junit = \u0026#34;4.13.2\u0026#34; junitVersion = \u0026#34;1.1.5\u0026#34; espressoCore = \u0026#34;3.5.1\u0026#34; appcompat = \u0026#34;1.6.1\u0026#34; material = \u0026#34;1.10.0\u0026#34; activity = \u0026#34;1.8.0\u0026#34; constraintlayout = \u0026#34;2.1.4\u0026#34; androidx-recyclerview = \u0026#34;1.3.2\u0026#34; [libraries] androidx-core-ktx = { group = \u0026#34;androidx.core\u0026#34;, name = \u0026#34;core-ktx\u0026#34;, version.ref = \u0026#34;coreKtx\u0026#34; } junit = { group = \u0026#34;junit\u0026#34;, name = \u0026#34;junit\u0026#34;, version.ref = \u0026#34;junit\u0026#34; } androidx-junit = { group = \u0026#34;androidx.test.ext\u0026#34;, name = \u0026#34;junit\u0026#34;, version.ref = \u0026#34;junitVersion\u0026#34; } androidx-espresso-core = { group = \u0026#34;androidx.test.espresso\u0026#34;, name = \u0026#34;espresso-core\u0026#34;, version.ref = \u0026#34;espressoCore\u0026#34; } androidx-appcompat = { group = \u0026#34;androidx.appcompat\u0026#34;, name = \u0026#34;appcompat\u0026#34;, version.ref = \u0026#34;appcompat\u0026#34; } material = { group = \u0026#34;com.google.android.material\u0026#34;, name = \u0026#34;material\u0026#34;, version.ref = \u0026#34;material\u0026#34; } androidx-activity = { group = \u0026#34;androidx.activity\u0026#34;, name = \u0026#34;activity\u0026#34;, version.ref = \u0026#34;activity\u0026#34; } androidx-constraintlayout = { group = \u0026#34;androidx.constraintlayout\u0026#34;, name = \u0026#34;constraintlayout\u0026#34;, version.ref = \u0026#34;constraintlayout\u0026#34; } androidx-recyclerview = { module = \u0026#34;androidx.recyclerview:recyclerview\u0026#34;, version.ref = \u0026#34;androidx-recyclerview\u0026#34; } [plugins] android-application = { id = \u0026#34;com.android.application\u0026#34;, version.ref = \u0026#34;agp\u0026#34; } jetbrains-kotlin-android = { id = \u0026#34;org.jetbrains.kotlin.android\u0026#34;, version.ref = \u0026#34;kotlin\u0026#34; } ","date":"2025-03-28T22:15:11+08:00","permalink":"https://rusthx.github.io/p/andorid-studio2024.1-api35%E5%BC%95%E5%85%A5recycleview/","title":"Andorid Studio2024.1 API35引入RecycleView"},{"content":"Android studio每次新建项目都要下载gradle，挂了代理也老是下载失败。更换阿里的镜像源也一直报错。直到接触到了腾子的大手……\n修改图中的两个文件。/gradle/wrapper.gradle-wrapper.properties、settings.gradle.kts\n1 2 3 4 5 6 #Sat Mar 22 15:05:59 CST 2025 distributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https://mirrors.cloud.tencent.com/gradle/gradle-8.7-bin.zip zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists 修改url那一项，注意gradle要改成自己的项目所用版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 pluginManagement { repositories { maven { url = uri(\u0026#34;https://mirrors.cloud.tencent.com/nexus/repository/maven-public/\u0026#34;) } mavenCentral() gradlePluginPortal() } } dependencyResolutionManagement { repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS) repositories { google() mavenCentral() } } rootProject.name = \u0026#34;AcivateTry\u0026#34; include(\u0026#34;:app\u0026#34;) 修改图中的镜像仓库链接部分。\n","date":"2025-03-22T16:32:44+08:00","permalink":"https://rusthx.github.io/p/andorid-studio2024.1%E9%85%8D%E7%BD%AE%E8%85%BE%E8%AE%AF%E9%95%9C%E5%83%8F%E6%BA%90/","title":"Andorid Studio2024.1配置腾讯镜像源"},{"content":"我出于自身需要，已经编译好了doriswriter和clickhousewriter，有相同需求的可以直接云盘下载。\nhttps://wwsx.lanzouw.com/b00y9w9ovg 密码:byxl\n前置准备工作：准备一个JDK8及以上，安装好maven。\n拉取DataX源码 这里有两种方法，一种是通过git直接拉取DataX的项目代码。另外一种则是下载项目提供的打包源码。 拉取源代码 建立一个空文件夹，然后在文件夹内执行如下命令： 1 2 git clone https://github.com/alibaba/DataX.git cd DataX 下载源码 直接下载压缩包，然后解压打开文件夹即可。 全量编译项目 执行命令：mvn clean package。这个命令会清理之前的构建输出 (clean) 并重新打包项目 (package)。它会遍历所有模块，执行测试，并生成最终的 jar 文件和其他构建产物。这一步需要保证网络畅通，因为编译时Maven需要从远程仓管下载所需的依赖库。\n顺利地话到这里就解决了，但是很显然没这么简单。接下来会语句一堆报错，而且还是我们所不关心的插件。那可不可以使用尚硅谷教程里提供的已经编译好的DataX呢？\n单独编译需要的插件 注：Doris官网中编译doriswriter插件是在Linux中执行shell命令。我懒得再建一个Linux环境。就放弃了这条路线。 使用Linux的可以直接查看Doris官网的相关部分\n尚硅谷提供的已经编译好的压缩包虽然好，但是我所需要的doriswriter插件是很早期的插件。和我现在用的Doris2.1.7不兼容。 那我就需要自行编译doriswriter插件，然后将编译好的jar包替换尚硅谷提供的压缩包里的jar包。\n在doriswriter目录下执行mvn clean package -DskipTests就可以得到所需jar包。\n然后就遇见了以下报错：\n1 2 3 [ERROR] Failed to execute goal on project doriswriter: Could not resolve dependencies for project com.alibaba.datax:doriswriter:jar:0.0.1-SNAPSHOT: The following artifacts could not be resolved: com.alibaba.datax:datax-common:jar:0.0.1-SNAPSHOT, com.alibaba.datax:plugin-rdbms-util:jar:0.0.1-SNAPSHOT: Could not find artifact com.alibaba.datax:datax-common:jar:0.0.1-S NAPSHOT in central (https://maven.aliyun.com/repository/central) -\u0026gt; [Help 1] 从错误信息来看，Maven 在尝试编译 doriswriter 插件时无法找到依赖项 datax-common 和 plugin-rdbms-util。这是因为这些依赖项没有在中央仓库中可用，它们是 DataX 项目内部的模块。\n所以需要先确保 DorisWriter 插件依赖的模块（ datax-common 和 plugin-rdbms-util）已经被正确安装到本地 Maven 仓库中。\n1 2 3 4 5 6 7 8 cd common mvn clean install -DskipTests cd ../plugin-rdbms-util mvn clean install -DskipTests cd ../doriswriter/ mvn clean package -DskipTests 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\u0026gt;mvn clean install -DskipTests [INFO] Scanning for projects... [INFO] [INFO] -------------------\u0026lt; com.alibaba.datax:datax-common \u0026gt;------------------- [INFO] Building datax-common 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ datax-common --- [INFO] Deleting D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\\target [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ datax-common --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] Copying 6 resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ datax-common --- [INFO] Compiling 45 source files to D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\\target\\classes [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ datax-common --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\\src\\test\\resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ datax-common --- [INFO] No sources to compile [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ datax-common --- [INFO] Tests are skipped. [INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ datax-common --- [INFO] Building jar: D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\\target\\datax-common-0.0.1-SNAPSHOT.jar [INFO] [INFO] --- maven-install-plugin:2.4:install (default-install) @ datax-common --- [INFO] Installing D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\\target\\datax-common-0.0.1-SNAPSHOT.jar to D:\\maven-3.8.6\\respository\\com\\alibaba\\datax\\datax-common\\0.0.1-SNAPSHOT\\datax-common-0.0.1-SNAPSHOT.jar [INFO] Installing D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\\pom.xml to D:\\maven-3.8.6\\respository\\com\\alibaba\\datax\\datax-common\\0.0.1-SNAPSHOT\\datax-common-0.0.1-SNAPSHOT.pom [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.041 s [INFO] Finished at: 2025-02-03T17:26:33+08:00 [INFO] ------------------------------------------------------------------------ D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\common\u0026gt;cd ../plugin-rdbms-util D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\u0026gt;mvn clean install -DskipTests [INFO] Scanning for projects... [INFO] [INFO] ----------------\u0026lt; com.alibaba.datax:plugin-rdbms-util \u0026gt;----------------- [INFO] Building plugin-rdbms-util 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- Downloading from spring: https://maven.aliyun.com/repository/spring/com/alibaba/datax/datax-all/0.0.1-SNAPSHOT/maven-metadata.xml Downloading from central: https://maven.aliyun.com/repository/central/com/alibaba/datax/datax-all/0.0.1-SNAPSHOT/maven-metadata.xml [INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ plugin-rdbms-util --- [INFO] Deleting D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\\target [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ plugin-rdbms-util --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] Copying 0 resource [INFO] [INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ plugin-rdbms-util --- [INFO] Compiling 25 source files to D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\\target\\classes [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ plugin-rdbms-util --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\\src\\test\\resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ plugin-rdbms-util --- [INFO] No sources to compile [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ plugin-rdbms-util --- [INFO] Tests are skipped. [INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ plugin-rdbms-util --- [INFO] Building jar: D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\\target\\plugin-rdbms-util-0.0.1-SNAPSHOT.jar [INFO] [INFO] --- maven-install-plugin:2.4:install (default-install) @ plugin-rdbms-util --- [INFO] Installing D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\\target\\plugin-rdbms-util-0.0.1-SNAPSHOT.jar to D:\\maven-3.8.6\\respository\\com\\alibaba\\datax\\plugin-rdbms-util\\0.0.1-SNAPSHOT\\plugin-rdbms-util-0.0.1-SNAPSHOT.jar [INFO] Installing D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\\pom.xml to D:\\maven-3.8.6\\respository\\com\\alibaba\\datax\\plugin-rdbms-util\\0.0.1-SNAPSHOT\\plugin-rdbms-util-0.0.1-SNAPSHOT.pom [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.183 s [INFO] Finished at: 2025-02-03T17:27:38+08:00 [INFO] ------------------------------------------------------------------------ D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\plugin-rdbms-util\u0026gt;cd ../doriswriter/ D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\doriswriter\u0026gt;mvn clean package -DskipTests [INFO] Scanning for projects... [INFO] [INFO] -------------------\u0026lt; com.alibaba.datax:doriswriter \u0026gt;-------------------- [INFO] Building doriswriter 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ doriswriter --- [INFO] Deleting D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\doriswriter\\target [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ doriswriter --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] Copying 0 resource [INFO] [INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ doriswriter --- [INFO] Compiling 13 source files to D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\doriswriter\\target\\classes [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ doriswriter --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] skip non existing resourceDirectory D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\doriswriter\\src\\test\\resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ doriswriter --- [INFO] No sources to compile [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ doriswriter --- [INFO] Tests are skipped. [INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ doriswriter --- [INFO] Building jar: D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\doriswriter\\target\\doriswriter-0.0.1-SNAPSHOT.jar [INFO] [INFO] --- maven-assembly-plugin:2.2-beta-5:single (dwzip) @ doriswriter --- [INFO] Reading assembly descriptor: src/main/assembly/package.xml [INFO] Copying files to D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\doriswriter\\target\\datax [WARNING] Assembly file: D:\\DeskTop\\煊\\笔记\\DataX\\DataX-datax_v202309\\doriswriter\\target\\datax is not a regular file (it may be a directory). It cannot be attached to the project build for installation or deployment. [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.912 s [INFO] Finished at: 2025-02-03T17:28:12+08:00 [INFO] ------------------------------------------------------------------------ 成功完成编译，这时候就可以在doriswriter/target目录下看到生成的 JAR 文件 然后再替换旧的插件。旧的插件放在datax\\datax\\plugin\\writer\\doriswriter。 注意不是在datax\\plugin\\writer\\doriswriter里。\n补充报错 实际在使用时还遇见了找不到 com.alibaba.fastjson2.JSON 类。的报错。这个错误通常是由于缺少必要的依赖库或版本不匹配导致的。\n1 2 3 4 5 6 7 8 9 10 11 12 DataX运行报错Exception in thread \u0026#34;Thread-1\u0026#34; java.lang.NoClassDefFoundError: com/alibaba/fastjson2/JSON at com.alibaba.datax.plugin.writer.doriswriter.DorisStreamLoadObserver.put(DorisStreamLoadObserver.java:186) at com.alibaba.datax.plugin.writer.doriswriter.DorisStreamLoadObserver.streamLoad(DorisStreamLoadObserver.java:63) at com.alibaba.datax.plugin.writer.doriswriter.DorisWriterManager.asyncFlush(DorisWriterManager.java:163) at com.alibaba.datax.plugin.writer.doriswriter.DorisWriterManager.access$000(DorisWriterManager.java:19) at com.alibaba.datax.plugin.writer.doriswriter.DorisWriterManager$1.run(DorisWriterManager.java:134) at java.lang.Thread.run(Thread.java:750) Caused by: java.lang.ClassNotFoundException: com.alibaba.fastjson2.JSON at java.net.URLClassLoader.findClass(URLClassLoader.java:387) at java.lang.ClassLoader.loadClass(ClassLoader.java:418) at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ... 6 more 在datax\\datax\\plugin\\writer\\doriswriter\\lib下有Doris插件所需的工具包，这里有一个fastjson-1.1.46.sec10.jar。果然是版本依赖错误。 直接下载最新版本的fastjson2，替换这个jar包。 下面这个命令直接在lib文件夹下执行，会直接下在jar包到当前目录下。下载好后记得删除或者将原来的jar包改后缀。\n1 wget https://repo1.maven.org/maven2/com/alibaba/fastjson2/fastjson2/2.0.3/fastjson2-2.0.3.jar 然后就能正常执行了。\n","date":"2025-02-02T22:44:44+08:00","permalink":"https://rusthx.github.io/p/datax-%E7%BC%96%E8%AF%91%E6%8F%92%E4%BB%B6/","title":"DataX 编译插件"},{"content":"主从复制可以用来做数据库的实时备份，保证数据的完整性；也可以做读写分离，提升数据库系统整体的读写性能。\n主从复制原理 参考资料：https://xiaolincoding.com/mysql/log/how_update.html#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0 MySQL集群的主从复制过程梳理成3个阶段：\n写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引l擎中的数据。 具体详细过程如下： MySQL 主库在收到客户端提交事务的请求之后，会先写入binlog，再提交事务，更新存储引擎中的数 据，事务提交完成后，返回给客户端\u0026quot;操作成功\u0026quot;的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库\u0026quot;复制成功\u0026quot;的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中 的数据，最终实现主从的数据一致性。 在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者 锁记录，也不会影响读请求的执行。 主从复制配置 前置工作 在两台机上分别安装MySQL,相关教程可查看我的相关博客Ubuntu22.04安装MySQL8.0.35\nhadoop1和hadoop2两台机上都将安装好MySQL，然后hadoop1将作为主节点，hadoop2将作为从节点。主节点提前创建用户用来进行主从连接。注意要给slave权限\n1 2 3 CREATE USER \u0026#39;rust\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;rust\u0026#39;@\u0026#39;%\u0026#39; WITH GRANT OPTION; FLUSH PRIVILEGES; 修改配置文件 修改主库配置文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 sudo gedit /etc/mysql/my.cnf # 也可以通过vim修改，命令如下。 #sudo vim /etc/mysql/my.cnf # 添加如下内容 [mysqld] server-id = 1 log-bin=mysql-bin binlog_format=row #这一行可省略，因为MySQL在5.7.7开始的默认值就是row了 binlog-do-db=master_try #用来主从复制的数据库。需要提前创建 修改后保存重启生效。\n1 sudo systemctl restart mysql 修改从库配置文件如下：\n1 2 3 4 5 6 [mysqld] server-id=2 # 从数据库的唯一标识符，通常大于主数据库的server-id relay-log=mysql-relay-bin # 启用中继日志，用于在从数据库上复制主数据库的操作 read-only=1 # 设置从数据库为只读，防止在从数据库上直接写入数据 enforce_gtid_consistency = ON gtid_mode = ON 登录从库，连接到主库 登录主库，查看所需信息：\n1 2 mysql -uroot -p show master status; 登录从库\n1 2 3 4 5 6 7 8 9 mysql -uroot -p #登录mysql CHANGE MASTER TO MASTER_HOST=\u0026#39;192.168.146.161\u0026#39;, #指向主库 MASTER_USER=\u0026#39;rust\u0026#39;, #用于复制的MySQL账户,需要提前给定权限 MASTER_PASSWORD=\u0026#39;123456\u0026#39;, #密码 MASTER_LOG_FILE=\u0026#39;mysql-bin.000093\u0026#39;, #主库bin log名称，主库上用SHOW MASTER STATUS查看 MASTER_LOG_POS=197; #主库bin log位置，主库上用SHOW MASTER STATUS查看 START SLAVE; #启动从库进程 SHOW SLAVE STATUS\\G; #显示进程状态 注意这里一定要出现两个Yes才行。\n测试 登录主库，建表插入如下数据。\n1 2 3 4 5 6 7 8 use master_try; create table student( `id` int auto_increment primary key, `name` varchar(50), `age` int ); insert into student(id,`name`,age) values (\u0026#39;1\u0026#39;,\u0026#39;姬丝秀忒·雅赛劳拉莉昂·刃下心\u0026#39;,\u0026#39;598\u0026#39;); 登录从库，查看建表及插入情况。 同步成功，完成主从复制搭建。\n踩坑与备注 slave_io_running：no 这是因为从库连接主库填写的信息有误，比如log_file和pos。补救方法，在从库执行如下命令：\n1 2 3 4 5 stop slave; reset slave all; #再执行一遍正确的连接主库命令 从库show slave status\\G;正常，但查看不到主库数据 这种是主库建的数据库和主库配置文件开启的数据库不同导致的，也就是说主库没有建立配置文件里的数据库。 （我呆得不行，主库配置文件里写的master_try，结果我建表转眼就建成mastertry。查不到数据我还以为是权限的问题，走了好多弯路）\n","date":"2025-01-24T22:23:10+08:00","permalink":"https://rusthx.github.io/p/mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","title":"MySQL主从复制"},{"content":"2025/01/13 腾讯音乐运维一面\nLinux Linux启动顺序 BIOS/UEFI阶段 BIOS(Basic Input Output System)：基本输入输出系统。它是一组固化到计算机内主板上一个daoROM芯片上的程序，它保存着计算机最重要的基本输入输出的程序、系统设置信息、开机后自检程序和系统自启动程序。 其主要功能是为计算机提供最底层的、最直接的硬件设置和控制。BIOS应该是连接软件程序与硬件设备的一座\u0026quot;桥梁\u0026quot;，负责解决硬件的即时要求。简单地说，BIOS就是一个加载在计算机主板上最基础的一段程序，负责最基础的硬件控制和设置。\nMBR(Master Boot Record):主引导程序，是分区计算器大容量存储设备（如固定硬盘或移动硬盘）的第一个块中的一种引导扇区。\n加载位于主板ROM中地址0xFFFF0的BIOS信息，主要包括系统BIOS和显卡BIOS；进行POST自检，检查CPU、内存、主板等硬件；建立终端向量表和中断服务程序；检测可引导设备；加载MBR(主引导程序)的前446字节到内存0x7c00处。\n注：老电脑用MBR，较新的电脑，2017年之后的系统如Ubuntu17.04都默认使用GPT+UEFI组合。UEFI最大的好处就是启动方便，有图形化界面。\n目前主板多设置成三种启动模式，即：Auto、UEFI、Legacy。在设置启动时，带有UEFI的BIOS还提供了启动选项供大家选择以何种方式启，各种模式含义如下：\nAuto(自动)/Both：自动按照启动设备列表中的顺序启动，优先采用UEFI方式；\nUEFI only(仅UEFI)：只选择具备UEFI启动条件的设备启动；\nLegacy only(仅Legacy)：只选择具备Legacy启动条件的设备启动。\n简单的来说uefi启动是新一代的bios，功能更加强大，而且它是以图形图像模式显示，让用户更便捷的直观操作。\nMBR最大支持2TB硬盘;最多支持4个主分区，或3个主分区+1个扩展分区;扩展分区可以包含多个逻辑分区;分区表只有一份，易损坏;启动代码占446字节，分区表占64字节;兼容传统BIOS系统 GPT (GUID Partition Table)支持超过2TB的硬盘（最大18EB）;支持最多128个分区（理论上可更多）;分区表有主副两份，更安全;每个分区都有全球唯一标识符(GUID);支持更多分区类型;需要UEFI启动支持\nBoot Loader阶段 显示内核选择菜单；加载选定的内核镜像到内存；加载initramfs到内存；然后将控制权移交给内核\nKernel初始化阶段 系统读取内存映像，并进行解压缩操作。此时，屏幕一般会输出“Uncompressing Linux”的提示。当解压缩内核完成后，屏幕输出“OK, booting the kernel”。 系统将解压后的内核放置在内存之中，并调用start_kernel()函数来启动一系列的初始化函数并初始化各种设备，完成Linux核心环境的建立。 然后系统会初始化CPU、内存、设备驱动；挂载根文件系统；运行/sbin/init(PID 1)\nInit进程(systemd)阶段 Unit:systemd管理的基本单元，包括service服务、socket进程间通信套接字、target启动目标（类似于运行级别）、mount文件系统挂载点、device设备文件、timer定时器。配置文件优先级依次是：/etc/systemd/system/ 系统管理员创建的配置，优先级最高;/run/systemd/system/ 运行时配置文件;/usr/lib/systemd/system/软件包安装的默认配置。\n读取默认target配置文件；分析unit间依赖关系；激活系统服务；准备各类daemon进程；准备用户环境。\n备注：系统级自启动程序就是在这时候启动的，如通过apt安装的Mysql、Docker。\n用户空间阶段 依次读取/etc/profile系统环境变量、 ~/.bashrc用户环境变量、 /etc/passwd用户账户信息、 /etc/shadow用户密码信息 启动显示管理器，加载桌面环境，准备终端设备；等待用户登录\n备注：用户级自启动程序在读取~/.bashrc后自动执行，也就是说想要设置用户级自启动命令可以将启动命令写入到~/.bashrc中\nLinux开机自启动 Linux有三种级别的开机自启动：\n系统级别 通过apt包管理器安装的包默认就是这种级别的开机自启。 可以通过 sudo systemctl enable mysql来启用开机自启（sudo systemctl disable mysql为关闭开机自启命令）。\n除了包管理器，还可以自己制作service来实现，以nginx为例：\n在/etc/systemd/system/下创建nginx.service文件 1 2 cd /etc/systemd/system/ vim nginx.service 写入如下内容：\n1 2 3 4 5 6 7 8 9 10 11 [Unit] Description=nginx - high performance web server After=nginx.service [Service] Type=forking ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/usr/local/nginx/sbin/nginx -s stop Execenable=/usr/local/nginx/sbin/nginx [Install] WantedBy=multi-user.target 设置开机自启 1 2 3 4 5 6 7 8 9 10 11 12 # 设置开机启动 systemctl enable nginx # 取消开机自启动 #systemctl disable nginx # 查看服务当前状态 systemctl status nginx # 启动nginx服务 systemctl start nginx # 停止nginx服务 systemctl stop nginx # 重启nginx服务 systemctl restart nginx 用户级别 将需要自启动的命令写入~/.bashrc中，即可实现用户级开机自启。具体操作时如果命令较为简单，比如使用alias给命令指定别名或者ulimit -n 65536修改最大打开文件句柄数（我的Ubuntu不知道为什么永久修改怎么也修改不成功，只能通过这种方式曲线救国），就可以直接把命令写到~/.bashrc中。如果命令比较复杂，可以考虑将命令打包成脚本，然后在~/.bashrc中执行启动脚本的命令。\n桌面级别 大体步骤类似系统级自启动：切换到/etc/xdg/autostart/目录，创建后缀为desktop的文件，编辑并保存。重启生效。 数据库 MySQL主从复制配置过程，宕机恢复 参考资料：https://xiaolincoding.com/mysql/log/how_update.html#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0\n主从复制可以用来做数据库的实时备份，保证数据的完整性；也可以做读写分离，提升数据库系统整体的读写性能。\n主从复制原理： MySQL集群的主从复制过程梳理成3个阶段：\n写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引l擎中的数据。 具体详细过程如下： MySQL 主库在收到客户端提交事务的请求之后，会先写入binlog，再提交事务，更新存储引擎中的数 据，事务提交完成后，返回给客户端\u0026quot;操作成功\u0026quot;的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库\u0026quot;复制成功\u0026quot;的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中 的数据，最终实现主从的数据一致性。 在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者 锁记录，也不会影响读请求的执行。 主从复制配置过程 参考资料：https://www.cnblogs.com/nulige/articles/9273537.html 两台机器都操作，确保 server-id 要不同，通常主ID要小于从ID。一定注意。\nMySQL binlog格式 binlog 是 MySQL 中的一种重要日志类型，用于记录所有的DDL和DML操作（不包括数据查询语句）。它主要用于数据恢复和主从复制。Binlog有三种模式：STATEMENT(语句模式)、ROW(行模式) 和 MIXED(混合模式)\nSTATEMENT模式：基于SQL语句的复制（Statement-Based Replication, SBR），每一条修改数据的SQL语句都会记录到binlog中\n优点：减少日志量：不需要记录每一行数据的变化，减少了磁盘IO，提高了性能 缺点：数据不一致：在某些情况下，主从库的数据可能会不一致。例如，使用 uuid() 函数时，每次执行都会生成不同的值；自增字段在执行时可能得到错误的值 ROW模式：基于行的复制（Row-Based Replication, RBR），不记录SQL语句的上下文信息，仅记录哪条数据被修改了\n优点：准确复制：不会出现存储过程、函数或触发器调用无法正确复制的问题 缺点：日志量大：尤其是在执行批量更新或删除操作时，会产生大量日志，影响IO性能 MIXED模式：是前两种模式的混合体（Mixed-Based Replication, MBR），MySQL会根据具体的SQL语句选择使用STATEMENT或ROW模式。 一般情况下，使用STATEMENT模式保存binlog，对于无法准确复制的操作则使用ROW模式\n配置和查看Binlog\n要开启和配置Binlog，可以修改MySQL的配置文件 my.cnf\n1 2 3 4 [mysqld] log-bin=ON binlog_format=mixed server-id=1 查看Binlog日志可以使用 mysqlbinlog 工具\n1 mysqlbinlog mysql-bin.000001 或者使用SQL命令查看事件：\n1 SHOW BINLOG EVENTS IN \u0026#39;mysql-bin.000001\u0026#39;; 通过合理选择和配置Binlog模式，可以有效提高MySQL的性能和数据一致性\nMySQL 备份，逻辑备份与物理备份 MySQL的物理备份和逻辑备份的主要区别在于备份文件的形式和备份恢复的灵活性。 物理备份直接复制数据库的二进制文件(binlog)，备份文件较大，恢复时只能在相同架构的MySQL服务器上使用。 逻辑备份将数据库导出为SQL语句的形式，备份文件较小，恢复时可跨平台使用，也可以进行数据的修改和筛选。\n容器化 k8s移动pod 参考资料：https://blog.csdn.net/yanggd1987/article/details/108139436\nk8s集群中的node节点要升级内存，以应对服务迁入、pod扩缩容导致的资源短缺，需要对node节点进行停机维护。此时就需要对pod进行迁移。\n默认迁移 当node节点关机后，k8s集群并没有立刻发生任何自动迁移动作，如果该node节点上的副本数为1，则会出现服务中断的情况。其实事实并非如此，k8s在等待5分钟后，会自动将停机node节点上的pod自动迁移到其他node节点上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 1.模拟node节点停机，停止kubelet systemctl stop kubelet # 2.集群状态 # 此时停机的node点处于NotReady状态 # kubectl get node NAME STATUS ROLES AGE VERSION k8s-3-217 Ready master 88d v1.18.2 k8s-3-218 NotReady \u0026lt;none\u0026gt; 88d v1.18.2 k8s-3-219 Ready \u0026lt;none\u0026gt; 88d v1.18.2 # 3.监控pod状态，大约等待5分钟左右，集群开始有动作 # kubectl get pod -n test -o wide -n test -w NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES helloworld-79956d95b4-q7jjg 1/1 Running 0 19h 10.244.1.154 k8s-3-218 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helloworld-79956d95b4-q7jjg 1/1 Running 0 19h 10.244.1.154 k8s-3-218 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 5分钟后，pod终止并进行重建 helloworld-79956d95b4-q7jjg 1/1 Terminating 0 19h 10.244.1.154 k8s-3-218 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helloworld-79956d95b4-nnlrq 0/1 Pending 0 0s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helloworld-79956d95b4-nnlrq 0/1 Pending 0 0s \u0026lt;none\u0026gt; k8s-3-219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helloworld-79956d95b4-nnlrq 0/1 ContainerCreating 0 1s \u0026lt;none\u0026gt; k8s-3-219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helloworld-79956d95b4-nnlrq 0/1 Running 0 3s 10.244.2.215 k8s-3-219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helloworld-79956d95b4-nnlrq 1/1 Running 0 66s 10.244.2.215 k8s-3-219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 4.访问测试：在pod重新迁移到其他node节点时，服务是不可用的。 # curl -x 192.168.3.219:80 hello.test.cn \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;503 Service Temporarily Unavailable\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;503 Service Temporarily Unavailable\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.17.8\u0026lt;/center\u0026gt; # 5.迁移完毕：服务正常访问 # curl -x 192.168.3.219:80 hello.test.cn Hello,world! 从以上过程看出，停机node节点上的pod在5分钟后先终止再重建，直到pod在新节点启动并由readiness探针检测正常后并处于1\\1 Running状态才可以正式对外提供服务。因此服务中断时间=停机等待5分钟时间+重建时间+服务启动时间+readiness探针检测正常时间。\n为什么pod在5分钟后开始迁移呢? 此时需要涉及到k8s中的Taint（污点）和 Toleration（容忍），这是从Kubernetes 1.6开始提供的高级调度功能。Taint和Toleration相互配合，可以避免pod被分配到不合适的节点上。每个节点上都可以应用一个或多个Taint，这表示对于那些不能容忍Taint的pod，是不会被该节点接受的。如果将Toleration应用于pod上，则表示这些pod可以（但不要求）被调度到具有匹配Taint的节点上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 1.查看停止服务节点的状态 # kubelet停止后，node节点自动添加了Taints； # kubectl describe node k8s-3-218 Name: k8s-3-218 Roles: \u0026lt;none\u0026gt; CreationTimestamp: Fri, 22 May 2020 11:36:16 +0800 Taints: node.kubernetes.io/unreachable:NoExecute node.kubernetes.io/unreachable:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-3-218 AcquireTime: \u0026lt;unset\u0026gt; RenewTime: Wed, 19 Aug 2020 09:31:22 +0800 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Tue, 18 Aug 2020 09:07:59 +0800 Tue, 18 Aug 2020 09:07:59 +0800 FlannelIsUp Flannel is running on this node MemoryPressure Unknown Wed, 19 Aug 2020 09:29:56 +0800 Wed, 19 Aug 2020 09:32:07 +0800 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Wed, 19 Aug 2020 09:29:56 +0800 Wed, 19 Aug 2020 09:32:07 +0800 NodeStatusUnknown Kubelet stopped posting node status. PIDPressure Unknown Wed, 19 Aug 2020 09:29:56 +0800 Wed, 19 Aug 2020 09:32:07 +0800 NodeStatusUnknown Kubelet stopped posting node status. Ready Unknown Wed, 19 Aug 2020 09:29:56 +0800 Wed, 19 Aug 2020 09:32:07 +0800 NodeStatusUnknown Kubelet stopped posting node status. ...省略... Events: \u0026lt;none\u0026gt; # 2.查看pod状态 # kubectl describe pod helloworld-8565c4687b-rrfmj -n test Name: helloworld-8565c4687b-rrfmj Namespace: test Priority: 0 ...... Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: \u0026lt;none\u0026gt; 此时pod的Tolerations 默认对于具有相应Taint的node节点容忍时间为300s，超过此时间pod将会被驱逐到其他可用node节点上。因此5分钟后node节点上所有的pod重新被调度，在此期间服务是中断的。\n手动迁移 默认的pod迁移无法避免服务中断，那么我们在node节点停机前，我们可以手动迁移。 为避免等待默认的5分钟，我们还可以使用cordon、drain、uncordor三个命令实现节点的主动维护。此时需要用到以下三个命令：\ncordon:标记节点不可调度，后续新的pod不会被调度到此节点，但是该节点上的pod可以正常对外服务； drain:驱逐节点上的pod至其他可调度节点； uncordon:标记节点可调度 具体操作如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 1.标记节点不可调度 # kubectl cordon k8s-3-219 node/k8s-3-219 cordoned # 查看节点状态，此时219被标记为不可调度 # kubectl get node NAME STATUS ROLES AGE VERSION k8s-3-217 Ready master 89d v1.18.2 k8s-3-218 Ready \u0026lt;none\u0026gt; 88d v1.18.2 k8s-3-219 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 88d v1.18.2 # 2.驱逐pod # kubectl drain k8s-3-219 --delete-local-data --ignore-daemonsets --force node/k8s-3-219 already cordoned WARNING: ignoring DaemonSet-managed Pods: ingress-nginx/nginx-ingress-controller-gmzq6, kube-system/kube-flannel-ds-amd64-5gfwh, kube-system/kube-proxy-vdckk evicting pod kube-system/tiller-deploy-6c65968d87-75pfm evicting pod kube-system/metrics-server-7f96bbcc66-bgt7j evicting pod test/helloworld-79956d95b4-nnlrq # 参数如下： --delete-local-data 删除本地数据，即使emptyDir也将删除； --ignore-daemonsets 忽略DeamonSet，否则DeamonSet被删除后，仍会自动重建； --force 不加force参数只会删除该node节点上的ReplicationController, ReplicaSet, DaemonSet,StatefulSet or Job，加上后所有pod都将删除； # 3. 查看驱逐，219上的pod迁移到218上了。 # kubectl get pod -n test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES helloworld-79956d95b4-gg58c 0/1 Running 0 20s 10.244.1.165 k8s-3-218 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; helloworld-79956d95b4-nnlrq 1/1 Terminating 0 77m 10.244.2.215 k8s-3-219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 此时与默认迁移不同的是，pod会先重建再终止，此时的服务中断时间=重建时间+服务启动时间+readiness探针检测正常时间，必须等到1/1 Running服务才会正常。因此在单副本时迁移时，服务中断是不可避免的。\n平滑迁移 要做到平滑迁移就需要用的pdb(PodDisruptionBudget)，即主动驱逐保护。无论是默认迁移和手动迁移，都会导致服务中断，而pdb能可以实现节点维护期间不低于一定数量的pod正常运行，从而保证服务的可用性。\n在仍以helloworld为例，由于只有一个副本，因此需要保证维护期间这个副本在迁移完成后，才会终止。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # 从218 驱逐到 219 # 1.标记节点不可调度 # kubectl cordon k8s-3-218 node/k8s-3-218 cordoned # 2.新建pdb vim pdb-test.yaml apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: pdb-test namespace: test spec: minAvailable: 1 selector: matchLabels: app: helloworld # 2.应用并查看状态 # kubectl apply -f pdb-test.yaml # kubectl get pdb -n test NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE pdb-test 1 N/A 0 7s # 3.驱逐 # kubectl drain k8s-3-218 --delete-local-data --ignore-daemonsets --force node/k8s-3-218 already cordoned WARNING: ignoring DaemonSet-managed Pods: ingress-nginx/nginx-ingress-controller-hhb6h, kube-system/kube-flannel-ds-amd64-pb4d7, kube-system/kube-proxy-rzdcj evicting pod kube-system/tiller-deploy-6c65968d87-ktqmm evicting pod kube-system/metrics-server-7f96bbcc66-6p6wm evicting pod test/helloworld-79956d95b4-gg58c error when evicting pod \u0026#34;helloworld-79956d95b4-gg58c\u0026#34; (will retry after 5s): Cannot evict pod as it would violate the pod\u0026#39;s disruption budget. evicting pod test/helloworld-79956d95b4-gg58c error when evicting pod \u0026#34;helloworld-79956d95b4-gg58c\u0026#34; (will retry after 5s): Cannot evict pod as it would violate the pod\u0026#39;s disruption budget. pod/tiller-deploy-6c65968d87-ktqmm evicted #此时由于只有一个副本，最小可用为1，则ALLOWED就为0，因此在一个副本通过pdb是不会发生驱逐的。我们需要先扩容，将副本数调整为大于1。 # 3.手动调整副本数，将replicas为2 kubectl edit deploy helloworld -n test # 再次查看pdb # kubectl get pdb -n test NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE pdb-test 1 N/A 1 13m #此时最小可用为1 ，ALLOWED为1，此时是数量会自动计算。 # 4.驱逐 # kubectl drain k8s-3-218 --delete-local-data --ignore-daemonsets --force # 5.维护完毕，将node调整为可调度 # kubectl uncordon k8s-3-218 本次驱逐，由于helloworld始终保持有一个pod在提供服务，因此服务是不中断的。 最后将副本数可以调节为1并将node节点调整为可调度，维护完毕。\n总结:通过简单了解了Taint（污点）和 Toleration（容忍）作用，我们既可以通过设置tolerationSeconds来缩短等待时间，也可以自行定义匹配规则实现符合实际情况的调度规则。\n另外还要注意先重建再终止和先终止再重建，在此过程中服务启动时间和探针检测时间决定你的服务中断时间。\nk8s镜像拉取配置参数及优先级 参考：https://www.cnblogs.com/leojazz/p/18686403\n在 Kubernetes（简称 K8s）中，容器镜像的更新行为主要由 imagePullPolicy 参数控制。该策略决定了 Kubernetes 在启动或重启容器时是否从镜像仓库拉取新的镜像版本。常见的镜像更新策略有三种：\nAlways 如果容器的imagePullPolicy设置为 Always，每次创建 Pod 或者重启容器时，Kubelet 都会从镜像仓库拉取最新的镜像版本。这对于使用 latest 标签或者希望始终获取最新镜像的场景非常有用，但在生产环境中应谨慎使用，因为 latest 标签的镜像内容可能会随时变化，导致版本不一致或潜在的不稳定。 建议： 在生产环境中避免直接使用 latest 标签，使用明确的版本号（如 v1.0.1）来确保一致性，并记录镜像版本历史以便于追踪和排查问题。\nIfNotPresent（默认值） 当imagePullPolicy设置为 IfNotPresent 时，如果本地节点上已经存在该镜像，则 Kubelet 不会尝试从镜像仓库拉取镜像；仅当本地不存在该镜像时，Kubelet 才会去远程仓库拉取镜像。通常，使用带有明确版本标签（如 v1.0）的镜像时，推荐使用此策略，以避免不必要的镜像拉取。 注意： 默认情况下，如果镜像标签是 latest，imagePullPolicy 会自动设置为 Always；如果是版本号标签（如 v1.0），则默认使用 IfNotPresent。\nNever 如果imagePullPolicy设置为 Never，无论本地是否存在该镜像，Kubelet 都不会尝试从镜像仓库拉取镜像，而是始终使用本地已有的镜像。这种策略适用于不希望自动升级镜像版本，且希望始终使用特定版本的场景。 更新应用镜像的常见方法 在 Kubernetes 中，更新应用镜像的常见方法是通过修改 Deployment、StatefulSet 等控制器中定义的 Pod 模板内的镜像版本，然后执行 kubectl apply 命令将更改推送到集群，触发滚动更新。\n滚动更新的过程中，Kubernetes 会逐步替换旧的容器实例，确保服务的持续可用性。您可以使用以下命令来更新镜像版本：\n示例：更新 Deployment 中的镜像 假设我们有一个名为 example-deployment 的 Deployment，其中定义了一个容器镜像 myapp:v1.0。我们将镜像版本更新为 myapp:v2.0，并通过 kubectl apply 命令触发更新。\n修改 Deployment 配置文件 deployment.yaml 中的镜像版本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: apps/v1 kind: Deployment metadata: name: example-deployment spec: replicas: 3 selector: matchLabels: app: example template: metadata: labels: app: example spec: containers: - name: example-container image: myapp:v2.0 # 修改为新的镜像版本 imagePullPolicy: IfNotPresent 执行 kubectl apply 命令将变更推送到集群： kubectl apply -f deployment.yaml 在执行该命令后，Kubernetes 会根据定义的镜像更新策略（例如，imagePullPolicy: IfNotPresent）决定是否从仓库拉取新的镜像，并按滚动更新的方式逐步替换旧版本的容器实例。\n滚动更新控制 在滚动更新过程中，您还可以通过以下参数控制更新过程的行为：\nmaxUnavailable：定义更新过程中允许不可用的最大 Pod 数量，控制更新期间服务的最小可用性。 maxSurge：定义在更新过程中可以超出期望副本数的最大 Pod 数量，帮助提升更新的速度。 合理设置这两个参数可以在保证服务可用性的同时加速或控制更新过程。例如： 1 2 3 4 5 6 spec: replicas: 3 strategy: rollingUpdate: maxUnavailable: 1 # 最多允许 1 个 Pod 不可用 maxSurge: 1 # 可以增加 1 个 Pod 以加速更新 通过设置这些参数，您可以精细控制容器的滚动更新行为，以平衡服务可用性和更新速度。\n这种修订版本提供了更为清晰的策略解释、更新的实践方法和对滚动更新的详细控制，适用于生产环境中的实际操作。\nk8s镜像仓库 k8s镜像仓库分为远程仓库和本地仓库。本地仓库就是存储在本机的镜像文件，可以通过工具(如Docker Registry、Harbor)搭建，用于管理和分发镜像；\n由于 Kubernetes 使用的是容器运行时(如 containerd),直接在 Docker 中构建的镜像无法直接被 Kubernetes 使用。需要将镜像导入到对应的容器运行时中。\n1 2 3 4 5 # containerd：使用 ctr 工具导入镜像 ctr -n k8s.io images import my-image.tar # CRI-O：使用 skopeo 或 crictl 工具导入镜像 crictl load my-image.tar Kubernetes 较新版本（1.24+）已移除对 Docker 的支持，转为使用 containerd 或 CRI-O。 不同运行时的镜像存储路径如下： Docker：/var/lib/docker containerd：/var/lib/containerd CRI-O：/var/lib/containers 如果需要使用本地镜像，需根据运行时类型将镜像导入到对应的存储系统中，例如使用 ctr 或 crictl。 Kubernetes 配置文件中需设置 imagePullPolicy 为 Never，并确保所有节点都加载了本地镜像。 远程仓库\n公共镜像仓库: 这些仓库通常是开放和免费的，供开发者和用户共享或下载镜像。\nDocker Hub:Docker 官方提供的公共镜像仓库，默认与 Docker CLI 配合使用。\nGoogle Container Registry (GCR):Google 提供的镜像仓库(k8s1.24版本前的默认仓库)，适合与 Google Cloud Platform (GCP) 集成。k8s1.24后的默认仓库：https://registry.k8s.io\nGitHub Container Registry (GHCR):GitHub 提供的容器镜像服务，适合与 GitHub 项目结合使用。\nQuay.io:Red Hat 提供的镜像仓库，支持高级功能和企业用途。\n私有镜像仓库: 为了安全性和隐私原因，很多企业会搭建自己的私有镜像仓库。\nHarbor:一个开源的企业级私有镜像仓库，支持镜像管理、访问控制和漏洞扫描。\nArtifactory:JFrog 提供的企业级解决方案，支持多种包管理器，包括 Docker 镜像。\n自建私有仓库:使用 Docker Registry 开源项目搭建的简单私有仓库。\n云厂商提供的镜像仓库: 各大云厂商为其云服务提供了专属的容器镜像仓库，方便与云平台集成。\nAmazon Elastic Container Registry (ECR):AWS 提供的镜像仓库，深度集成 AWS 生态。\nAzure Container Registry (ACR):Azure 提供的镜像仓库，支持与 Azure Kubernetes 服务无缝集成。\nAlibaba Cloud Container Registry (ACR):阿里云提供的镜像服务，支持国内加速访问和与阿里云服务集 成。\nTencent Container Registry (TCR):腾讯云提供的镜像仓库，适合国内用户使用。\nHuawei Cloud SWR (SoftWare Repository for Container):华为云提供的容器镜像服务，支持企业级容器管理。\n国内公共镜像加速器 由于国内网络环境的限制，许多镜像从国外仓库拉取较慢，因此国内厂商提供了一些公共镜像加速器。\n阿里云容器镜像服务加速器:https://cr.console.aliyun.com\n腾讯云容器镜像服务加速器：https://cloud.tencent.com/document/product/457/35996\n华为云容器镜像加速器：https://www.huaweicloud.com/product/swr.html\n网易云加速器：https://www.163yun.com/product/container\n监控 参考：https://flashcat.cloud/blog/ops-monitor/\nnginx ","date":"2025-01-13T14:58:48+08:00","permalink":"https://rusthx.github.io/p/%E8%BF%90%E7%BB%B4%E9%9D%A2%E7%BB%8F/","title":"运维面经"},{"content":" 作者由于水平问题，文中也许有一些错误遗漏的地方，欢迎联系指正(2024087171@qq.com)\n简介 参考资料：https://blog.csdn.net/weixin_42868529/article/details/84622803\nShuffle 过程本质上都是将 Map 端获得的数据使用分区器进行划分，并将数据发送给对应的 Reducer 的过程。\n前一个stage的ShuffleMapTask进行shuffle write，把数据存储在blockManager上面，并且把数据元信息上报到dirver的mapOutTarck组件中，下一个stage根据数据位置源信息，进行shuffle read，拉取上一个stage的输出数据\nHadoop(MapReduce) shuffle 参考资料：尚硅谷Hadoop相关课程\nMapReduce的shuffle机制：\nMapTask收集我们的map()方法输出的kv对，放到环形缓冲区中 从环形缓冲区不断溢写本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序 ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据 ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序） 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法） Spark shuffle详细机制 Spark的shuffle机制：\n前提条件：Spark的代码在运行到action算子时触发任务（job），然后DAGscheduler按算子间的血缘（依赖关系）划分形成DAG图（有向无环图）， 有shuffle操作的依赖关系称为宽依赖，没有的称为窄依赖。DAGscheduler按宽依赖将任务划分为多个stage，stage的数量就等于宽依赖数量+1。\n根据 spark.shuffle.manager 设置，SparkEnv 会在driver和每个executor上创建一个 ShuffleManager。 driver在其中注册shuffle，executor（或在driver中本地运行的任务）可以要求读写数据。 前一个stage的ShuffleMapTask将mapTaskID和partitions传入sortShuffleManager，调用getWriter（）方法后，首先会判断是否需要对计算结果进行聚合，然后将最终结果按照不同的 reduce 端进行区分，返回writeHandle, 根据不同的writeHandle选择不同的writer（UnsafeShuffleWriter、BypassMergeSortShuffleWriter、SortShuffleWriter） writer将数据写入到executor的blockManager中 shuffleManager为后一个stage创建一个BlockStoreShuffleReader，根据位置信息（startMapIndex, endMapIndex, startPartition, endPartition）拉取blockManger中的数据，根据数据的 Key 进行聚合，然后判断是否需要排序，最后生成新的 RDD。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 /** * Obtains a [[ShuffleHandle]] to pass to tasks. */ override def registerShuffle[K, V, C]( shuffleId: Int, dependency: ShuffleDependency[K, V, C]): ShuffleHandle = { if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) { // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don\u0026#39;t // need map-side aggregation, then write numPartitions files directly and just concatenate // them at the end. This avoids doing serialization and deserialization twice to merge // together the spilled files, which would happen with the normal code path. The downside is // having multiple files open at a time and thus more memory allocated to buffers. new BypassMergeSortShuffleHandle[K, V]( shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]]) } else if (SortShuffleManager.canUseSerializedShuffle(dependency)) { // Otherwise, try to buffer map outputs in a serialized form, since this is more efficient: new SerializedShuffleHandle[K, V]( shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]]) } else { // Otherwise, buffer map outputs in a deserialized form: new BaseShuffleHandle(shuffleId, dependency) } } /** * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive) to * read from a range of map outputs(startMapIndex to endMapIndex-1, inclusive). * If endMapIndex=Int.MaxValue, the actual endMapIndex will be changed to the length of total map * outputs of the shuffle in `getMapSizesByExecutorId`. * * Called on executors by reduce tasks. */ override def getReader[K, C]( handle: ShuffleHandle, startMapIndex: Int, endMapIndex: Int, startPartition: Int, endPartition: Int, context: TaskContext, metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C] = { val baseShuffleHandle = handle.asInstanceOf[BaseShuffleHandle[K, _, C]] val (blocksByAddress, canEnableBatchFetch) = if (baseShuffleHandle.dependency.isShuffleMergeFinalizedMarked) { val res = SparkEnv.get.mapOutputTracker.getPushBasedShuffleMapSizesByExecutorId( handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition) (res.iter, res.enableBatchFetch) } else { val address = SparkEnv.get.mapOutputTracker.getMapSizesByExecutorId( handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition) (address, true) } new BlockStoreShuffleReader( handle.asInstanceOf[BaseShuffleHandle[K, _, C]], blocksByAddress, context, metrics, shouldBatchFetch = canEnableBatchFetch \u0026amp;\u0026amp; canUseBatchFetch(startPartition, endPartition, context)) } /** Get a writer for a given partition. Called on executors by map tasks. */ override def getWriter[K, V]( handle: ShuffleHandle, mapId: Long, context: TaskContext, metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V] = { val mapTaskIds = taskIdMapsForShuffle.computeIfAbsent( handle.shuffleId, _ =\u0026gt; new OpenHashSet[Long](16)) mapTaskIds.synchronized { mapTaskIds.add(mapId) } val env = SparkEnv.get handle match { case unsafeShuffleHandle: SerializedShuffleHandle[K @unchecked, V @unchecked] =\u0026gt; new UnsafeShuffleWriter( env.blockManager, context.taskMemoryManager(), unsafeShuffleHandle, mapId, context, env.conf, metrics, shuffleExecutorComponents) case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =\u0026gt; new BypassMergeSortShuffleWriter( env.blockManager, bypassMergeSortHandle, mapId, env.conf, metrics, shuffleExecutorComponents) case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =\u0026gt; new SortShuffleWriter(other, mapId, context, shuffleExecutorComponents) } } 与Hadoop(MapReduce) shuffle的区别 参考：https://zhuanlan.zhihu.com/p/136466667\n功能上，MR的shuffle和Spark的shuffle是没啥区别的，都是对Map端的数据进行分区，要么聚合排序，要么不聚合排序，然后Reduce端或者下一个调度阶段进行拉取数据，完成map端到reduce端的数据传输功能。 方案上，有很大的区别，MR的shuffle是基于合并排序的思想，在数据进入reduce端之前，都会进行sort，为了方便后续的reduce端的全局排序，而Spark的shuffle是可选择的聚合，特别是1.2之后，需要通过调用特定的算子才会触发排序聚合的功能。 流程上，MR的Map端和Reduce区分非常明显，两块涉及到操作也是各司其职，而Spark的RDD是内存级的数据转换，不落盘，所以没有明确的划分，只是区分不同的调度阶段，不同的算子模型。 数据拉取，MR的reduce是直接拉取Map端的分区数据，而Spark是根据MapId和TaskContext读取，而且是在action触发的时候才会拉取数据。 ShuffleWriter及其选择策略 UnsafeShuffleWriter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @VisibleForTesting public void write(Iterator\u0026lt;Product2\u0026lt;K, V\u0026gt;\u0026gt; records) throws IOException { write(JavaConverters.asScalaIteratorConverter(records).asScala()); } @Override public void write(scala.collection.Iterator\u0026lt;Product2\u0026lt;K, V\u0026gt;\u0026gt; records) throws IOException { // Keep track of success so we know if we encountered an exception // We do this rather than a standard try/catch/re-throw to handle // generic throwables. boolean success = false; try { while (records.hasNext()) { insertRecordIntoSorter(records.next()); } closeAndWriteOutput(); success = true; } finally { if (sorter != null) { try { sorter.cleanupResources(); } catch (Exception e) { // Only throw this error if we won\u0026#39;t be masking another // error. if (success) { throw e; } else { logger.error(\u0026#34;In addition to a failure during writing, we failed during \u0026#34; + \u0026#34;cleanup.\u0026#34;, e); } } } } } @VisibleForTesting void insertRecordIntoSorter(Product2\u0026lt;K, V\u0026gt; record) throws IOException { assert(sorter != null); final K key = record._1(); final int partitionId = partitioner.getPartition(key); serBuffer.reset(); serOutputStream.writeKey(key, OBJECT_CLASS_TAG); serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG); serOutputStream.flush(); final int serializedRecordSize = serBuffer.size(); assert (serializedRecordSize \u0026gt; 0); sorter.insertRecord( serBuffer.getBuf(), Platform.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId); } 当shuffle后的分区数小于等于sortShuffleManager的最大分区数时，进行unsafeShuffle。主要步骤：\n将内存中的对象通过Java可迭代对象转换器转换为Scala的可迭代对象（并没有进行序列化相关操作，只是为了兼容性） 判断排序器（sorter）是否为空，使用分区器(partitioner)确认记录所属分区 重置序列化缓冲区，遍历可迭代对象（iterator），将记录的键和值依次序列化后写入，并刷写脏页，确保数据写入缓冲区 将序列化记录插入到排序器中。sorter负责按分区组织记录，并可能在每个分区内对其进行排序。方法使用缓冲区、偏移量、大小和分区ID来正确放置记录。 sorter什么时候排序？\nShuffle操作的需求（最高优先级）： 如果上层的Spark操作（如sortByKey）要求数据在每个分区内有序，那么排序器会对数据进行排序。 对于不需要排序的操作（如groupByKey），排序器可能只负责将数据按分区组织，而不进行排序。 排序器的类型和实现： Spark中有多种排序器实现，例如UnsafeExternalSorter。这些排序器可以根据需要对数据进行排序。 如果排序器的实现支持排序，并且配置要求排序，那么数据会在每个分区内被排序。 配置和优化（最低优先级）： Spark的某些配置参数可以影响排序行为。例如，spark.shuffle.sort.bypassMergeThreshold可以决定在某些情况下是否绕过排序。 在某些优化场景下，为了提高性能，Spark可能会选择不进行排序。 BypassMergeSortShuffleWriter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @Override public void write(Iterator\u0026lt;Product2\u0026lt;K, V\u0026gt;\u0026gt; records) throws IOException { assert (partitionWriters == null); ShuffleMapOutputWriter mapOutputWriter = shuffleExecutorComponents .createMapOutputWriter(shuffleId, mapId, numPartitions); try { if (!records.hasNext()) { partitionLengths = mapOutputWriter.commitAllPartitions( ShuffleChecksumHelper.EMPTY_CHECKSUM_VALUE).getPartitionLengths(); mapStatus = MapStatus$.MODULE$.apply( blockManager.shuffleServerId(), partitionLengths, mapId); return; } final SerializerInstance serInstance = serializer.newInstance(); final long openStartTime = System.nanoTime(); partitionWriters = new DiskBlockObjectWriter[numPartitions]; partitionWriterSegments = new FileSegment[numPartitions]; for (int i = 0; i \u0026lt; numPartitions; i++) { final Tuple2\u0026lt;TempShuffleBlockId, File\u0026gt; tempShuffleBlockIdPlusFile = blockManager.diskBlockManager().createTempShuffleBlock(); final File file = tempShuffleBlockIdPlusFile._2(); final BlockId blockId = tempShuffleBlockIdPlusFile._1(); DiskBlockObjectWriter writer = blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics); if (partitionChecksums.length \u0026gt; 0) { writer.setChecksum(partitionChecksums[i]); } partitionWriters[i] = writer; } // Creating the file to write to and creating a disk writer both involve interacting with // the disk, and can take a long time in aggregate when we open many files, so should be // included in the shuffle write time. writeMetrics.incWriteTime(System.nanoTime() - openStartTime); while (records.hasNext()) { final Product2\u0026lt;K, V\u0026gt; record = records.next(); final K key = record._1(); partitionWriters[partitioner.getPartition(key)].write(key, record._2()); } for (int i = 0; i \u0026lt; numPartitions; i++) { try (DiskBlockObjectWriter writer = partitionWriters[i]) { partitionWriterSegments[i] = writer.commitAndGet(); } } partitionLengths = writePartitionedData(mapOutputWriter); mapStatus = MapStatus$.MODULE$.apply( blockManager.shuffleServerId(), partitionLengths, mapId); } catch (Exception e) { try { mapOutputWriter.abort(e); } catch (Exception e2) { logger.error(\u0026#34;Failed to abort the writer after failing to write map output.\u0026#34;, e2); e.addSuppressed(e2); } throw e; } } BypassMergeSortShuffleWriter，专门用于处理小规模的 shuffle 操作。它通过绕过排序步骤来提高性能，适用于分区数较少的情况。\n初始化和检查：确保在写入开始时，partitionWriters 为空，防止重复初始化，然后创建一个用于写入 shuffle 输出的对象(ShuffleMapOutputWriter)。 处理空记录集：如果没有记录需要写入，更新 map 状态,直接向blockkManager提交所有分区并返回。 初始化序列化和写入器：创建一个新的序列化实例，初始化分区磁盘写入器数组(DiskBlockObjectWriter[numPartitions])，初始化分区文件段数组。 创建分区写入器：循环遍历每个分区，创建临时的 shuffle 块和对应的磁盘写入器。为每个分区创建一个磁盘写入器。 写入记录： 遍历所有记录，根据键的分区，将记录写入对应的分区写入器。 提交和获取分区数据：遍历每个分区，提交写入的数据并获取文件段。提交写入并获取文件段信息。 写入分区数据和更新状态： 将分区数据写入输出。更新 map 状态。 文件段信息通常指的是每个分区在磁盘上的物理存储信息，包括：文件路径(数据在磁盘上的具体存储位置)、偏移量(数据在文件中的起始位置)、 长度(数据的字节长度)。\n更新 map 状态是指在 shuffle 写入完成后，更新 Spark 的内部状态以反映当前任务的输出状态。具体来说：\nMapStatus: 这是 Spark 用于跟踪每个 map 任务输出状态的对象（在一个 stage 中，map 任务完成后生成的输出信息。这些信息用于指导后续的 shuffle 操作，确保数据能够正确地传递到下一个 stage 的 reduce 任务中。）。它包含了每个分区的数据长度信息。 源码中介绍如下：\nResult returned by a ShuffleMapTask to a scheduler. Includes the block manager address that the task has shuffle files stored on as well as the sizes of outputs for each reducer, for passing on to the reduce tasks.\nShuffleMapTask 返回给调度程序的结果。 包括该任务存储了 Shuffle 文件的blockManager地址，以及给每个reducer的输出文件大小，以便传递给reduce。\nblockManager.shuffleServerId(): 这是当前节点的标识符，用于标识数据存储的位置。 partitionLengths: 这是一个数组，包含了每个分区的数据长度。 更新 map 状态的目的是为了让 Spark 的调度器和后续的 reduce 任务知道每个分区的数据存储在哪里，以及每个分区的数据大小。这对于后续的 shuffle 读取操作至关重要，因为 reduce 任务需要知道从哪里读取数据。 也即是，前一个stage写数据确定分区是通过分区器，而后一个stage读数据确定分区是通过MapStatus。\nSortShuffleWriter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 override def write(records: Iterator[Product2[K, V]]): Unit = { sorter = if (dep.mapSideCombine) { new ExternalSorter[K, V, C]( context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer) } else { // In this case we pass neither an aggregator nor an ordering to the sorter, because we don\u0026#39;t // care whether the keys get sorted in each partition; that will be done on the reduce side // if the operation being run is sortByKey. new ExternalSorter[K, V, V]( context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer) } sorter.insertAll(records) // Don\u0026#39;t bother including the time to open the merged output file in the shuffle write time, // because it just opens a single file, so is typically too fast to measure accurately // (see SPARK-3570). val mapOutputWriter = shuffleExecutorComponents.createMapOutputWriter( dep.shuffleId, mapId, dep.partitioner.numPartitions) sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter) partitionLengths = mapOutputWriter.commitAllPartitions(sorter.getChecksums).getPartitionLengths mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths, mapId) } 初始化一个sorter，如果map侧（上一个stage）需要聚合，那么就创建带aggregater并按键排序的sorter，否则创建一个不带aggregater的sorter; 创建一个shuffleMapOutputWriter，写入器开启一个输出流，然后将给定reduce任务分区id的字节流持久化; Creates a writer that can open an output stream to persist bytes targeted for a given reduce partition id. The chunk corresponds to bytes in the given reduce partition. This will not be called twice for the same partition within any given map task. The partition identifier will be in the range of precisely 0 (inclusive) to numPartitions (exclusive), where numPartitions was provided upon the creation of this map output writer via ShuffleExecutorComponents.createMapOutputWriter(int, long, int).\n将记录写入sorter中，在sorter中经过处理后写出分区器分区后的数据 虽然叫sorter，但是如果map侧没有排序的需求，不会进行排序，如果map侧没有聚合的需求，也不会进行聚合。\n什么时候会排序：sortByKey,sortBy\n补充：\norderBy(SparkSQL中，RDD没有这个api):对 DataFrame 或 Dataset 进行全局排序。 类似于 SQL 中的 ORDER BY，会对整个数据集进行排序。 需要进行 shuffle 操作，以确保全局排序。\nsortWithinPartitions:数据被写入 ExternalSorter，在内存中进行排序。 如果数据量超过内存限制，ExternalSorter 会将部分数据溢出到磁盘，并在需要时进行归并排序。 最终，排序后的数据被取出并生成新的 RDD。\n什么时候会聚合：reduceByKey,combineByKey,aggregateByKey(map侧和reduce侧都进行聚合，支持不同的聚合操作),foldByKey(map侧和reduce侧聚合操作相同时等同于aggregateByKey)\n注：map侧也叫分区内，reduce侧也叫分区间\n向blockManager提交分区数据，并更新MapStatus 选择策略 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 override def getWriter[K, V]( handle: ShuffleHandle, mapId: Long, context: TaskContext, metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V] = { val mapTaskIds = taskIdMapsForShuffle.computeIfAbsent( handle.shuffleId, _ =\u0026gt; new OpenHashSet[Long](16)) mapTaskIds.synchronized { mapTaskIds.add(mapId) } val env = SparkEnv.get handle match { case unsafeShuffleHandle: SerializedShuffleHandle[K @unchecked, V @unchecked] =\u0026gt; new UnsafeShuffleWriter( env.blockManager, context.taskMemoryManager(), unsafeShuffleHandle, mapId, context, env.conf, metrics, shuffleExecutorComponents) case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =\u0026gt; new BypassMergeSortShuffleWriter( env.blockManager, bypassMergeSortHandle, mapId, env.conf, metrics, shuffleExecutorComponents) case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =\u0026gt; new SortShuffleWriter(other, mapId, context, shuffleExecutorComponents) } } 由源码可以看出，Spark的shuffle选择shuffleWriter的策略是匹配shuffleHandle，依次匹配SerializedShuffleHandle、BypassMergeSortShuffleHandle、BaseShuffleHandle。\n而shuffleHandle的类型如下：\nSerializedShuffleHandle:适用于需要高效序列化的场景。通常与 UnsafeShuffleWriter 搭配使用。 这种 handle 主要用于 Spark 的 Tungsten 引擎优化路径，利用了 Spark 的内存管理和序列化优化。 BypassMergeSortShuffleHandle:适用于小规模 shuffle 操作，特别是当分区数较少时。 通常与 BypassMergeSortShuffleWriter 搭配使用。 这种 handle 通过绕过排序步骤来提高性能，适合分区数小于 spark.shuffle.sort.bypassMergeThreshold 的情况。 BaseShuffleHandle:这是一个通用的 handle 类型，适用于大多数 shuffle 操作。 通常与 SortShuffleWriter 搭配使用。 适合需要排序的 shuffle 操作。 选择策略\n依赖类型: Spark 的 shuffle 依赖（ShuffleDependency）在 RDD 的转换操作中被创建。 依赖类型决定了 shuffle 的处理方式。例如，ShuffleDependency 中的 serializer 和 partitioner 会影响 ShuffleHandle 的选择。 配置参数: spark.shuffle.sort.bypassMergeThreshold: 这个参数决定了是否使用 BypassMergeSortShuffleHandle。如果分区数小于这个阈值，Spark 会选择 BypassMergeSortShuffleHandle。 spark.shuffle.manager: 这个参数可以配置 shuffle 的管理方式，影响 ShuffleHandle 的选择。 相关代码位于org.apache.spark.internal.config.package.scala1497行\n1 2 3 4 5 6 7 private[spark] val SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD = ConfigBuilder(\u0026#34;spark.shuffle.sort.bypassMergeThreshold\u0026#34;) .doc(\u0026#34;In the sort-based shuffle manager, avoid merge-sorting data if there is no \u0026#34; + \u0026#34;map-side aggregation and there are at most this many reduce partitions\u0026#34;) .version(\u0026#34;1.1.1\u0026#34;) .intConf .createWithDefault(200) 数据规模和分区数: 小规模数据和少量分区通常会使用 BypassMergeSortShuffleHandle。 大规模数据和大量分区通常会使用 SerializedShuffleHandle 或 BaseShuffleHandle。 ShuffleReader 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /** * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive) to * read from a range of map outputs(startMapIndex to endMapIndex-1, inclusive). * If endMapIndex=Int.MaxValue, the actual endMapIndex will be changed to the length of total map * outputs of the shuffle in `getMapSizesByExecutorId`. * * Called on executors by reduce tasks. */ override def getReader[K, C]( handle: ShuffleHandle, startMapIndex: Int, endMapIndex: Int, startPartition: Int, endPartition: Int, context: TaskContext, metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C] = { val baseShuffleHandle = handle.asInstanceOf[BaseShuffleHandle[K, _, C]] val (blocksByAddress, canEnableBatchFetch) = if (baseShuffleHandle.dependency.isShuffleMergeFinalizedMarked) { val res = SparkEnv.get.mapOutputTracker.getPushBasedShuffleMapSizesByExecutorId( handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition) (res.iter, res.enableBatchFetch) } else { val address = SparkEnv.get.mapOutputTracker.getMapSizesByExecutorId( handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition) (address, true) } new BlockStoreShuffleReader( handle.asInstanceOf[BaseShuffleHandle[K, _, C]], blocksByAddress, context, metrics, shouldBatchFetch = canEnableBatchFetch \u0026amp;\u0026amp; canUseBatchFetch(startPartition, endPartition, context)) } Spark的shuffleReader比起shuffleWriter来说就简单很多，只有一个BlockStoreShuffleReader子类。\nShuffleHandle 转换：ShuffleHandle 被转换为 BaseShuffleHandle，以便访问 shuffle 依赖的详细信息。 获取块地址和批量获取能力： 代码检查是否完成了 shuffle 合并（isShuffleMergeFinalizedMarked）。 如果合并已完成，使用推送式 shuffle 方法 getPushBasedShuffleMapSizesByExecutorId 获取块大小和批量获取能力。 否则，使用常规方法 getMapSizesByExecutorId 获取块地址。 创建 BlockStoreShuffleReader： 使用获取的块地址和批量获取能力创建 BlockStoreShuffleReader。 shouldBatchFetch 参数决定是否启用批量获取，取决于 canEnableBatchFetch 和 canUseBatchFetch 的结果。 BlockStoreShuffleReader读数据流程：\n初始化 获取数据块： 创建ShuffleBlockFetcherIterator实例，用于从其他节点获取数据块。 该迭代器负责处理数据块的网络传输、反序列化和错误处理。 支持批量获取连续的数据块（如果条件允许），以提高网络传输效率。 反序列化： 使用serializerManager.wrapStream包装从网络获取的输入流。 使用serializerInstance.deserializeStream将流反序列化为键值对迭代器。 聚合(可选)： 如果dep.aggregator被定义，使用聚合器对数据进行聚合。 如果mapSideCombine为 true，则数据已经在 map 端部分聚合，使用 combineCombinersByKey。 否则，使用 combineValuesByKey 进行聚合。 排序(可选)： 如果dep.keyOrdering被定义，使用 ExternalSorter 对数据进行排序。 ExternalSorter 可以处理大规模数据集，通过将数据溢出到磁盘来进行排序。 迭代器包装： 使用 InterruptibleIterator 包装最终的结果迭代器，以支持任务取消。 确保在任务取消时能够及时中断数据处理。 结果返回： 返回一个 Iterator[Product2[K, C]]，包含所有读取和处理后的键值对 ","date":"2024-12-09T22:01:04+08:00","permalink":"https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/","title":"Spark源码学习 Shuffle"},{"content":"学习源码所用的Spark的版本是Spark3.3.2_2.12(Scala2.12写的Spark3.3.2)\n类别 Spark底层有五种join实现方式\n前置介绍：HashJoin 参考资料：https://www.6aiq.com/article/1533984288407\n先来看看这样一条SQL语句：select * from order,item where item.id = order.i_id，很简单一个Join节点，参与join的两张表是item和order，join key分别是item.id以及order.i_id。现在假设这个Join采用的是hash join算法，整个过程会经历三步：\n确定Build Table以及Probe Table：这个概念比较重要，Build Table使用join key构建Hash Table，而Probe Table使用join key进行探测，探测成功就可以join在一起。通常情况下，小表会作为Build Table，大表作为Probe Table。此事例中item为Build Table，order为Probe Table。\n构建Hash Table：依次读取Build Table（item）的数据，对于每一行数据根据join key（item.id）进行hash，hash到对应的Bucket，生成hash table中的一条记录。数据缓存在内存中，如果内存放不下需要dump到外存。\n探测：再依次扫描Probe Table（order）的数据，使用相同的hash函数映射Hash Table中的记录，映射成功之后再检查join条件（item.id = order.i_id），如果匹配成功就可以将两者join在一起。\n基本流程可以参考上图，这里有两个小问题需要关注：\nhash join性能如何？ 很显然，hash join基本都只扫描两表一次，可以认为o(a+b)，较之最极端的笛卡尔集运算a*b，效率大大提升。\n为什么Build Table选择小表？ 道理很简单，因为构建的Hash Table最好能全部加载在内存，效率最高；这也决定了hash join算法只适合至少一个小表的join场景，对于两个大表的join场景并不适用。\nhash join是传统数据库中的单机join算法，在分布式环境下需要经过一定的分布式改造，就是尽可能利用分布式计算资源进行并行化计算，提高总体效率。hash join分布式改造一般有两种经典方案：\nbroadcast hash join： 将其中一张小表广播分发到另一张大表所在的分区节点上，分别并发地与其上的分区记录进行hash join。broadcast适用于小表很小，可以直接广播的场景。\nshuffled hash join： 一旦小表数据量较大，此时就不再适合进行广播分发。这种情况下，可以根据join key相同必然分区相同的原理，将两张表分别按照join key进行重新组织分区，这样就可以将join分而治之，划分为很多小join，充分利用集群资源并行化。\n下面分别进行详细讲解。\nBroadcastHashJoinExec broadcast阶段： 将小表广播分发到大表所在的所有主机。广播算法可以有很多，最简单的是先发给driver，driver再统一分发给所有executor；要不就是基于BitTorrent的TorrentBroadcast。\nhash join阶段： 在每个executor上执行单机版hash join，小表映射，大表试探。 SparkSQL规定broadcast hash join执行的基本条件为被广播小表必须小于参数spark.sql.autoBroadcastJoinThreshold，默认为10M。 源码位于org.apache.spark.sql.internal.SQLConf.scala,没找到SQLConf文件可以在BaseSessionStateBuilder里找到conf引用，然后CTRL+左键查看源码。 ShuffledHashJoinExec 在大数据条件下如果一张表很小，执行join操作最优的选择无疑是broadcast hash join，效率最高。但是一旦小表数据量增大，广播所需内存、带宽等资源必然就会太大，broadcast hash join就不再是最优方案。此时可以按照join key进行分区，根据key相同必然分区相同的原理，就可以将大表join分而治之，划分为很多小表的join，充分利用集群资源并行化。如下图所示，shuffle hash join也可以分为两步：\nshuffle阶段: 分别将两个表按照join key进行分区，将相同join key的记录重分布到同一节点，两张表的数据会被重分布到集群中所有节点。这个过程称为shuffle。\nhash join阶段： 每个分区节点上的数据单独执行单机hash join算法。 SortMergeJoinExec SparkSQL对两张大表join采用了全新的算法——sort merge join，整个过程分为三个步骤：\nshuffle阶段： 将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理。\nsort阶段： 对单个分区节点的两表数据，分别进行排序。\nmerge阶段： 对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边。如下图所示： CartesianProductExec: 初始化：两个数据集被加载并分区。 分区组合：每个分区的所有元素与另一个分区的所有元素组合。这在逻辑上类似于嵌套循环。 生成所有组合：对每一对元素生成一个元组，形成笛卡尔积。结果集的大小为两个数据集大小的乘积。 笛卡尔积会占用大量内存，使用笛卡尔积之前最好先过滤掉无用数据，其中一张表为极小表时建议广播。\nBroadcastNestedLoopJoinExec broadcast阶段：将小表的数据复制并广播到大表分区数据所在的每个执行节点。 嵌套循环连接： 在每个执行节点上，对于分区中的每一行，逐行扫描广播的小表。 对每一对行执行连接条件，生成匹配的结果。 生成结果：将匹配的行组合成结果集。结果在每个节点独立生成，最后输出为完整的连接结果。 另外，Spark对非等值连接的支持只有这种和笛卡尔积（CartesianProduct）。 如果两张表都很大且无法广播，Spark 可能需要通过优化或变通的方法来处理：\n数据过滤：在连接前尽量过滤数据，减少处理的数据量。 分区和缓存：有效地分区和缓存数据以提高性能。 自定义 UDF：在某些情况下，使用自定义 UDF 可能会帮助实现复杂的连接逻辑。 选择join的机制 相关源码位于org.apache.spark.sql.execution下的141行JoinSelection对象\n根据连接策略提示、等价连接键的可用性和连接关系的大小，选择合适的连接物理计划。 以下是现有的连接策略、其特点和局限性。\nbroadcast hash join（BHJ）： 仅支持等价连接，而连接键无需可排序。 支持除全外部连接外的所有连接类型。 当广播方较小时，BHJ 通常比其他连接算法执行得更快。 不过，广播表是一种网络密集型操作，在某些情况下可能会导致 OOM 或性能不佳，尤其是当构建/广播方较大时。 shuffled hash join： 仅支持等价连接，而连接键无需可排序。 支持所有连接类型。 从表中构建哈希映射是一个内存密集型操作，当联编侧很大时可能会导致 OOM。 shuffle sort merge join（SMJ）： 仅支持等连接，且连接键必须是可排序的。 支持所有连接类型。 Broadcast nested loop join(广播嵌套循环连接 BNLJ)： 支持等连接和非等连接。 支持所有连接类型，但优化了以下方面的实现： 1）在右外连接中广播左侧；2）在左外、左半、左反或存在连接中广播右侧；3）在类内连接中广播任一侧。 对于其他情况，我们需要多次扫描数据，这可能会相当慢。 Shuffle-and-replicate nested loop join (洗牌复制嵌套循环连接,又称笛卡尔积连接)： 支持等连接和非等连接。 只支持内同类连接。 源码中关于等值连接选择join的机制介绍如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 If it is an equi-join, we first look at the join hints w.r.t. the following order: 1. broadcast hint: pick broadcast hash join if the join type is supported. If both sides have the broadcast hints, choose the smaller side (based on stats) to broadcast. 2. sort merge hint: pick sort merge join if join keys are sortable. 3. shuffle hash hint: We pick shuffle hash join if the join type is supported. If both sides have the shuffle hash hints, choose the smaller side (based on stats) as the build side. 4. shuffle replicate NL hint: pick cartesian product if join type is inner like. If there is no hint or the hints are not applicable, we follow these rules one by one: 1. Pick broadcast hash join if one side is small enough to broadcast, and the join type is supported. If both sides are small, choose the smaller side (based on stats) to broadcast. 2. Pick shuffle hash join if one side is small enough to build local hash map, and is much smaller than the other side, and `spark.sql.join.preferSortMergeJoin` is false. 3. Pick sort merge join if the join keys are sortable. 4. Pick cartesian product if join type is inner like. 5. Pick broadcast nested loop join as the final solution. It may OOM but we don\u0026#39;t have other choice. 翻译如下：\n如果是等值连接，我们首先按以下顺序查看连接提示(join hints)：\nbroadcast hint：如果支持广播散列连接(BHJ)，则选择广播散列连接。 如果双方有广播提示，则选择较小的一方（根据统计信息）进行广播。 sort merge hint：如果连接键可排序，则选择排序合并连接。 shuffle hash hint：如果支持连接类型，我们会选择洗牌散列连接。 如果双方都有洗牌散列提示，则选择较小的一方（基于统计信息）作为构建方。 构建侧。 shuffle replicate NL 提示：如果连接类型是内连接，则选择笛卡尔积。 如果没有提示或提示不适用，我们将逐一遵循这些规则：\n如果有一方足够小，可以广播，且连接类型支持，则选择广播散列连接。 支持。 如果两边都很小，则选择较小的一边（根据统计数据） 进行广播。 如果一方的规模小到足以建立本地哈希映射，并且比另一方小很多，并且支持 \u0026ldquo;space\u0026rdquo;，则选择 \u0026ldquo;洗牌哈希连接\u0026rdquo;。 且 spark.sql.join.preferSortMergeJoin 为 false。 如果连接键可排序，则选择排序合并连接。 如果连接类型为inner join（就是不明写join的join，即类似select * from order,item），则选择笛卡尔积。 选择广播嵌套循环连接作为最终解决方案。 可能会出现 OOM，但我们没有 其他选择。 相关关键源码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def createJoinWithoutHint() = { createBroadcastHashJoin(false) .orElse(createShuffleHashJoin(false)) .orElse(createSortMergeJoin()) .orElse(createCartesianProduct()) .getOrElse { // This join could be very slow or OOM val buildSide = getSmallerSide(left, right) Seq(joins.BroadcastNestedLoopJoinExec( planLater(left), planLater(right), buildSide, joinType, j.condition)) } } if (hint.isEmpty) { createJoinWithoutHint() } else { createBroadcastHashJoin(true) .orElse { if (hintToSortMergeJoin(hint)) createSortMergeJoin() else None } .orElse(createShuffleHashJoin(true)) .orElse { if (hintToShuffleReplicateNL(hint)) createCartesianProduct() else None } .getOrElse(createJoinWithoutHint()) } 综上所述：\n有连接提示时,优先级从高到低：BroadcastHashJoin \u0026gt; SortMergeJoin \u0026gt; ShuffledHashJoin \u0026gt; CartesianProduct 没有连接提示时，优先级从高到低(join代价从小到大)：BroadcastHashJoin \u0026gt; ShuffledHashJoin \u0026gt; SortMergeJoin \u0026gt; CartesianProduct \u0026gt; BroadcastNestedLoopJoin 注意：有提示时优先SortMergeJoin,然后ShuffledHashJoin；而没有提示时，优先ShuffledHashJoin,然后SortMergeJoin。\n总结： 数据仓库设计时最好避免大表与大表的join查询，SparkSQL也可以根据内存资源、带宽资源适量将参数spark.sql.autoBroadcastJoinThreshold调大，让更多join实际执行为broadcast hash join。\n大表join极小表(表大小小于10M,可调整spark.sql.autoBroadcastJoinThreshold参数进行修改)，用BroadcastHashJoin 大表join小表，用ShuffledHashJoin 大表join大表，用SortMergeJoin 非等值连接仅CartesianProduct 和 BroadcastNestedLoopJoin支持，常用BroadcastNestedLoopJoin。\n补充：RDD中的join 前文中介绍的join都是SparkSQL中join的底层实现，但是在Spark的RDD中，也有一个join函数。 具体实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD. */ def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope { this.cogroup(other, partitioner).flatMapValues( pair =\u0026gt; for (v \u0026lt;- pair._1.iterator; w \u0026lt;- pair._2.iterator) yield (v, w) ) } /** * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the * pair (k, (v, None)) if no elements in `other` have key k. Uses the given Partitioner to * partition the output RDD. */ def leftOuterJoin[W]( other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))] = self.withScope { this.cogroup(other, partitioner).flatMapValues { pair =\u0026gt; if (pair._2.isEmpty) { pair._1.iterator.map(v =\u0026gt; (v, None)) } else { for (v \u0026lt;- pair._1.iterator; w \u0026lt;- pair._2.iterator) yield (v, Some(w)) } } } /** * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the * pair (k, (None, w)) if no elements in `this` have key k. Uses the given Partitioner to * partition the output RDD. */ def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner) : RDD[(K, (Option[V], W))] = self.withScope { this.cogroup(other, partitioner).flatMapValues { pair =\u0026gt; if (pair._1.isEmpty) { pair._2.iterator.map(w =\u0026gt; (None, w)) } else { for (v \u0026lt;- pair._1.iterator; w \u0026lt;- pair._2.iterator) yield (Some(v), w) } } } /** * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each * element (k, w) in `other`, the resulting RDD will either contain all pairs * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements * in `this` have key k. Uses the given Partitioner to partition the output RDD. */ def fullOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner) : RDD[(K, (Option[V], Option[W]))] = self.withScope { this.cogroup(other, partitioner).flatMapValues { case (vs, Seq()) =\u0026gt; vs.iterator.map(v =\u0026gt; (Some(v), None)) case (Seq(), ws) =\u0026gt; ws.iterator.map(w =\u0026gt; (None, Some(w))) case (vs, ws) =\u0026gt; for (v \u0026lt;- vs.iterator; w \u0026lt;- ws.iterator) yield (Some(v), Some(w)) } } RDD的join实际是匹配两个RDD,以join为例，遍历(iterator)两个RDD的分区，调用cogroup函数将两个RDD的相同分区联合成一个turple返回。\n","date":"2024-11-17T23:55:36+08:00","permalink":"https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/","title":"Spark源码学习 Join策略"},{"content":"简介 累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在 Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后， 传回 Driver 端进行 merge。\n快速上手 数据如下，数据格式为学生姓名，学生课程，课程成绩。要求计算选择了Database课程的人数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package SprakReview import org.apache.spark.util.LongAccumulator import org.apache.spark.{SparkConf, SparkContext} object AccumulatorTry { def main(args: Array[String]): Unit = { val sparkConf = new SparkConf().setMaster(\u0026#34;local[*]\u0026#34;).setAppName(\u0026#34;累加器学习\u0026#34;) val sc = new SparkContext(sparkConf) val data = sc.textFile(\u0026#34;E:\\\\TestData\\\\sparkjob3\\\\task1\\\\Data01.txt\u0026#34;) val dbCount: LongAccumulator = sc.longAccumulator(\u0026#34;dbCount\u0026#34;) data.foreach { line =\u0026gt; val course = line.split(\u0026#34;,\u0026#34;)(1) if (course == \u0026#34;DataBase\u0026#34;) { // 每当课程为\u0026#34;DataBase\u0026#34;时，累加器的值加1 dbCount.add(1) } } println(\u0026#34;DataBase count: \u0026#34; + dbCount.value) //累加器实现查找选择了DataBase课的学生 sc.stop() } } 原理简介 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`. */ def longAccumulator: LongAccumulator = { val acc = new LongAccumulator register(acc) acc } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`. */ def longAccumulator(name: String): LongAccumulator = { val acc = new LongAccumulator register(acc, name) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`. */ def doubleAccumulator: DoubleAccumulator = { val acc = new DoubleAccumulator register(acc) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`. */ def doubleAccumulator(name: String): DoubleAccumulator = { val acc = new DoubleAccumulator register(acc, name) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list. */ def collectionAccumulator[T]: CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list. */ def collectionAccumulator[T](name: String): CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc, name) acc } Spark中的累加器有三种doubleAccumulator、longAccumulator、collectionAccumulator。分别有传入累加器名字和不穿名字的函数。 累加器会新建一个对应的累加器类，然后在driver注册，传不传名字的区别是注册的时候会不会传入名字，最后返回已注册的累加器。\n源码中对注册的解释如下：\nRegisters an AccumulatorV2 created on the driver such that it can be used on the executors. All accumulators registered here can later be used as a container for accumulating partial values across multiple tasks. This is what org.apache.spark.scheduler.DAGScheduler does. Note: if an accumulator is registered here, it should also be registered with the active context cleaner for cleanup so as to avoid memory leaks. If an AccumulatorV2 with the same ID was already registered, this does nothing instead of overwriting it. We will never register same accumulator twice, this is just a sanity check.\n注册在driver上创建的 AccumulatorV2，以便在executor上使用。 在此注册的所有累加器以后都可用作容器，用于累加多个任务中的部分值。 这就是 org.apache.spark.scheduler.DAGScheduler 的作用。 注意：如果在这里注册了累加器，那么它也应注册到活动上下文清理器中进行清理，以避免内存泄漏。 如果已经注册了具有相同 ID 的 AccumulatorV2，则不会做任何操作，而会覆盖它。 我们永远不会注册同一个累加器两次，这只是为了进行合理性检查。\n累加器类 LongAccumulator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 /** * An [[AccumulatorV2 accumulator]] for computing sum, count, and average of 64-bit integers. * * @since 2.0.0 */ class LongAccumulator extends AccumulatorV2[jl.Long, jl.Long] { private var _sum = 0L private var _count = 0L /** * Returns false if this accumulator has had any values added to it or the sum is non-zero. * * @since 2.0.0 */ override def isZero: Boolean = _sum == 0L \u0026amp;\u0026amp; _count == 0 override def copy(): LongAccumulator = { val newAcc = new LongAccumulator newAcc._count = this._count newAcc._sum = this._sum newAcc } override def reset(): Unit = { _sum = 0L _count = 0L } /** * Adds v to the accumulator, i.e. increment sum by v and count by 1. * @since 2.0.0 */ override def add(v: jl.Long): Unit = { _sum += v _count += 1 } /** * Adds v to the accumulator, i.e. increment sum by v and count by 1. * @since 2.0.0 */ def add(v: Long): Unit = { _sum += v _count += 1 } /** * Returns the number of elements added to the accumulator. * @since 2.0.0 */ def count: Long = _count /** * Returns the sum of elements added to the accumulator. * @since 2.0.0 */ def sum: Long = _sum /** * Returns the average of elements added to the accumulator. * @since 2.0.0 */ def avg: Double = _sum.toDouble / _count override def merge(other: AccumulatorV2[jl.Long, jl.Long]): Unit = other match { case o: LongAccumulator =\u0026gt; _sum += o.sum _count += o.count case _ =\u0026gt; throw new UnsupportedOperationException( s\u0026#34;Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\u0026#34;) } private[spark] def setValue(newValue: Long): Unit = _sum = newValue override def value: jl.Long = _sum } LongAccumulator类是一个自定义累加器，用于计算64位整数的总和、计数和平均值。它继承自AccumulatorV2。 私有变量_sum:(存储累加的总和)、_count(存储累加的元素个数)。 用于计算整型数据的总和、计数和平均值，并在driver中访问结果。\nisZero方法:检查累加器是否未添加任何值或总和为零。\ncopy方法:创建累加器的副本，包括当前的_sum和_count。\nreset方法:将_sum和_count重置为零。\nadd方法:增加一个值到累加器中，更新_sum和_count。有两个重载版本，接收jl.Long和Long类型。 (jl.Long是java.lang.Long的缩写。是Java的类类型，用于包装一个long的值。 提供了许多方法来处理long类型的数据，比如转换、比较等。用于与Java类进行交互时的包装器类型。 Long是Scala的基本数据类型。直接表示一个64位的整数。)\ncount方法:返回添加到累加器中的元素个数。\nsum方法:返回累加器中的元素总和。\navg方法:返回累加器中元素的平均值。\nmerge方法:将另一个LongAccumulator的值合并到当前累加器。如果尝试与非LongAccumulator类型合并，会抛出异常。\nsetValue方法:设置累加器的总和值（仅用于内部）。\nvalue方法:返回累加器的当前总和。\nDoubleAccumulator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 /** * An [[AccumulatorV2 accumulator]] for computing sum, count, and averages for double precision * floating numbers. * * @since 2.0.0 */ class DoubleAccumulator extends AccumulatorV2[jl.Double, jl.Double] { private var _sum = 0.0 private var _count = 0L /** * Returns false if this accumulator has had any values added to it or the sum is non-zero. */ override def isZero: Boolean = _sum == 0.0 \u0026amp;\u0026amp; _count == 0 override def copy(): DoubleAccumulator = { val newAcc = new DoubleAccumulator newAcc._count = this._count newAcc._sum = this._sum newAcc } override def reset(): Unit = { _sum = 0.0 _count = 0L } /** * Adds v to the accumulator, i.e. increment sum by v and count by 1. * @since 2.0.0 */ override def add(v: jl.Double): Unit = { _sum += v _count += 1 } /** * Adds v to the accumulator, i.e. increment sum by v and count by 1. * @since 2.0.0 */ def add(v: Double): Unit = { _sum += v _count += 1 } /** * Returns the number of elements added to the accumulator. * @since 2.0.0 */ def count: Long = _count /** * Returns the sum of elements added to the accumulator. * @since 2.0.0 */ def sum: Double = _sum /** * Returns the average of elements added to the accumulator. * @since 2.0.0 */ def avg: Double = _sum / _count override def merge(other: AccumulatorV2[jl.Double, jl.Double]): Unit = other match { case o: DoubleAccumulator =\u0026gt; _sum += o.sum _count += o.count case _ =\u0026gt; throw new UnsupportedOperationException( s\u0026#34;Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\u0026#34;) } private[spark] def setValue(newValue: Double): Unit = _sum = newValue override def value: jl.Double = _sum } DoubleAccumulator与LongAccumulator区别不大，主要区别在DoubleAccumulator的_sum为Double类型。\nCollectionAccumulator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 /** * An [[AccumulatorV2 accumulator]] for collecting a list of elements. * * @since 2.0.0 */ class CollectionAccumulator[T] extends AccumulatorV2[T, java.util.List[T]] { private var _list: java.util.List[T] = _ private def getOrCreate = { _list = Option(_list).getOrElse(new java.util.ArrayList[T]()) _list } /** * Returns false if this accumulator instance has any values in it. */ override def isZero: Boolean = this.synchronized(getOrCreate.isEmpty) override def copyAndReset(): CollectionAccumulator[T] = new CollectionAccumulator override def copy(): CollectionAccumulator[T] = { val newAcc = new CollectionAccumulator[T] this.synchronized { newAcc.getOrCreate.addAll(getOrCreate) } newAcc } override def reset(): Unit = this.synchronized { _list = null } override def add(v: T): Unit = this.synchronized(getOrCreate.add(v)) override def merge(other: AccumulatorV2[T, java.util.List[T]]): Unit = other match { case o: CollectionAccumulator[T] =\u0026gt; this.synchronized(getOrCreate.addAll(o.value)) case _ =\u0026gt; throw new UnsupportedOperationException( s\u0026#34;Cannot merge ${this.getClass.getName} with ${other.getClass.getName}\u0026#34;) } override def value: java.util.List[T] = this.synchronized { java.util.Collections.unmodifiableList(new ArrayList[T](getOrCreate)) } private[spark] def setValue(newValue: java.util.List[T]): Unit = this.synchronized { _list = null getOrCreate.addAll(newValue) } } CollectionAccumulator类是一个用于收集元素列表的自定义累加器。它继承自AccumulatorV2(在Spark中创建累加器的基类,自定义累加器也需要继承这个类)。具有私有变量 _list，用于存储累积的元素列表，初始为null。使用synchronized确保了对_list的操作是线程安全的。主要用途是在任务中收集元素并在driver上提供收集到的数据。 _list类型是java.util.List，所以java.util.List的方法都可以在这里正常使用（setValue就是先情况_list，再调用List的addAll添加一个java.util.List）。\ngetOrCreate方法:确保_list已初始化，如果为null，则创建一个新的ArrayList(java.util.ArrayList)。\nisZero方法:检查累加器是否为空，如果为空则返回true。\ncopyAndReset方法:创建一个没有累积值的新CollectionAccumulator。\ncopy方法:创建一个新的CollectionAccumulator并复制当前元素。\nreset方法:通过将_list设置为null来清空累加器。\nadd方法:向累加器中添加一个元素。\nmerge方法:将另一个累加器的值合并到这个累加器中，仅支持与另一个CollectionAccumulator合并。\nvalue方法:返回累积列表的不可修改视图。\nsetValue方法:重置累加器并设置为新的元素列表。\n","date":"2024-11-09T20:39:50+08:00","permalink":"https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/","title":"Spark源码学习 累加器"},{"content":"简介 广播变量允许程序员在每台机器上缓存只读变量，而不是随任务一起发送副本。 例如，它们可以用来以高效的方式为每个节点提供一个大型输入数据集的副本。 Spark 还尝试使用高效的广播算法分发广播变量，以降低通信成本。 广播变量是通过调用 broadcast 从变量 v 中创建的。 广播变量是 v 的包装器，其值可通过调用 value 方法访问。\n快速上手 广播变量使用如简介中所说，使用sc.broadcast()包装一个变量，就创建了一个广播变量。访问广播变量的值可以通过调用其value方法，即broadV.value()\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import org.apache.spark.broadcast.Broadcast import org.apache.spark.{SparkConf, SparkContext} object BroadCastTry { def main(args: Array[String]): Unit = { val sparkConf: SparkConf = new SparkConf().setMaster(\u0026#34;local[*]\u0026#34;).setAppName(\u0026#34;广播变量练习\u0026#34;) val sc = new SparkContext(sparkConf) val v = Array(1,2,3,4,5,6) // 创建广播变量 val broadV:Broadcast[Array[Int]] = sc.broadcast(v) //打印广播变量 println(broadV.value.mkString(\u0026#34;Array(\u0026#34;, \u0026#34;, \u0026#34;, \u0026#34;)\u0026#34;)) //销毁广播变量 broadV.destroy() sc.stop() } } 原理简介 参考资料：Spark Core源码精读计划11 | Spark广播机制的实现\n注：本次源码阅读使用的是Spark_2.12-3.3.2(scala2.12版本写的Spark3.3.2)\n简单地说，广播变量的流程如下\n广播变量由Application的Driver使用BroadcastManager创建，并存储在BlockManager中 Driver将广播变量的值写到Block中，这样在driver上执行的tasks不会再创建一份新的广播变量副本 Executor需要使用这个变量时，先从本地BlockManager中查找有无该变量，没有的话从Driver的BlockManager远程读取该变量 Executor获取到这个广播变量后就将它缓存到本地的BlockManager中，避免重复地远程获取，提高性能。 使用广播变量能提高性能的原因是：\n减少数据传输：广播变量将数据从Driver节点传输到每个Executor，只进行一次网络传输，而不是在每个任务中重复传输。 本地缓存：一旦Executor接收到广播变量，它会将其缓存本地，供后续任务使用，避免重复的远程获取。 内存效率：通过共享相同的数据副本，广播变量减少了Executor内存中的数据冗余。 这些特性一起显著减少了网络I/O和内存开销，提升了分布式计算的性能和效率。但是广播变量只能用在只读变量上，而且只适合用在比较大的变量上。\n因为对于比较小的变量，直接传递给每个任务的开销很低，而且广播机制增大了任务复杂性。不能用于可变变量的原因也很明显，广播变量是由Driver创建，并由Executor远程读取。Driver或者Executor修改后都会导致计算结果错误。\n补充：\nBroadcastManager是位于org.apache.spark.broadcast下的类，用于创建和管理广播变量，BlockManager是位于org.apache.spark.storage下的类，用于在每个节点（Driver和Executor）上运行的管理器，为本地和远程向各种存储（内存、磁盘和堆外）放入和检索数据块提供接口。\nBroadcast是位于org.apache.spark.broadcast下的抽象类，只有TorrentBroadcast一个子类，早期还有一个HttpBroadcast子类。\nTorrentBroadcast是类似于 BitTorrent 的 Broadcast 实现。有同名类和对象。其机制如下： Driver将序列化对象分成小块，并将这些小块存储在Executor的 BlockManager 中。 在每个Executor上，Executor首先尝试从其 BlockManager 中获取对象。 如果对象不存在，Executor就会使用远程获取功能，从Driver和/或其他Executor（如果有的话）中获取小块对象。 获取小块后，它会将小块放入自己的 BlockManager 中，供其他执行器取用。 这样，驱动程序就不会成为发送多份广播数据（每个执行器一份）的瓶颈。 初始化时，TorrentBroadcast 对象会读取 SparkEnv.get.conf 文件。\n广播管理器BroadcastManager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 package org.apache.spark.broadcast import java.util.Collections import java.util.concurrent.atomic.AtomicLong import scala.reflect.ClassTag import org.apache.commons.collections4.map.AbstractReferenceMap.ReferenceStrength import org.apache.commons.collections4.map.ReferenceMap import org.apache.spark.SparkConf import org.apache.spark.api.python.PythonBroadcast import org.apache.spark.internal.Logging private[spark] class BroadcastManager( val isDriver: Boolean, conf: SparkConf) extends Logging { private var initialized = false private var broadcastFactory: BroadcastFactory = null initialize() // Called by SparkContext or Executor before using Broadcast private def initialize(): Unit = { synchronized { if (!initialized) { broadcastFactory = new TorrentBroadcastFactory broadcastFactory.initialize(isDriver, conf) initialized = true } } } def stop(): Unit = { broadcastFactory.stop() } private val nextBroadcastId = new AtomicLong(0) private[broadcast] val cachedValues = Collections.synchronizedMap( new ReferenceMap(ReferenceStrength.HARD, ReferenceStrength.WEAK) .asInstanceOf[java.util.Map[Any, Any]] ) def newBroadcast[T: ClassTag](value_ : T, isLocal: Boolean): Broadcast[T] = { val bid = nextBroadcastId.getAndIncrement() value_ match { case pb: PythonBroadcast =\u0026gt; // SPARK-28486: attach this new broadcast variable\u0026#39;s id to the PythonBroadcast, // so that underlying data file of PythonBroadcast could be mapped to the // BroadcastBlockId according to this id. Please see the specific usage of the // id in PythonBroadcast.readObject(). pb.setBroadcastId(bid) case _ =\u0026gt; // do nothing } broadcastFactory.newBroadcast[T](value_, isLocal, bid) } def unbroadcast(id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit = { broadcastFactory.unbroadcast(id, removeFromDriver, blocking) } } BroadcastManager创建时传入两个参数isDriver(是否是Driver)、conf(Spark配置文件)。早期版本还有第三个变量securityManager（对应的SecurityManager）,这里我只是看参考资料中的源码知道的，第三个变量具体的作用不做了解。\n成员变量 BroadcastManager内有四个成员变量：\ninitialized表示BroadcastManager是否已经初始化完成。 broadcastFactory持有广播工厂的实例（类型是BroadcastFactory特征的实现类）。 nextBroadcastId表示下一个广播变量的唯一标识（AtomicLong类型的）。 cachedValues用来缓存已广播出去的变量。它属于ReferenceMap类型，是apache-commons提供的一个弱引用映射数据结构。与我们常见的各种Map不同，它的键值对有可能会在GC过程中被回收。 初始化逻辑 initialize()方法做的事情也非常简单，它首先判断BroadcastManager是否已初始化。如果未初始化，就新建广播工厂TorrentBroadcastFactory，将其初始化，然后将初始化标记设为true。\n1 2 3 4 5 6 7 8 9 private def initialize(): Unit = { synchronized { if (!initialized) { broadcastFactory = new TorrentBroadcastFactory broadcastFactory.initialize(isDriver, conf) initialized = true } } } 对外提供的方法 BroadcastManager提供的方法有两个：newBroadcast()方法，用于创建一个新的广播变量；以及unbroadcast()方法，将已存在的广播变量取消广播。它们都是直接调用了TorrentBroadcastFactory中的同名方法。因此我们必须通过阅读TorrentBroadcastFactory的相关源码，才能了解Spark广播机制的细节。\n广播工厂类TorrentBroadcastFactory 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package org.apache.spark.broadcast import scala.reflect.ClassTag import org.apache.spark.SparkConf /** * A [[org.apache.spark.broadcast.Broadcast]] implementation that uses a BitTorrent-like * protocol to do a distributed transfer of the broadcasted data to the executors. Refer to * [[org.apache.spark.broadcast.TorrentBroadcast]] for more details. */ private[spark] class TorrentBroadcastFactory extends BroadcastFactory { override def initialize(isDriver: Boolean, conf: SparkConf): Unit = { } override def newBroadcast[T: ClassTag](value_ : T, isLocal: Boolean, id: Long): Broadcast[T] = { new TorrentBroadcast[T](value_, id) } override def stop(): Unit = { } /** * Remove all persisted state associated with the torrent broadcast with the given ID. * @param removeFromDriver Whether to remove state from the driver. * @param blocking Whether to block until unbroadcasted */ override def unbroadcast(id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit = { TorrentBroadcast.unpersist(id, removeFromDriver, blocking) } } 由源码可知，TorrentBroadcastFactory的newBroadcast()方法实际是新建了一个TorrentBroadcast类，并传入了这个类的id和值。 TorrentBroadcast类的详情参见下节。\nTorrentBroadcastFactory的unbroadcast()方法传入了TorrentBroadcast类的id、 removeFromDriver（是否从驱动程序中移除状态）、blocking （是否直到未广播仍然在堵塞）。 然后删除与给定 ID 的 torrent 广播相关的所有持久化状态。这个删除持久化状态实际是SparkEnv.get.blockManager.master.removeBroadcast(id, removeFromDriver, blocking)。相关代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 def removeBroadcast(broadcastId: Long, removeFromMaster: Boolean, blocking: Boolean): Unit = { val future = driverEndpoint.askSync[Future[Seq[Int]]]( RemoveBroadcast(broadcastId, removeFromMaster)) future.failed.foreach(e =\u0026gt; logWarning(s\u0026#34;Failed to remove broadcast $broadcastId\u0026#34; + s\u0026#34; with removeFromMaster = $removeFromMaster - ${e.getMessage}\u0026#34;, e) )(ThreadUtils.sameThread) if (blocking) { // the underlying Futures will timeout anyway, so it\u0026#39;s safe to use infinite timeout here RpcUtils.INFINITE_TIMEOUT.awaitResult(future) } } TorrentBroadcast类 成员变量 _value：广播块的具体数据。注意它由lazy关键字定义，因此是懒加载的，也就是在TorrentBroadcast构造时不会调用readBroadcastBlock()方法获取数据，而会推迟到第一次访问_value时。 compressionCodec：广播块的压缩编解码逻辑。当配置项spark.broadcast.compress为true时，会启用压缩。 blockSize：广播块的大小。由spark.broadcast.blockSize配置项来控制，默认值4MB。 broadcastId：广播变量的ID。BroadcastBlockId是个结构非常简单的case class，每产生一个新的广播变量就会自增。 numBlocks：该广播变量包含的块数量。它与_value不同，并没有lazy关键字定义，因此在TorrentBroadcast构造时就会直接调用writeBlocks()方法。 checksumEnabled：是否允许对广播块计算校验值，由spark.broadcast.checksum配置项控制，默认值true。 checksums：广播块的校验值。 writeBlocks() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 private def writeBlocks(value: T): Int = { import StorageLevel._ // Store a copy of the broadcast variable in the driver so that tasks run on the driver // do not create a duplicate copy of the broadcast variable\u0026#39;s value. val blockManager = SparkEnv.get.blockManager if (!blockManager.putSingle(broadcastId, value, MEMORY_AND_DISK, tellMaster = false)) { throw new SparkException(s\u0026#34;Failed to store $broadcastId in BlockManager\u0026#34;) } try { val blocks = TorrentBroadcast.blockifyObject(value, blockSize, SparkEnv.get.serializer, compressionCodec) if (checksumEnabled) { checksums = new Array[Int](blocks.length) } blocks.zipWithIndex.foreach { case (block, i) =\u0026gt; if (checksumEnabled) { checksums(i) = calcChecksum(block) } val pieceId = BroadcastBlockId(id, \u0026#34;piece\u0026#34; + i) val bytes = new ChunkedByteBuffer(block.duplicate()) if (!blockManager.putBytes(pieceId, bytes, MEMORY_AND_DISK_SER, tellMaster = true)) { throw new SparkException(s\u0026#34;Failed to store $pieceId of $broadcastId \u0026#34; + s\u0026#34;in local BlockManager\u0026#34;) } } blocks.length } catch { case t: Throwable =\u0026gt; logError(s\u0026#34;Store broadcast $broadcastId fail, remove all pieces of the broadcast\u0026#34;) blockManager.removeBroadcast(id, tellMaster = true) throw t } } 获取BlockManager实例，调用其putSingle()方法将广播数据作为单个对象写入本地存储。注意StorageLevel为MEMORY_AND_DISK，亦即在内存不足时会溢写到磁盘，且副本数为1，不会进行复制。 调用blockifyObject()方法将广播数据转化为块(block)，即Spark存储的基本单元。 使用的序列化器为SparkEnv中指定的序列化器（默认Java自带的序列化，另外Spark实现了kryo序列化，可以在SparkEnv中指定）。 如果校验值开关有效，就用calcChecksum()方法为每个块计算校验值。 为广播数据切分成的每个块（称为piece）都生成一个带\u0026quot;piece\u0026quot;的广播ID，调用BlockManager.putBytes()方法将各个块写入MemoryStore（内存）或DiskStore（磁盘）。StorageLevel为MEMORY_AND_DISK_SER，写入的数据会序列化。 最终返回块的计数值。 readBroadcastBlock() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 private def readBroadcastBlock(): T = Utils.tryOrIOException { TorrentBroadcast.torrentBroadcastLock.withLock(broadcastId) { // As we only lock based on `broadcastId`, whenever using `broadcastCache`, we should only // touch `broadcastId`. val broadcastCache = SparkEnv.get.broadcastManager.cachedValues Option(broadcastCache.get(broadcastId)).map(_.asInstanceOf[T]).getOrElse { setConf(SparkEnv.get.conf) val blockManager = SparkEnv.get.blockManager blockManager.getLocalValues(broadcastId) match { case Some(blockResult) =\u0026gt; if (blockResult.data.hasNext) { val x = blockResult.data.next().asInstanceOf[T] releaseBlockManagerLock(broadcastId) if (x != null) { broadcastCache.put(broadcastId, x) } x } else { throw new SparkException(s\u0026#34;Failed to get locally stored broadcast data: $broadcastId\u0026#34;) } case None =\u0026gt; val estimatedTotalSize = Utils.bytesToString(numBlocks * blockSize) logInfo(s\u0026#34;Started reading broadcast variable $id with $numBlocks pieces \u0026#34; + s\u0026#34;(estimated total size $estimatedTotalSize)\u0026#34;) val startTimeNs = System.nanoTime() val blocks = readBlocks() logInfo(s\u0026#34;Reading broadcast variable $id took ${Utils.getUsedTimeNs(startTimeNs)}\u0026#34;) try { val obj = TorrentBroadcast.unBlockifyObject[T]( blocks.map(_.toInputStream()), SparkEnv.get.serializer, compressionCodec) // Store the merged copy in BlockManager so other tasks on this executor don\u0026#39;t // need to re-fetch it. val storageLevel = StorageLevel.MEMORY_AND_DISK if (!blockManager.putSingle(broadcastId, obj, storageLevel, tellMaster = false)) { throw new SparkException(s\u0026#34;Failed to store $broadcastId in BlockManager\u0026#34;) } if (obj != null) { broadcastCache.put(broadcastId, obj) } obj } finally { blocks.foreach(_.dispose()) } } } } } 使用广播id对读取操作加锁，保证线程安全 调用broadcastManager.cacheedValues,根据广播id检查广播变量是否已在本地缓存中，如果存在，直接返回 调用setConf()传入配置信息 尝试从本地blockManager调用getLocalValues获取指定广播id的数据，如果有就直接读取并缓存 如果本地blockManager不存在，就调用readBlocks()方法，从driver和其他executor读取指定广播id对应的piece（片）数据 从其他节点获取到分块数据后，将其反序列化，重建对象并存储在BlockManager中,并将重建的对象缓存 补充：readBlocks() readBlocks()是在本地缓存和BlockManager都读取不到数据后，从其他节点读取数据的方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 private def readBlocks(): Array[BlockData] = { // Fetch chunks of data. Note that all these chunks are stored in the BlockManager and reported // to the driver, so other executors can pull these chunks from this executor as well. val blocks = new Array[BlockData](numBlocks) val bm = SparkEnv.get.blockManager for (pid \u0026lt;- Random.shuffle(Seq.range(0, numBlocks))) { val pieceId = BroadcastBlockId(id, \u0026#34;piece\u0026#34; + pid) logDebug(s\u0026#34;Reading piece $pieceId of $broadcastId\u0026#34;) // First try getLocalBytes because there is a chance that previous attempts to fetch the // broadcast blocks have already fetched some of the blocks. In that case, some blocks // would be available locally (on this executor). bm.getLocalBytes(pieceId) match { case Some(block) =\u0026gt; blocks(pid) = block releaseBlockManagerLock(pieceId) case None =\u0026gt; bm.getRemoteBytes(pieceId) match { case Some(b) =\u0026gt; if (checksumEnabled) { val sum = calcChecksum(b.chunks(0)) if (sum != checksums(pid)) { throw new SparkException(s\u0026#34;corrupt remote block $pieceId of $broadcastId:\u0026#34; + s\u0026#34; $sum != ${checksums(pid)}\u0026#34;) } } // We found the block from remote executors/driver\u0026#39;s BlockManager, so put the block // in this executor\u0026#39;s BlockManager. if (!bm.putBytes(pieceId, b, StorageLevel.MEMORY_AND_DISK_SER, tellMaster = true)) { throw new SparkException( s\u0026#34;Failed to store $pieceId of $broadcastId in local BlockManager\u0026#34;) } blocks(pid) = new ByteBufferBlockData(b, true) case None =\u0026gt; throw new SparkException(s\u0026#34;Failed to get $pieceId of $broadcastId\u0026#34;) } } } blocks } 初始化一个BlockData数组，长度为广播id对应piece数，获取BlockManager实例 通过Random.shuffle()随机顺序遍历每个块的id，确保负载均衡 如果本地BlockManager存在块id对应数据块，直接获取并存储 这里好像有点冗余，因为readBlocks()本来就是在本地缓存和BlockManager中找不到，才远程访问其他节点时调用的方法。但代码中这么写的原因已在注释中给出:\nFirst try getLocalBytes because there is a chance that previous attempts to fetch the broadcast blocks have already fetched some of the blocks. In that case, some blocks would be available locally (on this executor).\n首先尝试 getLocalBytes，因为之前获取广播数据块的尝试有可能已经获取了部分数据块。在这种情况下，一些区块将在本地（在此executor上）可用。\n如果本地没有，调用blockManager.getRemoteBytes(pieceId)从远程节点获取。若开启了校验和，则调用calChecksum()计算校验和并比较，来验证数据完整性 将从远端获取的数据块存储到本地BlcokManager中。 获取远端数据封装了很多层，大体读取顺序如下图 获取远端数据的最后一步，从其他节点读取数据属于BlockManager的部分，下次有机会再读一读\n总结 广播变量的底层机制总结如下图： ","date":"2024-11-07T14:51:19+08:00","permalink":"https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/","title":"Spark源码学习 广播变量"},{"content":"Saprk定义 Apache Spark 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习。\n## Spark组件 Spark三大抽象数据结构 RDD:弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行 计算的集合。\n弹性 存储的弹性：内存与磁盘的自动切换； 容错的弹性：数据丢失可以自动恢复； 计算的弹性：计算出错重试机制； 分片的弹性：可根据需要重新分片。 分布式：数据存储在大数据集群不同节点上 数据集：RDD 封装了计算逻辑，并不保存数据 数据抽象：RDD 是一个抽象类，需要子类具体实现 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑 可分区、并行计算 累加器：累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在 Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后， 传回 Driver 端进行 merge。\n广播变量：广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个 或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表， 广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务 分别发送\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 package RDD1 import org.apache.spark.util.LongAccumulator import org.apache.spark.{SparkConf, SparkContext} object task1 { def main(args: Array[String]): Unit = { val sparkConf: SparkConf = new SparkConf().setMaster(\u0026#34;local[*]\u0026#34;).setAppName(\u0026#34;job3task1\u0026#34;) val sc = new SparkContext(sparkConf) //通过文件生成RDD val data = sc.textFile(\u0026#34;hdfs://hadoop1:8020/Data01.txt\u0026#34;) println(data.map( x =\u0026gt; { val student = x.split(\u0026#34;,\u0026#34;)(0) student } ).distinct().count())//学生数量 println(data.map(_.split(\u0026#34;,\u0026#34;)(1)).distinct().count())//课程数量 val value = data.map( x =\u0026gt; { val student = x.split(\u0026#34;,\u0026#34;)(0) val grade = Integer.parseInt(x.split(\u0026#34;,\u0026#34;)(2)) (student, grade) }).filter(_._1==\u0026#34;Tom\u0026#34;) .groupByKey().map{ y=\u0026gt;{ y._2.sum/y._2.size.toDouble }} value.foreach(println) //Tom的课程平均分 val studentCourse = data.map( x=\u0026gt;{ val student = x.split(\u0026#34;,\u0026#34;)(0) (student,1) } ).groupByKey().mapValues(_.size) studentCourse.foreach(println)//学生选课数 val database = data.map( x=\u0026gt;{ val course = x.split(\u0026#34;,\u0026#34;)(1) (course,1) } ).filter(_._1==\u0026#34;DataBase\u0026#34;).groupByKey().map(y=\u0026gt;y._2.size) database.foreach(println)//选择了DataBase课的学生 val avgGrade = data.map( x=\u0026gt;{ val course = x.split(\u0026#34;,\u0026#34;)(1) val grade = Integer.parseInt(x.split(\u0026#34;,\u0026#34;)(2)) (course,grade) } ).groupByKey().mapValues(y=\u0026gt;{y.sum/y.size.toDouble}) avgGrade.foreach(println)//每门课的平均分 val dbCount:LongAccumulator = sc.longAccumulator(\u0026#34;dbCount\u0026#34;) data.foreach { line =\u0026gt; val course = line.split(\u0026#34;,\u0026#34;)(1) if (course == \u0026#34;DataBase\u0026#34;) { // 每当课程为\u0026#34;DataBase\u0026#34;时，累加器的值加1 dbCount.add(1) } } println(\u0026#34;DataBase count: \u0026#34; + dbCount.value) //累加器实现查找选择了DataBase课的学生 sc.stop() } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package SprakReview import org.apache.spark.broadcast.Broadcast import org.apache.spark.{SparkConf, SparkContext} object BroadCastTry { def main(args: Array[String]): Unit = { val sparkConf: SparkConf = new SparkConf().setMaster(\u0026#34;local[*]\u0026#34;).setAppName(\u0026#34;广播变量练习\u0026#34;) val sc = new SparkContext(sparkConf) val v = Array(1,2,3,4,5,6) // 创建广播变量 val broadV:Broadcast[Array[Int]] = sc.broadcast(v) //打印广播变量 println(broadV.value.mkString(\u0026#34;Array(\u0026#34;, \u0026#34;, \u0026#34;, \u0026#34;)\u0026#34;)) //销毁广播变量 broadV.destroy() sc.stop() } } Spark行动算子 Spark转换算子 map mapPartitions flatMap 区别 map 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。 1 2 3 4 5 6 7 /** * Return a new RDD by applying a function to all elements of this RDD. */ def map[U: ClassTag](f: T =\u0026gt; U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u0026gt; iter.map(cleanF)) } mapPartitions 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处 理，哪怕是过滤数据。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn\u0026#39;t modify the keys. */ def mapPartitions[U: ClassTag]( f: Iterator[T] =\u0026gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) =\u0026gt; cleanedF(iter), preservesPartitioning) } map 和 mapPartitions 的区别？\n数据处理角度 Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作。\n功能的角度 Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。 MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变， 所以可以增加或减少数据\n性能的角度 Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。\nflatMap 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射 1 2 3 4 5 6 7 8 /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */ def flatMap[U: ClassTag](f: T =\u0026gt; TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u0026gt; iter.flatMap(cleanF)) } 补充：mapPartitionsWithIndex 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处 理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn\u0026#39;t modify the keys. */ def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) =\u0026gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) =\u0026gt; cleanedF(index, iter), preservesPartitioning) } ","date":"2024-10-27T23:18:18+08:00","permalink":"https://rusthx.github.io/p/spark%E9%9D%A2%E7%BB%8F/","title":"Spark面经"},{"content":"各类型编程语言对比 面向对象式编程(Java、C++)：面向对象编程，是一种程序设计范式，也是一种编程语言的分类。它以对象作为程序的基本单元，将算法和数据封装其中，程序可以访问和修改对象关联的数据。在面向对象编程中，我们可以操作对象，而不需要关心对象的内部结构和实现。\n面向过程式编程(C)：是一种以过程为中心的编程思想, 分析出解决问题的所需要的步骤,然后用函数把这些步骤一步一步实现,然后依次调用;\n面向函数式编程(scala)：函数式编程（Functional Programming, FP）是一种编程范式，它将计算视为数学中函数的求值过程，并避免使用程序状态及可变数据。在函数式编程中，函数被视为\u0026quot;第一等公民\u0026quot;，这意味着函数可以作为参数传递、作为返回值，甚至可以赋值给变量。函数式编程强调使用一系列的函数来处理数据，而不是依赖于数据的状态变化。\n封装、继承与多态 封装 概念： 将一些属性和相关方法封装在一个对象中，对外隐藏内部具体实现细节。内部实现，外界不需要关心，外界只需要根据“内部提供的接口”去使用就可以。\n好处： 使用起来更加方便：因为已经把很多相关的功能，封装成一个整体，类似于像外界提供一个工具箱，针对于不同的场景，使用不同的工具箱就可以；\n保证数据的安全：针对于安全级别高的数据，可以设置成”私有“，可以控制数据为只读（外界无法修改），也可以拦截数据的写操作（进行数据校验和过滤）；\n利于代码维护：如果后期，功能代码需要维护，则直接修改这个类内部代码即可；只要保证接口名称不变，外界不需要做任何代码修改。\n继承 概念： 通过必要的说明能够实现某个类无需重新定义就能拥有另一个类的某些属性和方法，这种关系就称为继承，并且允许多层的继承关系。先定义的类称为父类（基类、超类），后定义的类称为子类（派生类）。\n注：Java中只能单继承（一个子类继承一个父类），而Python、C++中则支持多继承\n多态 多态是指父类的变量可以指向子类对象。允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用）。\n重写与重载 方法的重写(Overriding)和重载(Overloading)是Java多态性的不同表现。\n重写是父类与子类之间多态性的一种表现，重载是一个类中多态性的一种表现。\n如果在子类中定义某方法与其父类有相同的名称和参数，我们说该方法被重写。子类的对象使用这个方法时将调用子类的定义，对它而言，父类中的定义如果被覆盖了。\n如果一个类中定义了多个同名的方法，它们或有不同的参数个数，或有不同的参数类型，则称为方法的重载。重载的方法可以修改返回值的类型。\n构造方法的特殊之处 构造方法必须比类目相同 构造方法没有返回值，但不用void声明 构造方法不能使用static、final、abstract、synchonized和native等修饰符 构造方法不能像一般方法那样用对象.构造方法()显示地直接调用，应用new关键字调用构造方法，给新对象初始化 实例方法与类方法 static修饰地方法称为类方法（或静态方法），而没用static修饰地方法称为是实例方法，二者调用方式不同。 实例方法属于实例，必须通过实例调用；类方法属于类，一般通过类名调用，也可以通过实例调用。二者访问地成员不同。实例方法可以直接访问该类地实例变量和实例方法，也可以访问类变量和类方法；类方法只能访问该类地类变量和类方法，不同直接访问实例变量和实例方法。\n类方法要访问实例变量或调用实例方法，必须首先获得该实例，然后通过该实例访问相关地实例变量或调用实例方法。\n抽象类与接口 相同点：抽象类和接口都可以有抽象方法，都不可以被实例化\n不同点：\n创建类关键字不同：抽象类用关键字abstract，接口用关键字interface创建 成员变量不同：抽象类可以包含普通的变量，接口内的变量只能是final的 方法不同：抽象类可以包含普通的方法，接口内只能有抽象的方法，且都是public的 继承/实现不同：接口可以被多实现，抽象类只能被单继承 Java权限修饰符 在Java编程语言中，权限修饰符用于控制类、变量、方法和构造器的访问级别。Java中有四种主要的权限修饰符：public、protected、default（不写）和private。\npublic修饰符 public修饰符提供了最广泛的访问权限，可以应用于类、成员变量、成员方法和构造方法。使用public修饰的元素可以在任何其他类中访问，无论这些类是否在同一个包中，或者甚至在不同的包中。\nprivate修饰符 private修饰符是最严格的访问控制级别，只能用于成员变量、成员方法和构造方法，但不能用于修饰类（指外部类，内部类除外）。private修饰的元素只能在其所在的类内部访问。尽管如此，可以通过set和get方法向外界提供访问这些私有成员的方式。\ndefault修饰符 default修饰符不需要写出任何关键字，它是当没有指定任何访问修饰符时的默认访问级别。default修饰的元素只能被同一个包中的类访问。\nprotected修饰符 protected修饰符提供的访问权限介于public和default之间。它可以用于成员变量、成员方法和构造方法，但同样不能用于修饰类（外部类，内部类除外）。protected修饰的元素可以被同一个包中的其他类访问，以及不同包中的子类访问。但是，如果不同包中的类想要访问protected修饰的成员，这个类必须是其子类。\n使用原则 在实际开发中，通常遵循最小权限原则，即尽可能使用最严格的访问级别。属性通常使用private封装起来，方法一般使用public以供调用。如果方法需要被子类继承，通常使用protected。default修饰符使用得较少，通常是新手在不了解修饰符的情况下使用。\n通过以上的访问修饰符，Java允许我们在设计类时对信息进行封装，并控制对于类成员的访问级别，这是实现面向对象编程中封装特性的重要手段。\n","date":"2024-10-25T21:13:16+08:00","permalink":"https://rusthx.github.io/p/java%E9%9D%A2%E7%BB%8F%E5%A4%87%E5%BF%98%E5%BD%95/","title":"Java面经备忘录"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 usage: hive -d,--define \u0026lt;key=value\u0026gt; Variable substitution to apply to Hive commands. e.g. -d A=B or --define A=B --database \u0026lt;databasename\u0026gt; Specify the database to use -e \u0026lt;quoted-query-string\u0026gt; SQL from command line (SQL来自命令行) -f \u0026lt;filename\u0026gt; SQL from files (SQL来自文件) -H,--help Print help information --hiveconf \u0026lt;property=value\u0026gt; Use value for given property (通过命令行参数的方式进行配置信息的设置) --hivevar \u0026lt;key=value\u0026gt; Variable substitution to apply to Hive commands. e.g. --hivevar A=B -i \u0026lt;filename\u0026gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) (详细模式,在控制台输出SQL执行) ","date":"2024-10-25T21:12:09+08:00","permalink":"https://rusthx.github.io/p/hive%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0/","title":"Hive启动参数"},{"content":"定义 数据库死锁是在多个事务执行过程中发生的一种状态，其中每个事务都在等待其他事务释放它们需要的资源，而这些资源又被其他事务占用。这种相互等待的情况导致事务无法继续执行，因为没有任何事务能够获取它们所需的全部资源来完成操作。\n死锁死循环四要素 互斥条件：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。 请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。 死锁避免方案 数据库管理系统通常会实现死锁检测和解决机制。当检测到死锁时，系统会选择一个事务进行回滚，以解除死锁状态。选择回滚哪个事务通常基于事务的复杂度，系统会尽量选择代价最小的事务进行回滚。\n为了避免死锁，可以采取以下措施：\n保持一致的加锁顺序：确保所有事务都以相同的顺序请求锁。\n减少锁的持有时间：尽快完成事务操作并释放锁，避免长时间持有锁。\n使用锁超时：设置锁的超时时间，超时后事务自动回滚，释放锁。\n检测死锁并重试：在应用程序中捕获死锁异常，并实现重试机制。\n","date":"2024-10-13T21:32:40+08:00","permalink":"https://rusthx.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%AD%BB%E9%94%81/","title":"数据库死锁"},{"content":" 参考资料：B站@左美美_ 相关视频\n数据倾斜定义 任务进度长时间维持在99%，查看任务监异页面，发现只有少量1个或几个reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多，最长时长远大于平均时长。\n数据倾斜产生原因 map输出数据按keyHash的分配到reduce中，由于key分布不均匀、业务数据本身的特性、建表时考虑不周、某些SQL语句本身就有数据倾斜等原因，造成的reduce上的数据量差异过大，所以如何将数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。\nKey为空引起数据倾斜 倾斜原因 join的key值发生倾斜，key值包含很多空值或是异常值。\n解决方案 对值为空的key进行打散，为空key赋一个随机的值，使得key值为空的数据随机均匀地分布到不同的reducer上。\n测试案例 设置多个reduce任务\nset mapreduce.job.reduces = 5;\n两张大表join，做全连接\n1 2 3 4 5 6 7 select t.id ,t.year ,t.temperature ,s.state from temperature t full join station s on nvl(t.id,rand())=s.id limit 10; 备注：nvl()为空值转换函数，rand()为随机函数。也可以使用ifnull()、coalesce()\ngroup by 引起数据倾斜 倾斜原因 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n解决方案 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。 实现方式1\n1）是否在Map端进行聚合，默认为True（使用Combiner局部合并）\nset hive.map.aggr = true;\n2）设置map端预聚合的行数阈值\nset hive.groupby.mapaggr.checkinterval=100000;\n实现方式2\n有数据倾斜的时候进行负载均衡（默认是false）\nset hive.groupby.skelindata= true;\n当遇到数据倾斜时，groupby会启动两个MR job。第一个job会将map端数据随机输入reducer，每个reducer做部分聚合，相同的key就会分布在不同的reducer中。第二个job再将前面预处理过的数据按key聚合并输出结果，这样就起到了均衡的效果。\n测试案例 1 2 3 set hive.map.aggr =true; set hive.groupby.mapaggr.checkinterval= 100000; set hive.groupby.skewindata= true; 1 2 3 select id,count(*) from temperature group by id; count distinct引起数据倾斜 倾斜原因 count distinct聚合时存在大量特殊值，比如存在大量值为NULL或空的记录。\n解决方案 做count distinct时，将值为空的情况单独处理。\n1）如果只是统计去重后的记录数，可以不用处理空值，先把空值过滤掉，然后在最后结果中加1即可\n2）如果还包含其他计算，需要进行groupby操作，先将值为空的记录单独处理，然后再跟其他计算结果union操作。\njoin操作引起数据倾斜 大表join小表(hive旧版本) 新版本(Hive3)已经自动自动优化\n1）产生原因\n业务数据本身就存在key分布不均匀的情况，一般情况会产生数据倾斜\n2）解决方式\n使用map join让小的维度表先进内存，在map端完成join\n3）实现原理\n使用map join，直接在map端就完成表的join操作，进入map端的数据都是经过split得到的，没有根据key分区这一操作，所以数据都是相对均匀地分布在每个maptask中的，所以就不会产生数据倾斜。\n大表join大表 1）产生原因\n业务数据本身的特性，导致两个表都是大表。\n2）解决方式\n业务消减\n3）实现原理\n业务数据有数据倾斜的风险，但是这些导致数据倾斜风险的key一般都是无效的，如uid为空，因为uid为空的记录是没有意义的。\n所以当业务数据很大，但是数据中的大部分（一般都是80%）可能都是无效数据，那么就可以在join时过滤掉空值uid，没有了这些无效数据，自然就不存在这么大量集中的key，数据倾斜的风险就会消失。\n","date":"2024-09-19T08:54:12+08:00","permalink":"https://rusthx.github.io/p/%E6%95%B0%E4%BB%93hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/","title":"数仓(Hive)数据倾斜产生原因及处理方式"},{"content":"问题 有一张交易流水表（transaction），主键为账号，每个账号有所属公司。有一张公司信息表（company_info），主键为公司id，表中有上级公司id。\n需要得到每个公司的交易信息（资金流入流出余额），但是每个公司的数据都应该是该公司及下属公司的汇总。 但是数据库并不支持树形结构也不支持层级结构。\n方案 主要难题在于公司信息表，公司只有上面一级的信息，没有更上面的信息，也没有下属公司信息。\n所以需要做一张公司树表（company_tree），记录上下关系和层级信息，加工方案如下：\n设置字段为公司id，公司id树（从最顶层公司到当前公司，类似'0011-0022\u0026rsquo;），公司层级,上级公司id(sup_comp_id)。\n插入最顶层公司信息（\u0026lsquo;0011\u0026rsquo;）,设置该公司层级为1级。id树为'0011\u0026rsquo; 然后查询company_info表，插入上级为（\u0026lsquo;0011\u0026rsquo;）的公司信息，设置层级为2级，拼接上级公司id树(\u0026lsquo;0011\u0026rsquo;)和当前公司id作为当前公司的id树。 然后插入三级公司、四级公司，直到最底层公司。不知道到底有多少层可以插入一层后观察company_tree表有无新增数据。\n注意：三级及更下级的公司需要冗余多行，每行的上级公司id不同（分别为顶级到当前公司的父级公司的公司id）\n关联company_tree表和transaction,group by sup_comp_id,再对资金流入流出余额进行sum()聚合。 这里加一个按层级的过滤条件，一层一层给地查，然后再union查询结果即可得到所有公司的信息 观察上面的方案不难发现，这张加工的company_tree不好用，公司信息比较少变（除了股市），一般都是作为数仓的维度表。 但是每次需要关联查询company_tree时写的查询语句都很复杂，那么怎样才能不用union各层公司信息呢？\n设计company_tree的时候再加一个up_comp_tree字段。这样聚合的时候group by up_comp_tree就能拿到所有公司的汇总信息。 （因为公司的下属公司的所有下一级公司的up_comp_tree都是当前公司的id树）\n","date":"2024-09-14T17:49:28+08:00","permalink":"https://rusthx.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0%E6%A0%91%E7%8A%B6%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84/","title":"数据库实现树状（层级）结构"},{"content":"DataX介绍 DataX的Github介绍如下：\nDataX 是阿里云 DataWorks数据集成 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具/平台。DataX 实现了包括 MySQL、Oracle、OceanBase、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、Hologres、DRDS, databend 等各种异构数据源之间高效的数据同步功能。\nDataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\n问题 数仓开发时需要从异构数据源获取数据作为ODS层（原始数据层），同时还需要在数仓间进行ETL(数据抽取、转换、加载)。通过DataX来实现数据同步。\n实现 方案1 shell实现 1.写通过DataX将数据同步到ODS层的配置文件（reader、writter）\n2.通过shell脚本传入传入target_dir(数据库表名，判断是否是需要全量同步的表，若是，则进行全量同步)和datax_config(即步骤1的配置文件)\n3.将数仓间ETL的SQL脚本也封装成shell脚本\n4.通过dolphinscheduler调度步骤2和步骤3的shell脚本实现数据同步到数仓ODS层并在数仓间ETL\n缺点：系统复杂度高，不易维护\n优点：更加灵活，在部分自定义函数功能支持不是很友好的框架（Hive）里也能很好地发挥作用\n方案2 python实现 1.写通过DataX将数据同步到ODS层的配置文件（reader、writter）\n2.将异构数据源配置信息写成文件，然后在python脚本中调用配置信息，拼接成完整的DataX同步命令。\n3.将数仓间ETL的SQL脚本封装成自定义函数，再在步骤2的python脚本中连接数仓执行自定义函数\n4.在服务器上通过contrab设置定时任务实现自动执行\n缺点：Hive的自定义函数支持不是特别友好，部分功能可能没法实现。通用性差\n优点：系统复杂度低，DataX和python脚本都不用一直在线，定时调度任务开始执行时才会启动python脚本，降低了服务器负担\n补充：DataX的增量同步 数仓的增量同步一般都是用消息队列+数据库监听框架+数据同步框架（kafka+Maxwell+Flume）。 但是在离线数仓实时性要求不高的场景下也可以用DataX来实现增量同步。 实现原理：配置文件中reader部分支持column和querySQL,可以在querySQL中加入过滤条件来获取较新的数据。\n比如如果需要获取最新的每日交易数据，就可以加一个时间为最新日期或者传入日期参数的过滤条件。\n如果表中没有时间这种自然增长的字段也可以使用单调递增的id之类的。总之就是获取最新的一批数据就可以。\n然后为了保证数据一致性，还需要在wrritter的preSQL中删除符合reader的querySQL的数据。\n补充：crontab命令 参考资料：https://cloud.tencent.com/developer/article/2359335\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 （1）语　法： crontab [-u \u0026lt;用户名称\u0026gt;][配置文件] 或 crontab { -l | -r | -e } -u #\u0026lt;用户名称\u0026gt; 是指设定指定\u0026lt;用户名称\u0026gt;的定时任务，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设定自己的定时任务。 -l #列出该用户的定时任务设置。 -r #删除该用户的定时任务设置。 -e #编辑该用户的定时任务设置。 （2）命令时间格式 : * * *　*　*　command 分　时　日　月　周　命令 第1列表示分钟1～59 每分钟用*或者 */1表示 第2列表示小时1～23（0表示0点） 第3列表示日期1～31 第4列表示月份1～12 第5列标识号星期0～6（0表示星期天） 第6列要运行的命令 （3）一些Crontab定时任务例子： 30 21 * * * /usr/local/etc/rc.d/lighttpd restart #每晚的21:30 重启apache 45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart #每月1、10、22日的4 : 45重启apache 10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart #每周六、周日的1 : 10重启apache 0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart #每天18 : 00至23 : 00之间每隔30分钟重启apache 0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart #每星期六的11 : 00 pm重启apache * 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart #晚上11点到早上7点之间，每隔一小时重启apache * */1 * * * /usr/local/etc/rc.d/lighttpd restart #每一小时重启apache 0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart #每月的4号与每周一到周三的11点重启apache 0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart #一月一号的4点重启apache */30 * * * * /usr/sbin/ntpdate cn.pool.ntp.org #每半小时同步一下时间 0 */2 * * * /sbin/service httpd restart #每两个小时重启一次apache 50 7 * * * /sbin/service sshd start #每天7：50开启ssh服务 50 22 * * * /sbin/service sshd stop #每天22：50关闭ssh服务 0 0 1,15 * * fsck /home #每月1号和15号检查/home 磁盘 1 * * * * /home/bruce/backup #每小时的第一分执行 /home/bruce/backup这个文件 00 03 * * 1-5 find /home \u0026#34;*.xxx\u0026#34; -mtime +4 -exec rm {} \\; #每周一至周五3点钟，在目录/home中，查找文件名为*.xxx的文件，并删除4天前的文件。 30 6 */10 * * ls #每月的1、11、21、31日是的6：30执行一次ls命令 ","date":"2024-09-14T16:58:47+08:00","permalink":"https://rusthx.github.io/p/%E9%80%9A%E8%BF%87datax%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE/","title":"通过DataX同步数据仓库数据"},{"content":"分类 join有如下种类\n(inner) join\nleft (outer) join\nright (outer) join\ncross join :笛卡尔积，与inner join不指定on等效\nstraight_join :效果等同于inner join，只是指定左表为驱动表\nfull (outer) join :全外连接，MySQL中不支持。可用left join union right join 实现。\n例如：\n1 2 3 select t1.* from t1 full join t2 on t1.id =t2.id 在MySQL中可以用下列方式实现：\n1 2 3 4 5 6 7 select t1.* from t1 left join t2 on t1.id =t2.id union select t1.* from t1 right join t2 on t1.id =t2.id 驱动表与被驱动表 inner join :由驱动器决定\nleft join :左表为驱动表，右表为被驱动表\nright join :左表为驱动表，右表为被驱动表\nstraight_join :固定左表为驱动表，右表为被驱动表\njoin执行流程 每次取驱动表一行数据，去和被驱动表匹配，即双重for循环\njoin执行的实现方式 Nest Loop Join(NLJ):单纯的双层循环\nBlock Nest Loop Join(BNLJ):在NLJ的基础上，利用Join Buffer，一次取出一批驱动表数据，可以减少循环匹配次数\nIndex Nest Loop Join(INLJ):在NLJ的基础上，利用被驱动表连接字段的索引直接找到匹配数据，可以减少循环次数\n小表驱动大表 参考资料：https://www.bilibili.com/video/BV1ms4y177mr/?spm_id_from=333.337.search-card.all.click\u0026amp;vd_source=2db7c64d895a2907954a5b8725db55d5\n小表驱动大表是一种常见的SQL优化手段，其原因如下： 两表关联时会产生一个Join Buffer(关联缓存区)。Join Buffer是优化器用于处理连接查询操作时的临时缓冲区，简单来说当需要比较两个或多个表的数据进行Join操作时，JOin Buffer可以帮助MySQL临时存储结果，以减少磁盘读取和CPU负担，提高查询效率。需要注意的是每个join都有一个单独的缓冲区。\nBNLJ会将驱动表数据加载到Join Buffer里，然后再批量与被驱动表进行匹配，如果驱动表数据流量较大，Join Buffer无法一次性装载驱动表的结果集，将会分阶段与被驱动表进行批量数据匹配，然后记录结果并将结果返回。如果数据量过大，Join Buffer无法一次性加载完成就会分阶段匹配，增大了磁盘读取，降低了效率\n所以总结如下：\n小表可以被完全加载到内存（Join Buffer）中： 小表的数据量相对较少，可以被完整加载到内存中，减少了磁盘IO的开销。而大表的数据量较大，可能无法完全加载到内存，需要进行磁盘IO操作，会导致性能下降。\n减少了数据传输量： 将小表作为驱动表可以先获取小表的结果集，再根据小表的结果集进行大表的关联查询。这样可以减少传输到被驱动表的数据量，减少网络传输的开销。\n利用索引优化(INLJ)： MySQL的查询优化器通常会选择使用索引来优化关联查询。将小表作为驱动表可以更好地利用索引，因为小表的索引更容易被缓存并快速定位。\njoin on on后跟连接条件，一般必须指定，且只对被驱动表有效，即使对驱动表加了过滤条件，该条件也无效。\n所以，在join on之后，驱动表包含全部数据，被驱动表只包含on条件过滤后的数据。\n注：inner join 后的数据只会是下面两个椭圆的交集\non和where on 在join时就会过滤数据，而where是join完成后再对数据进行过滤，所以on比where先作用。\n所以，理论上过滤条件放在on后比放在where后性能更好，因为这样可以有更少的数据进入磁盘IO。\n但是，由于on后的条件只对被驱动表有效，过滤条件放在on后和where后的结果可能会不一致，所以谨慎在on后加驱动表的过滤条件。\n对于inner join，on和where就没有区别了\n多表关联查询优化 加过滤条件要想清楚，先对被驱动表过滤还是join完后再一起过滤\n尽量小表驱动大表，这是针对left join和right join的情况，inner join会由优化器自行选择\nexplain分析SQL语句得到的执行计划的第一行即是驱动表\n优化join思路：一切为了减少join 时驱动表匹配被驱动表时的循环次数。如果join后的数据量很大，并且还要进行聚合操作，在不影响查询结果的情况下可以考虑先聚合出临时表再进行join\n减少单表数据量，如水平分表、垂直分表\n静态的数据可以在后端进行缓存\n补充：分表设计 分表原理：一个大表按照一定的规则分解成多张具有独立存储空间的实体表。这些表可以分布在同一块磁盘上，也可以在不同的磁盘上。\n通过分表实现用户在访问数据时，因不同的条件而访问不同的表，将数据分散在各个实体表中，减少单表的访问压力，提升数据查询效率\n水平分表 以字段为依据，按照一定的策略，使用hash、range、list等方式将一个表的数据拆分成多个相同结构的表中。 水平分表是为了降低单表的数据量，解决单表的热点问题。\n比如按时间特性进行划分，将表数据分成历年数据表或者历史数据表（已完成）+在线数据表（正在进行）。 水平分表后的表通过union能还原回原来的表。 垂直分表 根据字段查询频率将表中数据拆分为不同结构的表（主表和扩展表）。 例如将热点数据和非热点数据分块存储，这样在查询热点数据时就能将数据缓存起来，减少了随机读取IO，提高了命中率。 适用于由于字段较多引起数据量和访问量较大的情况，且每个业务场景只访问部分字段。\n例如：用户对商品感兴趣才会查看详细描述，而详细描述占用存储较多（Text）,可以将该字段垂直分割 垂直分表后的表通过join可以还原回原来的表\n垂直分表的优点：\n不同的业务场景访问不同的内容，数据量小，提升性能 集成中数据传输量小 不同业务场景业务量访问频率不一样，表的操作更新可以更加灵活地控制 降低业务耦合度 垂直分割可使行数据变小，一个数据块就能存更多数据，在查询时可以有效减少IO次数。垂直分表可以有效利用Cache 分区分表对比 定义：分区是在一张表中，根据某种规则将数据分散到不同的物理存储区域。分表则是将一张大表拆分成多张小表。 数据访问：在分区中，用户无需知道数据在哪个分区，可以像访问普通的表一样访问数据，但在分表中，用户必须先知道数据在哪张表中才能访问到所需数据 适用场景：分区适用于数据量大，但查询范围有限的场景，而分表适用于数据量大，查询范围广的场景。 性能：分区可以提高查询性能，因为查询只需要在一个分区内进行（过滤条件使用了分区字段），而不是在整张表中。分表可以提高整体性能，因为每个表的数据量都变少了 管理：分区可以减少数据的恢复。分表可以使每个表的大小更容易得到控制 广播表与分布式表 广播表：小表广播功能能提高跨库场景的性能和简化跨库场景的开发。 将需要广播的数据推送到目标库，冗余了表数据，方便在库内关联查询。\n1 2 3 4 5 create table config( id int primary key, config_key varchar(255), config_value varchar(255) )broadcast; 分布式表：分布式表是指其数据根据某种分片策略在分布式数据库系统的不同节点上。 数据被分成多个分段，每个分段存储在不同的节点上。 分布式表的分布策略可以基于hash、range、list等方式。 分布式表适用于数据量大且需要水平扩展的场景.\n1 2 3 4 5 create table users( user_id int primary key, uname varchar(255), email varchar(255) )distributed by hash(user_id); 反范式 属性冗余：在一个表中除了存储关联表的主键外，将关联表的非键字段也存储到此表的处理方式。 有点像垂直分表的反向操作\n1 2 3 (user_id check_date score),(user_id user_name telephone) -\u0026gt; (user_id check_date score user_name telephone) 级联属性冗余：多表关联时，A关联B,B关联C。在查询时需要同时获取A、B、C三个表的属性或以它们的属性进行条件过滤 ，为了减少表关联以提高性能，可以考虑在B表中冗余需要访问的C表字段，减少频繁的表关联操作。\n例如：在员工表中冗余部门表的信息（部门id 部门名称 部门负责人）\n表冗余：针对数据记录进行冗余，即A表的数据复制多份，或者多表关联的结果数据存储成一张表。\n直接复制：“广播表模式”，可以提高跨库访问的性能 加工派生：冗余的数据是源表加工后的数据或多表关联的结果 ","date":"2024-09-12T23:32:36+08:00","permalink":"https://rusthx.github.io/p/mysql-join%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BC%98%E5%8C%96/","title":"MySQL Join介绍与优化"},{"content":"我写的这篇博客只是我学习的一点总结，并不系统，也不全面。想要学习较为全面的知识可以看 小林coding的图解MySQL相关部分\n密集列索引 密集列，是指一张表上列的数据没有离散度，列的取值范围较小，高度集中在少数几个值中（如性别）。\n一般来说，密集列由于没有离散度，不适合作为索引列。使用这类密集索引的实际效率可能会低于全表扫描。但是，假如密集列的倾斜度很高时，例如有一个状态列表示是否停用，大部分对象都为停用，那么就可用使用密集列作为索引 需要保证要查询的数据分布较少，（低于总数据量的5%）。同时还要确定是否有为这一个查询单独优化的需要，因为索引也会占用空间，建太多索引反而会降低查询效率，甚至会出现加了索引，查询效率反而变慢的情况。优化并不是将每个查询都单独优化到1s内，而是衡量损失后在妥协中得到一个平衡，让慢查询只占很少比例，优先保证查询次数多的语句。 可能会频繁变更的列不宜作为索引列，因为索引列变更会导致索引重排，也即是B+树的树结构变更。这会导致修改效率大幅下降 索引合并(index_merge) 当数据过滤条件分布在多个索引列上时（无论是and还是or），MySQL为了提升数据访问效率会使用多个索引合并的方式过滤数据。、\n索引合并会导致单个索引过滤的数据量越大，查询效率下降越明显\n锁 MySQL的行锁是通过对索引加锁来实现的。数据修改时会根据过滤条件匹配对应的索引，在对应的索引上加锁。如果语句中修改了索引的值，就还会在修改后的值上加锁。其他事务使用相同的索引值修改数据不管是否是相同的行均会被阻塞。\n多线程下应该使用主键来修改数据降低阻塞的概率。\n如果update/delete的where条件没有使用索引或者没有where条件，就会全表扫描，会对所有记录加上next-key锁（record锁+gap锁）,相当于把整张表锁住了\n","date":"2024-09-12T23:05:47+08:00","permalink":"https://rusthx.github.io/p/mysql-%E7%B4%A2%E5%BC%95/","title":"MySQL 索引"},{"content":"删除 删除未在他表出现的数据 下面有一条效率较差的删除语句，主要功能是将t1表中id未出现在t2表的记录删除。效率差的原因是in中用了子查询，导致删除语句不会走索引，从而导致锁全表，继而导致删除效率差。\n1 2 3 4 5 delete from t1 where id not in ( select id from t2 ) 优化方法:取消子查询，改用关联删除，这样就可以使用建在id列上的索引。关联删除/更新在建立索引的情况下效率远高于in/exists\n优化后的SQL语句如下：\n1 2 3 delete t1 from t1 left join t2 on t1.id=t2.id where t2.id is null 注：上面这条是MySQL独有的优化，达梦数据库和Oracle中可用如下语句。（拾人牙慧，未经验证，用这两个数据库的朋友可以自行验证一下）(+)表示单侧关联，该符号在哪边哪边就是副表。\n1 2 3 4 5 6 delete from t1 where rowid in( select t1.rowid from t1,t2 where t2.id(+)=t1.id and t2.id is null ) 删除多表相同数据 好的删除语句如下：\n1 2 3 delete t1,t2 from t1,t2 where t1.id=t2.id 这样可以同时删除从主从表删除。比如一条记录记录在多张表中，删除这条记录需要同时删除两张表的记录。这样可以保证要么全部删除，要么全部不删，间接满足了事务一致性。（数据库只会从一个状态转移至另外一个状态，即拥有这条记录和没有这条记录的两个状态。这一条记录可以看成是一个入库记录、一张支票）\n删除全表数据 用truncate替代delete。这里涉及delete的机制，delete并不是直接在磁盘中删除记录，而是将记录加一个标记，并设置为不可见，然后在数据库压力小时异步删除磁盘中的数据。但是这样有一个问题，虽然标记为删除后，查询表记录不可见。但是记录仍然占有着磁盘空间，这会拖慢查询数据库的速度。\n1 truncate table t1; 更新 差的更新语句如下：\n1 2 3 update t1 set c1=\u0026#39;\u0026#39; where id in (...) 如删除篇中所说，由于in中属性过多，in不会再走索引（当属性值大于4个后就不会再走索引，in中是子查询的话就不会走索引）。所以这里的更新效率慢，并且还会锁住整表。\n关于为什么全表扫描会锁住整张表可以看小林的教程：update 没加索引会锁全表？\n优化方式：\n创建id临时表，临时表只有id一个字段 批量插入临时表，记录为上面差的更新语句中的记录，也即是需要更新的记录的id 将临时表于原表关联更新 补充：插入更新 插入一条数据，如果存在主键或唯一键冲突，则更新记录\n注：使用此语句时，必须在表中定义主键或唯一约束\n1 2 3 4 5 insert into [table_name] (column1,column2,column3...) values (values1,values2,values3...) on duplicate key update column1 = values(column1), column2 = values(column2),... 例如：\n1 2 3 4 insert into users (id,`name`) values (1,\u0026#39;Alice\u0026#39;) on duplicate key update name = values(`name`) PostgreSQL中用法如下：\n1 2 3 4 insert into users (id,`name`) values (1,\u0026#39;Alice\u0026#39;) on conflicate (id) do update set name = excluded.name 如果想同时修改多个字段也可用下面的写法\n1 2 3 4 insert into users (id,`name`，age) values (1,\u0026#39;Alice\u0026#39;,18) on conflicate (id) do update set (`name`,age) = excluded.(`name`,age) 如果想遇见冲突主键不做处理可用如下语句\n1 2 3 4 insert into users (id,`name`) values (1,\u0026#39;Alice\u0026#39;) on conflicate (id) do nothing ","date":"2024-09-12T22:15:33+08:00","permalink":"https://rusthx.github.io/p/mysql%E5%88%A0%E9%99%A4%E4%BF%AE%E6%94%B9%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96/","title":"MySQL删除修改数据优化"},{"content":"题目 给定一个整数数组，进行随机交换，要求交换后的数组中每个元素都不在其原来的位置上\n思路 1.遍历数组，随机一个不为当前下标的下标，将两个位置的元素交换位置\n2.可能会出现某个元素交换多次后又回到原位置的情况，所有需要再遍历一遍数组，如果出现此情况就将交换过的数组再递归交换，直到没有这种情况\n答案 注意：数组在交换前需要先拷贝一份，这样才能验证每个元素是否在原位置上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public static int[] randomSwap(int[] nums){ int[] arr = nums.clone(); Random random = new Random(); for (int i = 0; i \u0026lt; nums.length; i++) { int j=i; while(j==i){ j=random.nextInt(nums.length-1); } int temp =arr[j]; arr[j]=arr[i]; arr[i]=temp; } for (int i = 0; i \u0026lt; nums.length; i++) { if (arr[i] == nums[i]) { return randomSwap(nums); } } return arr; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import random def derange_array(array): n = len(array) result = array[:] for i in range(n): j = i while j == i: j = random.randint(0, n - 1) # 交换元素 result[i], result[j] = result[j], result[i] # 确保没有元素在其原来的位置上 for i in range(n): if result[i] == array[i]: return derange_array(array) # 如果有元素在原来位置上就递归交换 return result # 验证测试 array = [1, 2, 3, 4, 5] result = derange_array(array) print(result) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import scala.util.Random object Derangement { def main(args: Array[String]): Unit = { val array = Array(1, 2, 3, 4, 5) val result = derangeArray(array) println(result.mkString(\u0026#34;, \u0026#34;)) } def derangeArray(array: Array[Int]): Array[Int] = { val n = array.length val result = array.clone() val rand = new Random() for (i \u0026lt;- 0 until n) { var j = i while (j == i) { j = rand.nextInt(n) } // 交换元素 val temp = result(i) result(i) = result(j) result(j) = temp } // 确保没有元素在其原来的位置上 for (i \u0026lt;- 0 until n) { if (result(i) == array(i)) { return derangeArray(array) // 如果有元素在原来位置上就递归交换 } } result } } ","date":"2024-09-11T14:43:00+08:00","permalink":"https://rusthx.github.io/p/java%E9%9A%8F%E6%9C%BA%E4%BA%A4%E6%8D%A2%E6%B4%97%E7%89%8C/","title":"Java随机交换(洗牌)"},{"content":" 参考资料：《SQL进阶》P106 (鹿书)\n关系（表）结构 现有一张住宿表(stay_people)如下\nguest(入住客人) start_date(入住时间) end_date(退房时间) 阿良良木历 2006-10-26 2006-10-27 阿良良木月火 2006-10-28 2006-10-31 阿良良木火怜 2006-10-31 2006-11-01 忍野忍 2006-10-29 2006-11-01 忍野扇 2006-10-28 2006-11-02 战场原黑仪 2006-10-28 2006-10-30 千石抚子 2006-10-30 2006-11-02 问题：判断这些客人住店时间存在重叠，如果存在重叠，则展示客人的名字、入住时间和退房时间\n问题分析 很明显这道题的重点是判断两个时间段是否相交，那么时间相交有如下三种情况：\n答案 1.自关联然后判断是否为三种情况之一，如果符合一种，那么时间相交\n1 2 3 4 5 6 7 select t1.guest ,t1.start_date ,t2.start_date from stay_people t1,stay_people t2 where (t1.start_date\u0026lt;=t2.end_date and t1.start_date\u0026gt;=t2.start_date) or (t1.end_date\u0026gt;=t2.start_date and t1.start_date\u0026lt;=t2.start_date) or (t1.start_date\u0026gt;=t2.start_date and t1.end_date\u0026lt;=t2.end_date) 2.比较自关联后一行的最小的end_date和最大的start_date来判断两个时间段是否相交。\n1 2 3 4 5 select t1.guest ,t1.start_date ,t2.start_date from stay_people t1,stay_people t2 where greatest(t1.start_date,t2.start_date)\u0026lt;=least(t1.end_date,t2.end_date) 3 使用数据库的内置函数判断时间段是否相交\n1 2 3 4 5 select t1.guest ,t1.start_date ,t2.start_date from stay_people t1,stay_people t2 where (t1.start_date,t1.end_date) overlaps (t2.start_date,t2.end_date) 但是，这种判断默认的时间段是左闭右开的，即认为住宿时间为[start_date,end_date),并且这个函数只有SQL Server、PostgreSQL、Oracle支持，MySQL并不支持这种写法。未列举的数据库不一定不支持，可以查一下相关文档\nPostgreSQL时间函数文档\n我写的只是三种类型的处理方法，除了我的写法，还有许多别的写法，我只是做一个简单总结\n","date":"2024-09-08T10:15:20+08:00","permalink":"https://rusthx.github.io/p/sql%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E4%BA%A4%E9%9B%86/","title":"SQL计算时间交集"},{"content":"这里以老师的指导文档为例，实现一个基于flask和redis的web网页，用户每输入网址浏览一次就加一次浏览量并显示在网页上。\nDocker拉取镜像 拉取Python镜像\n1 sudo docker pull python 拉取redis镜像\n1 sudo docker pull redis 查看镜像\n1 docker images 编辑所需文件 编辑Python程序，实现一个基于flask的web应用，在redis中存一个数值，初值为0，每访问一次网站，计数加1。 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import time import redis from flask import Flask app = Flask(__name__) cache = redis.Redis(host=\u0026#39;127.0.0.1\u0026#39;, port=6379) def get_hit_count(): retries = 5 while True: try: return cache.incr(\u0026#39;hits\u0026#39;) except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5) @app.route(\u0026#39;/\u0026#39;) def hello(): count = get_hit_count() return \u0026#39;Hello rust! Hello hx! I have been seen {} times.\\n\u0026#39;.format(count) 编写dockerfile\n1 2 3 4 5 6 7 8 FROM python:latest WORKDIR /code ENV FLASK_APP hxapp.py ENV FLASK_RUN_HOST 0.0.0.0 RUN pip install redis flask -i https://mirror.baidu.com/pypi/simple COPY hxapp.py hxapp.py EXPOSE 5000 CMD [\u0026#34;flask\u0026#34;,\u0026#34;run\u0026#34;] 豆瓣源下redis和flask会报错如下，根据Windows下载这两个包的经验，猜测应该是镜像源的问题，换成百度源，解决问题。 生成新的镜像 1 sudo docker build -t onccn/myflask:v2 -f ./dockerfile . 从kubernetes拉取docker本地镜像 编辑flaskredisdeploy.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: apps/v1 kind: Deployment metadata: name: flaskredis spec: selector: matchLabels: app: flaskredis template: metadata: labels: app: flaskredis spec: containers: - name: flaskredis image: onccn/myflask:v2 imagePullPolicy: Never resources: limits: memory: \u0026#34;1500Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; - name: redis image: redis:latest imagePullPolicy: Never resources: limits: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; 1 kubectl apply -f flaskredisdeploy.yaml 访问Web和Windows访问 虚拟器中浏览器输入上面查看日志得到的链接 编辑service.yaml\n1 gedit myflaskservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: myflaskservice spec: selector: app: flaskredis ports: - protocol: TCP port: 8080 #Service的端口号 targetPort: 5000 #容器暴露的真实端口号 nodePort: 30081 #node的真实端口号 type: NodePort 创建service\n1 kubectl apply -f myflaskservice.yaml 遇见报错,这个报错的意思是该端口已经被占用，修改端口号 成功创建service,查看service\n1 kubectl get svc -o wide Windows访问k8s集群的service，进而访问相应的pod ip地址为node的地址（k8s集群中应用service的节点） ","date":"2024-09-07T12:50:40+08:00","permalink":"https://rusthx.github.io/p/kubernetesk8spod%E9%83%A8%E7%BD%B2/","title":"Kubernetes（K8s）Pod部署"},{"content":" 参考资料：https://www.youtube.com/watch?v=3mdCiFu52XA\u0026amp;t=8s\nvscode安装k8s插件 值得一提的是安装k8s插件后编辑k8s所需yaml文件会非常简单。比如需要一个deploy，那只需要输入deploy再按一下TAB键就可以使用自动补全，得到一份模板代码。同时这个插件还能检查k8s语法，比如container是否限制资源。 在yaml文件中加入参数imagePullPolicy: Never，这个参数的意思是禁用docker注册表和docker hub，从本地的docker拉取镜像。 ","date":"2024-09-07T12:48:13+08:00","permalink":"https://rusthx.github.io/p/kubernetesk8s%E4%BD%BF%E7%94%A8%E6%9C%AC%E5%9C%B0docker%E9%95%9C%E5%83%8F/","title":"Kubernetes(K8s)使用本地Docker镜像"},{"content":" 参考资料:https://cloud.tencent.com/developer/article/2347138\n禁用Ubuntu Swap 1 sudo gedit /etc/fstab ubuntu24.04这个swapfile配置文件略有调整 Ubuntu安装网桥工具 1 sudo apt-get install bridge-utils -y 添加k8sAPT镜像源 新版命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 1. 安装必要工具 sudo apt update sudo apt install -y apt-transport-https ca-certificates curl gpg # 2. 创建 keyring 目录 sudo mkdir -p /etc/apt/keyrings # 3. 下载官方 GPG 密钥 curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg # 4. 添加 APT 源（官方地址，支持 noble） echo \u0026#34;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list # 5. 更新并安装 sudo apt update sudo apt install -y kubelet kubeadm kubectl # 6. 锁定版本（防止意外升级） sudo apt-mark hold kubelet kubeadm kubectl 旧版命令 下面这个是旧版的命令，在Ubuntu24.04里已经不支持了。权且保留以作参考。\n1 2 3 4 5 sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026#34;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\u0026#34; sudo curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - sudo add-apt-repository \u0026#34;deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\u0026#34; 配置containerd 旧版命令 这里需要安装containerd，但是我在docker安装的时候已经装好了，相关命令为\n1 sudo apt-get install containerd.io （1）Containerd安装完成后，其自带的配置文件/etc/containerd/config.toml中的内容，需要用打印出的containerd默认配置替换。 （2）Containerd的Cgroup设为systemd，以和k8s默认的Cgroup保持一致。 （3）pause镜像路径改为国内源registry.aliyuncs.com/google_containers/pause:3.9。\n1 2 3 4 5 sudo cp /etc/containerd/config.toml /etc/containerd/config.toml.ori sudo chmod 777 /etc/containerd/config.toml sudo containerd config default \u0026gt; /etc/containerd/config.toml 1 sudo gedit /etc/containerd/config.toml 配置后，重启containerd服务，并保证containerd状态正确\n1 2 sudo systemctl restart containerd.service sudo systemctl status containerd.service 新版命令 使用Ubuntu官方仓库的container(Ubuntu官方仓库叫containerd,不叫containerd.io,containerd.io是docder仓库的包)。安装命令如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 sudo apt install containerd # 配置containrd sudo mkdir -p /etc/containerd # 创建/etc/containerd/config.yaml，修改镜像源和cgroup sudo tee /etc/containerd/config.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; version = 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; EOF 创建/etc/containerd/certs.d目录，在这个目录填入docker.io和registry.k8s.io的镜像源。新版本的k8s在config.yaml改会有冲突问题，不能生效。\n注意：k8s里修改镜像源之后，使用kubectl describe pod \u0026lt;pod_name\u0026gt; 查看时还是显示的docker.io和registry.k8s.io。配置镜像源只物理修改从哪里修改，不改镜像拉取的逻辑源。所以改好镜像源之后也不太好验证成功，随便拉个镜像sudo crictl pull nginx:1.14.2，能拉下来就是成了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Docker Hub 加速 sudo mkdir -p /etc/containerd/certs.d/docker.io sudo tee /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry-1.docker.io\u0026#34; [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # K8s 镜像加速 sudo mkdir -p /etc/containerd/certs.d/registry.k8s.io sudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://registry.cn-hangzhou.aliyuncs.com/v2/google_containers\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] override_path = true EOF 重启containerd\n1 2 sudo systemctl restart containerd sudo systemctl status containerd 安装Kubernetes 安装k8s软件，这里会默认下载最新的kubernetes（阿里云镜像源上的），后面指定版本时需要根据自己的版本进行修改。这里也可以手动指定kubernetes版本。\n1 sudo apt install kubelet kubeadm kubectl 此时k8s没有启动成功是正常的，因为kubelet服务成功启动的先决条件，需要kubelet的配置文件，所在目录/var/lib/kubelet还没有建立。\nk8s配置单机节点 查看k8s版本\n1 kubeadm config images list 将kubernetes的控制面的几个镜像拉到本地，为了保证镜像和安装的k8s软件版本严格一致，这里的镜像拉取时，显性指定版本。\n1 sudo kubeadm config images pull --kubernetes-version v1.28.10 --image-repository registry.aliyuncs.com/google_containers kubernetes初始化\n1 sudo kubeadm init --control-plane-endpoint=192.168.146.111 --kubernetes-version v1.28.10 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers 初始化成功如下图 克隆虚拟机作为从节点 关闭虚拟机，克隆虚拟机作为从节点。 右键虚拟机，克隆虚拟机。选择当前状态，创建完整克隆，完成克隆。 克隆完成后修改从节点的hosts、hostname和ip。 为了便于管理，修改主节点的hosts为k8s1,从节点分别取名k8s2和k8s3。对应ip指定为192.168.146.112和192.168.146.113。（这里的ip和host要根据自己的情况进行修改，主节点的名字也可以通过修改hotsname来改变） 创建hosts映射。 配置Kubernetes集群 在主节点上跟着初始化成功后的提示继续后续工作\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看集群状态\n1 2 kubectl cluster-info kubectl get nodes 在k8s1上创建永久token\n1 kubeadm token create --print-join-command 如果在克隆虚拟机作为从节点之前就已经跟着k8s提示完成了上面几步，那么在从节点上执行提示的join命令，会遇见报错。 原因：拷贝虚拟机时已完成k8s集群配置文件创建，应该在初始化后，配置文件创建前进行克隆。 解决办法：在虚拟机重置节点后加入集群。这个重置节点的命令也可以在后续k8s出现问题时恢复默认设置重做时使用。\n1 2 sudo rm -rf /etc/kubernetes/kubelet.conf /etc/kubernetes/pki/ca.crt sudo kubeadm reset 重新加入集群（join命令为主节点创建永久token时回显的命令） 在k8s3上进行相同步骤。 在k8s1上查看集群节点，集群搭建成功。 细心的读者可能发现了我的1号机之前交VirtualClass，但是现在却叫k8s1，这是因为我在1号机初始化之前没有修改一号机的名字，导致集群建立后节点名字如下，但是这个看着不太舒服，我就把三台机都使用上面的重置命令重置后再重做了一遍上述步骤。 配置Kubernetes网络插件Calico 1 2 curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O kubectl apply -f calico.yaml 成功截图如下（我这里因为网络问题没有成功，姑且偷一张图），如果失败可以使用另外一种手动方法如下（我的办法）。 手动解决办法： 打开链接(可能需要借助某些上网工具) https://projectcalico.docs.tigera.io/manifests/calico.yaml 全选复制，粘贴到~/calico.yaml 重新执行\n1 kubectl apply -f calico.yaml 成功截图，这里要看见所有的pod状态都为Running才是成功，如果是ContainerCreating则表明正在制作容器，需要稍等一会（可能会很慢，但也不会超过十分钟）。如果过了很久还没Running，可以通过kubectl describe pods \u0026lt;pod-name\u0026gt; -n kube-system 查看pod进度。 -n的意思是指定命名空间（namespace），默认的pod命名空间是default，所以使用kubectl get pods会得到defalut下的pods。\n1 2 kubectl get pods -n kube-system kubectl get nodes k8s命名空间查看，更多命令及参数可以通过kubectl --help查看\n1 kubectl get namespace 如果应用calico.yaml后查看状态如下，pod的镜像一直拉取不下来(ImagePullBackOff)。查看pod情况，发现镜像用的是docker.io的，calico镜像拉不下来。解决办法就是自己手动拉取（用docker/nerdctl 拉下来，保存成tar包，再应用到k8s上）。没装docker建议用nerdctl,nerdctl是 containerd的官方CLI工具，功能和docker命令几乎一样，但它是为containerd设计的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 rust@k8s1:~$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-5b9b456c66-j54xc 0/1 Pending 0 4m43s calico-node-4h4kb 0/1 Init:ImagePullBackOff 0 2m56s calico-node-l7mrg 0/1 Init:ImagePullBackOff 0 2m56s calico-node-psvnr 0/1 Init:ImagePullBackOff 0 2m56s coredns-6d58d46f65-4gfzx 0/1 Pending 0 129m coredns-6d58d46f65-hsl2c 0/1 Pending 0 129m etcd-k8s1 1/1 Running 3 (120m ago) 129m kube-apiserver-k8s1 1/1 Running 3 (120m ago) 129m kube-controller-manager-k8s1 1/1 Running 3 (120m ago) 129m kube-proxy-hd4sr 1/1 Running 2 (120m ago) 129m kube-proxy-hpcdx 1/1 Running 0 108m kube-proxy-trwf6 1/1 Running 0 108m kube-scheduler-k8s1 1/1 Running 3 (120m ago) 129m rust@k8s1:~$ kubectl describe pod -n kube-system calico-node-4h4kb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 拉取镜像 docker pull calico/cni:v3.25.0 docker pull calico/node:v3.25.0 docker pull calico/kube-controllers:v3.25.0 docker pull calico/pod2daemon-flexvol:v3.25.0 # 打包成 tar docker save \\ calico/cni:v3.25.0 \\ calico/node:v3.25.0 \\ calico/kube-controllers:v3.25.0 \\ calico/pod2daemon-flexvol:v3.25.0 \\ -o calico-v3.25.0-images.tar # 在每个k8s节点上应用calico镜像，ctr是containerd原生命令 sudo ctr -n k8s.io images import calico-v3.25.0-images.tar # 删除旧pod，触发重建 kubectl delete pod -n kube-system -l k8s-app=calico-node kubectl delete pod -n kube-system -l app=calico-kube-controllers ","date":"2024-09-07T12:35:29+08:00","permalink":"https://rusthx.github.io/p/kubernetesk8s%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/","title":"Kubernetes（K8s）集群安装"},{"content":"我的docker是安装在Ubuntu22.04虚拟机上的，不同的操作系统细微差别，请自行必应搜索。\nDocker安装 更新apt数据源 1 sudo apt-get update 下载依赖 1 sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common -y 添加Docker的官方GPG密钥 从这里开始可以选择docker官方的密钥和仓库，也可以选择国内镜像（阿里云）\n1 2 3 4 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg # 阿里云密钥 curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 设置稳定仓库 1 2 3 4 sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; # 阿里云仓库 sudo add-apt-repository \u0026#34;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\u0026#34; 安装docker 1 sudo apt-get install docker-ce docker-ce-cli containerd.io 添加docker用户组，将登陆用户加入到docker用户组中，更新用户组 1 2 3 sudo groupadd docker sudo gpasswd -a $USER docker newgrp docker docker测试 1 sudo docker run hello-world 配置加速镜像和Cgroup，后面再docker使用过程中遇见了拉取镜像缓慢的问题，于是我又多加了几个镜像源。不过事后想想，可能是那几天校园网太卡的问题 1 sudo gedit /etc/docker/daemon.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://kfwkfulq.mirror.aliyuncs.com\u0026#34; ], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34; } Docker简单使用 拉取nginx和tomcat的镜像 1 2 sudo docker pull nginx sudo docker pull tomee 1 sudo docker network create testnet 启动 两个tomcat 创建两个目录，分别挂载到tomcat的跟目录上，内容可以调整，主要区分是哪个服务上的文件。\n1 2 3 4 5 6 7 cd ~ sudo mkdir tomcatone echo \u0026#34;tomcat onet\u0026#34; \u0026gt; index.html sudo mkdir tomcattwo sudo cp index.html tomcatone/ echo \u0026#34;tomcat two\u0026#34; \u0026gt; index.html sudo cp index.html tomcattwo 运行容器\n1 sudo docker run -id --name tomcatone -p 8088:8080 --network testnet --network-alias tomcatone -v $PWD/tomcatone:/usr/local/tomee/webapps/a tomee 查看网页显示如下 启动nginx，命令使用两次 1 sudo docker run -it -d -p 8080:80 --name web -v ~/nginx:/etc/nginx/conf.d -v ~/nginxweb:/usr/share/nginx/html --network testnet --network-alias nginxs nginx 刷新后网页显示如下 ","date":"2024-09-07T12:27:13+08:00","permalink":"https://rusthx.github.io/p/docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"Docker安装与基本操作"},{"content":" 参考：https://www.bilibili.com/video/BV11A411L7CK?p=188\u0026amp;vd_source=2db7c64d895a2907954a5b8725db55d5\n终端打印大量日志影响结果查看可以看我首页的博客解决。 踩坑如下： 1.socket编程不会，写socket发送数据查了很多资料才写出来 2.Windows没有netcat命令，但是MACOS和Ubuntu有,所以理所当然的想到用虚拟机的端口来收集数据和输入数据，事实上这个想法确实没有问题，分别做的话是能正常实现的，但是这也为后续的错误埋下了大坑。想当然的把socket当成kafka用（producer和consumer），是我踩坑的一大原因。 3.被教程误导，socket发送数据到端口，但是不知道socket有服务器和客户端之分，发送数据和处理数据的都是客户端，导致发送端可以和nc -lk 结合使用，能正常监听到数据；接收端也能和nc -lk 结合使用，在监听的端口出输入数据可以正常计算；但是两者结合就没办法计算了\nWindows安装netcat 下载链接： https://nmap.org/download.html#windows 下载的是一个exe包，点击exe包一路next即可完成安装。\n端口同时接收数据和计算数据时使用命令监听会无法访问，即启动socket数据发送程序和SparkStreaming数据计算程序后无法监听。但是监听命令可以用来分别调试两个程序。\n注意：Windows的netcat命令与Ubuntu和MacOS都不一样。Windows的命令是 ncat -lk \u0026lt;Port\u0026gt;，参数的意思可以通过ncat -h查看。 SparkStreaming socket编程 题目：1）写一个应用程序利用套接字每隔2秒生成20条大学主页用户访问日志（可以自定义内容），数据形式如下：“系统时间戳，位置城市，用户ID+姓名，访问大学主页”。其中城市自定义 9个，用户ID 10个，大学主页 7个。 2）写第二个程序每隔2秒不断获取套接字产生的数据，并将词频统计结果打印出来。 导入依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-streaming_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package SparkStreaming1 import java.io.{BufferedWriter, IOException, OutputStreamWriter} import java.net.ServerSocket import scala.util.Random object task11 { def main(args: Array[String]): Unit = { // 定义城市和用户ID val cities = Seq(\u0026#34;杭州\u0026#34;, \u0026#34;南京\u0026#34;, \u0026#34;长沙\u0026#34;, \u0026#34;天津\u0026#34;, \u0026#34;北京\u0026#34;, \u0026#34;上海\u0026#34;, \u0026#34;成都\u0026#34;, \u0026#34;广州\u0026#34;, \u0026#34;深圳\u0026#34;) val userIds = Seq(\u0026#34;0:阿良良木历\u0026#34;, \u0026#34;1:忍野忍\u0026#34;, \u0026#34;2:战场原黑仪\u0026#34;, \u0026#34;3:羽川翼\u0026#34;, \u0026#34;4:八九寺真宵\u0026#34;, \u0026#34;5:神原骏河\u0026#34;, \u0026#34;6:千石抚子\u0026#34;, \u0026#34;7:阿良良木火怜\u0026#34;, \u0026#34;8:阿良良木月火\u0026#34;, \u0026#34;9:姬丝秀忒·雅赛劳拉莉昂·刃下心\u0026#34;) val universityUrls = Seq(\u0026#34;www.nju.edu.cn\u0026#34;, \u0026#34;www.ustc.edu.cn\u0026#34;, \u0026#34;www.zju.edu.cn\u0026#34;, \u0026#34;www.fudan.edu.cn\u0026#34;, \u0026#34;www.tsinghua.edu.cn\u0026#34;, \u0026#34;www.pku.edu.cn\u0026#34;, \u0026#34;www.scu.edu.cn\u0026#34;) try { // 创建一个 socket 连接 // val socket = new Socket(\u0026#34;hadoop3\u0026#34;, 9765) val socketServer = new ServerSocket(9765) val client = socketServer.accept() println(\u0026#34;连接！\u0026#34;) val out = new BufferedWriter(new OutputStreamWriter(client.getOutputStream)) //val in = new BufferedReader(new InputStreamReader(client.getInputStream)) while (true){ for (_ \u0026lt;- 1 to 20) { // 发送多条数据 val currentTime = System.currentTimeMillis() val city = cities(Random.nextInt(cities.length)) val userId = userIds(Random.nextInt(userIds.length)) val universityUrl = universityUrls(Random.nextInt(universityUrls.length)) val logLine = s\u0026#34;$currentTime $city $userId $universityUrl\u0026#34; out.write(logLine + \u0026#34;\\n\u0026#34;) // 添加换行符以区分消息 out.flush() // 确保数据被发送出去 println(logLine) } Thread.sleep(2000) // 休眠2秒，模拟连续发送 } // 关闭 socket out.close() // socket.close() client.close() } catch { case e: IOException =\u0026gt; e.printStackTrace() } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package SparkStreaming1 import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.apache.spark.streaming.{Seconds, StreamingContext} object task12{ def main(args: Array[String]): Unit = { val sparkConf: SparkConf = new SparkConf().setMaster(\u0026#34;local[*]\u0026#34;).setAppName(\u0026#34;job7task12\u0026#34;) val spark = SparkSession.builder().config(sparkConf).getOrCreate() val ssc = new StreamingContext(spark.sparkContext, Seconds(2)) // 从套接字获取数据流 val lines = ssc.socketTextStream(\u0026#34;localhost\u0026#34;, 9765) lines.map(_.split(\u0026#34; \u0026#34;)(2)) .map((_, 1)) .reduceByKey(_ + _) .print() ssc.start() ssc.awaitTermination() } } 启动时需要先启动数据计算程序，再启动数据发送程序。 成功运行截图： ","date":"2024-09-07T11:55:09+08:00","permalink":"https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/","title":"SparkStreaming使用socket"},{"content":"连接MySQL 参考链接：https://www.cnblogs.com/Jaryer/p/13671449.html\nmaven添加依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.33\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 连接数据库 1 2 3 4 5 val host = \u0026#34;localhost\u0026#34; val port = 3306 val database = \u0026#34;sparktest\u0026#34; val jdbcUrl = s\u0026#34;jdbc:mysql://$host:$port/$database?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, \u0026#34;root\u0026#34;, \u0026#34;123456\u0026#34;) 执行查询 SQL语句在执行时有三种：executeQuery,executeUpdate,execute。具体细节可查看此节开头的参考资料。\n1 2 3 4 5 6 7 8 9 val statement: Statement = mysqlConn.createStatement() //插入数据 statement.executeUpdate(\u0026#34;insert into employee values (3,\u0026#39;Mary\u0026#39;,\u0026#39;F\u0026#39;,26)\u0026#34;) statement.executeUpdate(\u0026#34;insert into employee values (4,\u0026#39;Tom\u0026#39;,\u0026#39;M\u0026#39;,23)\u0026#34;) val result: ResultSet = statement.executeQuery(\u0026#34;select max(age) as max_age,avg(age) as avg_age from employee\u0026#34;) while (result.next()) { println(result.getString(\u0026#34;max_age\u0026#34;),result.getString(\u0026#34;avg_age\u0026#34;)) } 完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package sparkjob5 import java.sql.{Connection, DriverManager, ResultSet, Statement} object task3 { def main(args: Array[String]): Unit = { //连接mysql val host = \u0026#34;localhost\u0026#34; val port = 3306 val database = \u0026#34;sparktest\u0026#34; val jdbcUrl = s\u0026#34;jdbc:mysql://$host:$port/$database?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, \u0026#34;root\u0026#34;, \u0026#34;123456\u0026#34;) val statement: Statement = mysqlConn.createStatement() //插入数据 statement.executeUpdate(\u0026#34;insert into employee values (3,\u0026#39;Mary\u0026#39;,\u0026#39;F\u0026#39;,26)\u0026#34;) statement.executeUpdate(\u0026#34;insert into employee values (4,\u0026#39;Tom\u0026#39;,\u0026#39;M\u0026#39;,23)\u0026#34;) val result: ResultSet = statement.executeQuery(\u0026#34;select max(age) as max_age,avg(age) as avg_age from employee\u0026#34;) while (result.next()) { println(result.getString(\u0026#34;max_age\u0026#34;),result.getString(\u0026#34;avg_age\u0026#34;)) } result.close() statement.close() } } 连接Hive 参考链接：https://www.jianshu.com/p/27a798013990\n连接Hive前需要开启Hive的metastore和hiverserver2。开启命令如下。\n开启Hadoop集群 1 start-all.sh 开启Hive,第二三行的启动命令需要分别开一个终端启动，输出的日志在/usr/local/hive/logs。 1 2 3 cd /usr/local/hive hive --service metastore \u0026gt;logs/metastore.log 2\u0026gt;\u0026amp;1 hive --service hiveserver2 \u0026gt;logs/hiveServer2.log 2\u0026gt;\u0026amp;1 添加依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-hive_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 完整依赖如下（包含了Scala连接MySQL的依赖）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;Spark\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;sparkCore\u0026lt;/artifactId\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-core_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-sql_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.33\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-hive_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hive\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hive-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;17\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;17\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!-- 该插件用于将 Scala 代码编译成 class 文件 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.alchim31.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.2\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;!-- 声明绑定到 maven 的 compile 阶段 --\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;descriptorRefs\u0026gt; \u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt; \u0026lt;/descriptorRefs\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;make-assembly\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 修改配置文件hive-site.xml 在resource下新建一个hive-site.xml，填入下列内容。注意：要把hadoop1修改成自己的Hadoop集群主节点名字或者ip。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 添加文件调用 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.exec.scratchdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/user/hive/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/user/hive/warehouse\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.querylog.location\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/user/hive/log\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定存储元数据要连接的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://hadoop1:9083\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的URL --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true\u0026amp;amp;characterEncodeing=UTF-8\u0026amp;amp;allowPublicKeyRetrieval=true\u0026amp;amp;useSSL=false\u0026amp;amp;serverTimezone=GMT\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的Driver--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的username--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的password --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的host --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.bind.host\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的端口号 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.active.passive.ha.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Scala代码 在spark.sql()里写上正常的SQL语句即可完成查询。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package sparkjob5 import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession object task4 { val driverName = \u0026#34;org.apache.hive.jdbc.HiveDriver\u0026#34; try { Class.forName(driverName) } catch { case e: ClassNotFoundException =\u0026gt; println(\u0026#34;Missing Class\u0026#34;, e) } def main(args: Array[String]): Unit = { val conf = new SparkConf().setMaster(\u0026#34;local[3]\u0026#34;).setAppName(\u0026#34;hive\u0026#34;) val spark = SparkSession.builder().config(conf).enableHiveSupport().getOrCreate() spark.sql(\u0026#34;use spark_test\u0026#34;) spark.sql(\u0026#34;show tables\u0026#34;).show() spark.close() } } 补充：将查询结果保存到hdfs上，如果想保存到本地，则可以将save的路径改成本地路径。\n1 2 3 4 5 6 7 val dataFrame = spark.sql(\u0026#34;select uid,keyword from sougou_records where keyword like \u0026#39;%仙剑奇侠传%\u0026#39;\u0026#34;) dataFrame.write .format(\u0026#34;csv\u0026#34;) .option(\u0026#34;header\u0026#34;, \u0026#34;false\u0026#34;) .option(\u0026#34;sep\u0026#34;, \u0026#34;\\t\u0026#34;) .save(\u0026#34;hdfs://hadoop1:8020/xianJianTest\u0026#34;) 如果想以表格保存到MySQL或者Hive,可以使用saveAsTable()。\n1 2 3 4 5 6 7 8 9 val host = \u0026#34;localhost\u0026#34; val port = 3306 val database = \u0026#34;sparktest\u0026#34; val jdbcUrl = s\u0026#34;jdbc:mysql://$host:$port/$database?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; val connectionProperties = new java.util.Properties() connectionProperties.put(\u0026#34;user\u0026#34;, \u0026#34;root\u0026#34;) connectionProperties.put(\u0026#34;password\u0026#34;, \u0026#34;123456\u0026#34;) df.write.mode(SaveMode.Overwrite).jdbc(jdbcUrl,\u0026#34;company\u0026#34;,connectionProperties) 1 df.write.mode(SaveMode.Overwrite).saveAsTable(\u0026#34;spark_test.company\u0026#34;) ","date":"2024-09-07T11:47:55+08:00","permalink":"https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/","title":"Scala连接MySQL和Hive"},{"content":"Scala中的部分函数和RDD中的部分算子名字一样，功能一样，用起来也差不多。但是为什么一个叫函数，一个却要叫算子，函数和算子的区别在哪，这让我有些好奇。于是查看了源码，对函数和算子进行了比较。下面以map为例。\nScala中的map函数 Scala中的map通常定义在集合类中，例如Map、List、Seq、Set。作用是对该可迭代集合的所有元素应用一个函数，从而建立一个新的可迭代集合。\nBuilds a new iterable collection by applying a function to all elements of this iterable collection.\n注意：List属于scala.collection.immutable的子类，而Map、Seq、Set属于scala.collection的子类 map函数最底层的源码应该是scala.collection里的如下代码 具体实现以List中的map举例 源码为\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 final override def map[B](f: A =\u0026gt; B): List[B] = { if (this eq Nil) Nil else { val h = new ::[B](f(head), Nil) var t: ::[B] = h var rest = tail while (rest ne Nil) { val nx = new ::(f(rest.head), Nil) t.next = nx t = nx rest = rest.tail } releaseFence() h } } List中的map重写了其父类collection.IterableOnce的map函数，定义了一个匿名函数(f:A=\u0026gt;B),对List中的每个元素A处理后输出B类型的List。比如List[String]经过map处理后可以变成List[Interger] 其他collection的代码可以自行查看，也可以查看Scala的官方文档\nhttps://www.scala-lang.org/api/current/scala/collection\nSpark中的map算子 Spark中的算子分为转换算子（Transformations (return a new RDD)）和行动算子（Actions (launch a job to return a value to the user program)）， 转换算子根据数据处理方式的不同将算子整体上分为 Value 类型、双 Value 类型和 Key-Value 类型 。具体不再细讲，可以自行查询。 RDD中的map代码如下：\n1 2 3 4 5 6 7 /** * Return a new RDD by applying a function to all elements of this RDD. */ def map[U: ClassTag](f: T =\u0026gt; U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u0026gt; iter.map(cleanF)) } 可以看出，这里的map首先创建了一个MapPartitionsRDD并生成可迭代对象iter,然后调用了Scala的map函数处理iter。\n结论 Scala里的map函数首先定义在scala.collection里，然后子类（List、Set）重写父类的函数。因为Scala里的类型是隐式的，并且查看源代码在一个子类里也只发现了一个map函数，所以Scala里的map并没有重载，而是通过定义父类，子类重写父类函数的方法实现对不同数据结构的操作。 Spark里的map是对RDD进行操作的算子，实际使用了可迭代对象来调用Scala中的map函数。算子本身的定义就是对RDD操作的函数，所以算子应该也可以被称为是函数，但是为了区分Scala中的函数，所以使用了不同的名字。\n补充 Spark中的算子并非全都有同名函数，原因可以从RDD的原理上分析。 行动算子需要进行shuffle操作，在shuffle时需要按键分区，对每个分区进行操作后输出。Scala中并没有Shuffle操作，所以行动算子没有同名函数。 而转换算子是生成RDD或者将RDD转换成另外的RDD，Scala本身也有将collection转换为collection的函数，并且转换算子本身就调用了Scala的函数，所以有同名的也正常。\n","date":"2024-09-07T11:46:08+08:00","permalink":"https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"Scala中函数与Spark的算子的区别"},{"content":"Spark程序在启动后会在控制台打印大量日志，找了很多教程也没有解决，本来一直可以忍受的。但是学SparkStreaming时实在受不了了，日志已经严重影响到我查看计算结果。遂痛下决心，解决这个一直困扰我的问题。如果使用方法1没有解决的可以直接去看第三步，第二步是解决日志依赖冲突的问题的\n常规解决办法 在resource下建立一个log4j.properties，填入下列内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 log4j.rootCategory=ERROR, console log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.target=System.err log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{yy/MM/ddHH:mm:ss} %p %c{1}: %m%n # Set the default spark-shell log level to ERROR. When running the spark-shell,the # log level for this class is used to overwrite the root logger\u0026#39;s log level, so that # the user can have different defaults for the shell and regular Spark apps. log4j.logger.org.apache.spark.repl.Main=ERROR # Settings to quiet third party logs that are too verbose log4j.logger.org.spark_project.jetty=ERROR log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR log4j.logger.org.apache.parquet=ERROR log4j.logger.parquet=ERROR # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR 查看日志寻求解决办法 常规解决办法没有正常解决，遂查看日志寻求解决办法，查看日志可以明显看出有日志依赖冲突\n1 2 3 4 5 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/D:/maven-3.8.6/respository/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/D:/maven-3.8.6/respository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 怀疑是日志冲突的问题（事实证明不是），分析日志依赖 在SparkCore下面发现了slf4j的依赖，在spark-core的dependency里加入下列内容以屏蔽日志包。\n1 2 3 4 5 6 \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-reload4j\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; 还是日志冲突，把所有dependency下面都加了排除日志依赖的标签 还是不起作用，观察日志可知冲突是因为reload4hj，干脆把仓库里的reload4j删掉，成功解决日志冲突 但是但是，日志依赖冲突的问题解决了，大量info日志的问题却还在\n最终解决办法 在resource下新建一个log4j2.xml文件，填入下面内容即可解决。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;error\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34; /\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; ","date":"2024-09-07T11:39:08+08:00","permalink":"https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","title":"Spark程序大量Info日志问题解决"},{"content":"Spark启动方式有：local模式、standalone模式、Yarn模式、K8S和Mesos模式，本教程只涉及前三种模式，另外两种可以自行查找资料。\nLocal模式 1.下载Spark https://archive.apache.org/dist/spark/ 由于我的Hadoop版本是3.1.3，所以下载的Spark版本也是Spark3,这里下的是Spark3.3.1，只要是Spark3都可以和Hadoop3兼容。\n2.解压Spark压缩包 解压Spark的压缩包，移动到/usr/local/下，修改文件夹的名字为spark\n1 2 3 4 cd ~/Downloads sudo tar -zxvf spark-3.3.1-bin-hadoop3.tgz -C /usr/local/ cd /usr/local/ mv spark-3.3.1-bin-hadoop3.2 spark 3.Local模式启动Spark 1 bin/spark-shell 启动成功后，可以输入网址主机名：4040进行 Web UI 监控页面访问 Standalone模式 1.进入spark文件夹下的conf目录，修改workers.template文件名为workers 1 2 cd conf/ mv workers.template workers 2.修改workers文件，添加worker节点 1 vim workers 3.修改spark-env.sh.template文件名为spark-env.sh 4.修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点 Java默认安装路径如下，手动安装的Java可以指定自己的Java路径 5.分发Spark 6.Standalone模式启动Spark集群 1 2 3 cd spark/ sbin/start-all.sh xcall jps 7.查看进程 Spark正常启动输入网址主机名:8080进行监控 8.提交应用测试Spark 1 2 3 4 bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://hadoop1:7077 ./examples/jars/spark-examples_2.12-3.3.1.jar 10 注意：\u0026ndash;master后面指定的主机名要改成自己的主机名（hadoop1改成自己的主机名） 指定的jar包要指定为自己的jar包，不同版本的示例jar包名字不同。 10是指当前应用的任务数量 提交任务时会有一个SparkSubmit进程，任务结束后进程停止 Yarn 模式 1.修改Hadoop配置文件 修改/usr/local/hadoop/etc/hadoop/yarn-site.xml, 并分发\n1 vim /usr/local/hadoop/etc/hadoop/yarn-site.xml 1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认 是true --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.pmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认 是true --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.vmem-check-enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 分发修改后的配置文件\n1 xsync /usr/local/hadoop/etc/hadoop/yarn-site.xml 2. 修改conf/spark-env.sh，添加 JAVA_HOME 和 YARN_CONF_DIR 配置 1 vim conf/spark-env.sh 3.分发更改后的Spark-env.sh 1 xsync conf/spark-env.sh 4.Yarn模式提交任务测试 Client模式 1 2 3 4 bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client ./examples/jars/spark-examples_2.12-3.3.1.jar 10 Cluster模式 1 2 3 4 bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.3.1.jar 10 5.在hadoop1:8088查看，程序运行成功 补充：提交参数说明 参数 解释 可选值举例 \u0026ndash;class Spark 程序中包含主函数的类 \u0026ndash;master Spark 程序运行的模式(环境) 模式：local[*]、spark://hadoop1:7077、Yarn \u0026ndash;executor-memory 1G 指定每个executor 可用内存为1G 符合集群内存配置即可，具体情况具体分析。 \u0026ndash;total-executor-cores 2 指定所有executor使用的cpu核数析。为2个 \u0026ndash;executor-cores 指定每个executor使用的cpu核数 application-jar 打包好的应用 jar，包含依赖。这个URL 在集群中全局可见。比如 hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的 path 都包含同样的 jar application-arguments 传给 main()方法的参数 ","date":"2024-09-07T11:22:39+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/","title":"Ubuntu22.04配置Spark3.3.1集群"},{"content":"前置准备：配置Hive的MySQL连接用户 MySQL的配置可参考我的教程 MySQL安装教程\n创建Hive元数据库 1 create database metastore; 创建用户hive，设置密码为123456 1 2 3 create user \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;123456\u0026#39;; grant all privileges on metastore.* to \u0026#39;hive\u0026#39;@\u0026#39;%\u0026#39; with grant option; flush privileges; 安装Hive 参考资料：B站尚硅谷 062.Hive的安装部署_哔哩哔哩_bilibili\n下载Hive安装包 注意：apache原装的Hive只支持Spark2.3.0，不支持Spark3.3.0，需要重新编译Hive的源码，尚硅谷已经编译好了，这里我就直接使用了\n修改配置文件（cd 到hive下的conf文件夹，这里我已经将Hive安装包改名为hive并移动到/usr/local/下） 1 2 sudo mv hive-default.xml.template hive-default.xml sudo vim hive-site.xml 将以下内容写入文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- jdbc连接的URL --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true\u0026amp;amp;characterEncodeing=UTF-8\u0026amp;amp;allowPublicKeyRetrieval=true\u0026amp;amp;useSSL=false\u0026amp;amp;serverTimezone=GMT\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的Driver--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的username--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- jdbc连接的password --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- Hive默认在HDFS的工作目录 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/user/hive/warehouse\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定存储元数据要连接的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://hadoop1:9083\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的host --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.bind.host\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hadoop1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hiveserver2连接的端口号 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.thrift.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.active.passive.ha.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.cli.print.header\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.cli.print.current.db\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 配置环境变量 1 sudo vim ~/.bashrc 在下面添加\n1 2 export HIVE_HOME=/usr/local/hive export PATH=$PATH:$HIVE_HOME/bin 启动Hive 启动Hadoop 1 start-all.sh 初始化Hive 1 2 cd /usr/local/hive ./bin/schematool -dbType mysql -initSchema 正常初始化会日志刷屏并出现大片空白，然后最后一行出现succeed或者complete的字样 如果没有正常初始化就复制最下面几行中的报错信息，粘贴到必应进行查找\n启动Hive 启动Hive前需要先启动Hive的元数据库metastore和hiveserver2 注意：这里的metastore和hiveserver2每个都要单独开启一个终端，开启一个后再开一个新的终端进行命令 日志被重定向到了logs文件夹下，需要查看日志可以在这个文件夹下查看\n1 2 3 cd /usr/local/hive/ hive --service metastore \u0026gt;logs/metastore.log 2\u0026gt;\u0026amp;1 hive --service hiveserver2 \u0026gt;logs/hiveServer2.log 2\u0026gt;\u0026amp;1 1 bin/hive 正常启动会出现一个交互界面如下：\n1 hive(default)\u0026gt; 解决Hive shell中打印大量日志的问题 当在Hive的命令行中查询时出现大量日志时，可以在conf下新建日志配置文件如下\n1 2 cd /usr/local/hive/conf/ vim log4j.properties 粘贴如下内容\n1 2 3 4 log4j.rootLogger=WARN, CA log4j.appender.CA=org.apache.log4j.ConsoleAppender log4j.appender.CA.layout=org.apache.log4j.PatternLayout log4j.appender.CA.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n Hive on Spark配置 在官网下载纯净版Spark（不带Hadoop依赖的） http://spark.apache.org/downloads.html\n解压Spark 1 2 tar -zxvf spark-3.3.1-bin-without-hadoop.tgz -C /usr/local/ mv /usr/local/spark-3.3.1-bin-without-hadoop /usr/local/spark 修改spark-env.sh配置文件 1 2 mv /usr/local/spark/conf/spark-env.sh.template /usr/local/spark/conf/spark-env.sh vim /usr/local/spark/conf/spark-env.sh 增添下面内容\n1 export SPARK_DIST_CLASSPATH=$(hadoop classpath) 配置Spark环境变量 1 sudo vim ~/.bashrc 添加下列内容\n1 2 export SPARK_HOME=/usr/local/spark export PATH=$PATH:$SPARK_HOME/bin 在Hive中创建spark配置文件 1 vim /usr/local/hive/conf/spark-defaults.conf 添加如下内容\n1 2 3 4 5 spark.master yarn spark.eventLog.enabled true spark.eventLog.dir hdfs://hadoop1:8020/spark-history spark.executor.memory 1g spark.driver.memory\t1g 在HDFS中创建如下路径，用于存储历史日志\n1 hadoop fs -mkdir /spark-history 向HDFS上传Spark纯净版jar包 说明1：采用Spark纯净版jar包，不包含hadoop和hive相关依赖，能避免依赖冲突。 说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到\n1 2 hadoop fs -mkdir /spark-jars hadoop fs -put /usr/local/spark/jars/* /spark-jars 修改hive-site.xml文件 1 vim /usr/local/hive/conf/hive-site.xml 添加如下内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;spark.yarn.jars\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hadoop1:8020/spark-jars/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--Hive执行引擎--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.execution.engine\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;spark\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--连接超时时间--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.spark.client.connect.timeout\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;30000ms\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Hive on Spark测试 启动hive客户端 1 2 cd /usr/local/hive/ bin/hive 创建一张测试表 1 hive (default)\u0026gt; create table student(id int, name string); 通过insert测试效果 1 hive (default)\u0026gt; insert into table student values(1,\u0026#39;abc\u0026#39;); 结果如下则配置成功 Yarn环境配置 增加ApplicationMaster资源比例 容量调度器对每个资源队列中同时运行的Application Master占用的资源进行了限制，该限制通过yarn.scheduler.capacity.maximum-am-resource-percent参数实现，其默认值是0.1，表示每个资源队列上Application Master最多可使用的资源为该队列总资源的10%，目的是防止大部分资源都被Application Master占用，而导致Map/Reduce Task无法执行。 生产环境该参数可使用默认值。但学习环境，集群资源总数很少，如果只分配10%的资源给Application Master，则可能出现，同一时刻只能运行一个Job的情况，因为一个Application Master使用的资源就可能已经达到10%的上限了。故此处可将该值适当调大。\n在hadoop1的/usr/local/hadoop/etc/hadoop/capacity-scheduler.xml文件中修改如下参数值 1 vim capacity-scheduler.xml 1 2 3 4 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.scheduler.capacity.maximum-am-resource-percent\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;0.8\u0026lt;/value\u0026gt; \u0026lt;/property 分发capacity-scheduler.xml配置文件 1 xsync capacity-scheduler.xml 重启集群 1 2 stop-all.sh start-all.sh ","date":"2024-09-07T11:15:48+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/","title":"Ubuntu22.04配置Hive及Hive on Spark"},{"content":"安装依赖 使用以下命令安装依赖\n1 sudo apt-get install build-essential libssl-dev libffi-dev python3-dev python3-pip libsasl2-dev libldap2-dev default-libmysqlclient-dev 配置Superset元数据库 本教程使用MySQL数据库作为Superset的数据库（Superset支持MySQL和PostgreSQL)，安装完成后需进行以下配置。\n在MySQL中创建Superset元数据库 1 CREATE DATABASE superset DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; 创建Superset用户，用户名为superset,密码为superset。superset用户拥有所有数据库的全部权限。 1 2 3 create user superset@\u0026#39;%\u0026#39; identified WITH mysql_native_password BY \u0026#39;superset\u0026#39;; grant all privileges on *.* to superset@\u0026#39;%\u0026#39; with grant option; flush privileges; 为Superset创建Python虚拟环境 更新pip 1 sudo pip install --upgrade pip 下载必要的虚拟环境包 1 sudo pip install virtualenv -i https://pypi.tuna.tsinghua.edu.cn/simple 创建虚拟环境 1 virtualenv superset 激活虚拟环境 1 source superset/bin/activate 安装Superset 1 pip install apache-superset -i https://pypi.tuna.tsinghua.edu.cn/simple 安装其他Python依赖 1 pip install gunicorn pymysql mysqlclient -i https://pypi.tuna.tsinghua.edu.cn/simple 说明：gunicorn是一个Python Web Server，可以和java中的TomCat类比\n配置Superset 在~/superset/bin目录下创建superset配置文件superset_config.py，详细配置可参考官方文档（https://superset.apache.org/docs/installation/configuring-superset/）或者GitHub（https://github.com/apache/superset/）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 # Superset specific config ROW_LIMIT = 5000 #SUPERSET_WEBSERVER_PORT = 8088 # Flask App Builder configuration # Your App secret key will be used for securely signing the session cookie # and encrypting sensitive information on the database # Make sure you are changing this key for your deployment with a strong key. # You can generate a strong key using `openssl rand -base64 42` \u0026#39;\u0026#39;\u0026#39; 使用命令“openssl rand -base64 42”创建SECRET_KEY填写到下面\u0026#39;\u0026#39;\u0026#39; SECRET_KEY = \u0026#39;\u0026#39; # The SQLAlchemy connection string to your database backend # This connection defines the path to the database that stores your # superset metadata (slices, connections, tables, dashboards, ...). # Note that the connection information to connect to the datasources # you want to explore are managed directly in the web UI \u0026#39;\u0026#39;\u0026#39; 数据库连接，我是用的是MySQL数据库 链接字符串：mysql+pymysql://\u0026lt;数据库用户\u0026gt;:\u0026lt;密码\u0026gt;@\u0026lt;主机名/ip\u0026gt;/\u0026lt;数据库名\u0026gt;\u0026#39;\u0026#39;\u0026#39; SQLALCHEMY_DATABASE_URI = \u0026#39;mysql+pymysql://superset:superset@hadoop01:3306/superset?charset=utf8\u0026#39; ENABLE_CSRF_PROTECTION = True # Flask-WTF flag for CSRF WTF_CSRF_ENABLED = True WTF_CSRF_CHECK_DEFAULT = True # Add endpoints that need to be exempt from CSRF protection #WTF_CSRF_EXEMPT_LIST = [] # A CSRF token that expires in 1 year #WTF_CSRF_TIME_LIMIT = 60 * 60 * 24 * 365 #不填这个会出现登录界面输入正确的用户名和密码后登录无反应的现象 #但是关掉这个可能会降低安全性，可能是superset版本太新（3.0.0），旧版本貌似没有这个问题 TALISMAN_ENABLED=False # Set this API key to enable Mapbox visualizations MAPBOX_API_KEY = \u0026#39;\u0026#39; COMPRESS_REGISTER = False #默认中文 BABEL_DEFAULT_LOCALE = \u0026#34;zh\u0026#34; #superset支持的语言 LANGUAGES = { \u0026#34;en\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;us\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;English\u0026#34;}, \u0026#34;es\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;es\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Spanish\u0026#34;}, \u0026#34;it\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;it\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Italian\u0026#34;}, \u0026#34;fr\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;fr\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;French\u0026#34;}, \u0026#34;zh\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;cn\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Chinese\u0026#34;}, \u0026#34;ja\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;jp\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Japanese\u0026#34;}, \u0026#34;de\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;de\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;German\u0026#34;}, \u0026#34;pt\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;pt\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Portuguese\u0026#34;}, \u0026#34;pt_BR\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;br\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Brazilian Portuguese\u0026#34;}, \u0026#34;ru\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;ru\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Russian\u0026#34;}, \u0026#34;ko\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;kr\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Korean\u0026#34;}, \u0026#34;sk\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;sk\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Slovak\u0026#34;}, \u0026#34;sl\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;si\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Slovenian\u0026#34;}, \u0026#34;nl\u0026#34;: {\u0026#34;flag\u0026#34;: \u0026#34;nl\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Dutch\u0026#34;}, } SHOW_STACKTRACE = False DEBUG = False APP_NAME = \u0026#34;Superset\u0026#34; #不填这个会导致报错如下 #ModuleNotFoundError: No module named \u0026#39;MySQL_db\u0026#39; SQLALCHEMY_TRACK_MODIFICATIONS = False import logging LOG_LEVEL = \u0026#39;DEBUG\u0026#39; # 设置日志级别为DEBUG可以获得最详细的日志信息 LOG_FILE = \u0026#39;/home/rust/superset/superset_logfile.log\u0026#39; # 指定日志文件路径 logging.basicConfig( filename=LOG_FILE, level=LOG_LEVEL, format=\u0026#39;%(asctime)s %(levelname)s %(name)s %(threadName)s : %(message)s\u0026#39;, ) 初始化Superset 初始化数据库 1 2 (superset) rust@hadoop01:~$ export FLASK_APP=superset (superset) rust@hadoop01:~$ superset db upgrade 在元数据库中创建一个superset管理员用户 1 (superset) rust@hadoop01:~$ superset fab create-admin 初始化superset 1 (superset) rust@hadoop01:~$ superset init 启动Superset 1 (superset) rust@hadoop01:~$gunicorn --workers 5 --timeout 120 --bind hadoop01:8787 \u0026#34;superset.app:create_app()\u0026#34; --daemon 说明： \u0026ndash;workers：指定进程个数 \u0026ndash;timeout：worker进程超时时间，超时会自动重启 \u0026ndash;bind：绑定本机地址，即为Superset访问地址 \u0026ndash;daemon：后台运行\n登录Supperset 访问http://hadoop102:8787，并使用初始化Supperset中创建的管理员账户进行登录。\n停止Superset 停止gunicorn进程 1 (superset) rust@hadoop01:~$ ps -ef | awk \u0026#39;/superset/ \u0026amp;\u0026amp; !/awk/{print $2}\u0026#39; | xargs kill -9 退出superset虚拟环境 1 2 3 (superset) rust@hadoop01:~$ ps -ef |grep superst #查到进程号后杀掉进程 (superset) rust@hadoop01:~$ kill -9 pid Superset启停脚本 创建superset.sh文件 1 vim superset.sh 内容如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #!/bin/bash superset_status(){ result=`ps -ef | awk \u0026#39;/gunicorn/ \u0026amp;\u0026amp; !/awk/{print $2}\u0026#39; | wc -l` if [[ $result -eq 0 ]]; then return 0 else return 1 fi } superset_start(){ source ~/.bashrc superset_status \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [[ $? -eq 0 ]]; then source ~/superset/bin/activate ; gunicorn --workers 5 --timeout 120 --bind hadoop01:8787 --daemon \u0026#39;superset.app:create_app()\u0026#39; else echo \u0026#34;superset正在运行\u0026#34; fi } superset_stop(){ superset_status \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [[ $? -eq 0 ]]; then echo \u0026#34;superset未在运行\u0026#34; else ps -ef | awk \u0026#39;/gunicorn/ \u0026amp;\u0026amp; !/awk/{print $2}\u0026#39; | xargs kill -9 fi } case $1 in start ) echo \u0026#34;启动Superset\u0026#34; superset_start ;; stop ) echo \u0026#34;停止Superset\u0026#34; superset_stop ;; restart ) echo \u0026#34;重启Superset\u0026#34; superset_stop superset_start ;; status ) superset_status \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [[ $? -eq 0 ]]; then echo \u0026#34;superset未在运行\u0026#34; else echo \u0026#34;superset正在运行\u0026#34; fi esac 加执行权限 1 sudo chmod +x superset.sh 测试 1 2 3 4 #启动superset superset.sh start #停止superset superset.sh stop ","date":"2024-09-07T11:09:06+08:00","permalink":"https://rusthx.github.io/p/ubuntu-22.04%E9%83%A8%E7%BD%B2apache-superset/","title":"Ubuntu 22.04部署Apache Superset"},{"content":"更新软件包 1 sudo apt-get update 下载MySQL 1 sudo apt-get install mysql-server 登入MySQL MySQL安装完成后会有默认用户和密码，通过默认的用户和密码登入MySQL后可以新建用户并对该用户赋权\n查看默认用户和密码的命令 1 sudo cat /etc/mysql/debian.cnf 使用默认用户和密码登入数据库 mysql -u用户名 -p 输入密码 用户名和密码分别为上图中的user 和password\n1 mysql -udebian-sys-maint -p 新建用户 设置密码 赋权 设置root用户的密码（我的密码设置为123456，根据自己的需求修改命令） 1 2 3 4 use mysql; update user set authentication_string=\u0026#39;\u0026#39; where user=\u0026#39;root\u0026#39;; alter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified with mysql_native_password by \u0026#39;123456\u0026#39;; quit; 使用root用户登入数据库，并新建用户和赋权 下图为查询用户密码（加密过的）的命令 下列命令的意思是： 创建用户rust 并设置rust用户可以访问的位置为%（本地访问和远程访问，仅本地访问为localhost） 复制所有数据库的所有权限给rust用户 刷新权限\n1 2 3 CREATE USER \u0026#39;rust\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;123456\u0026#39;; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;rust\u0026#39;@\u0026#39;%\u0026#39; WITH GRANT OPTION; flush privileges; 设置MySQL数据库允许远程访问 默认情况下，MySQL服务器只允许本地连接。\n编辑MySQL配置文件（/etc/mysql/mysql.conf.d/mysqld.cnf）并注释掉以下行（在 bind-address 行前面添加#）： 1 sudo gedit /etc/mysql/mysql.conf.d/mysqld.cnf 保存文件并重启MySQL服务器 1 sudo systemctl restart mysql MySQL执行顺序 参考资料：https://blog.csdn.net/Elsa15/article/details/108544943\n1 2 3 4 5 6 7 8 9 (9) SELECT (10)DISTINCT column,(6) AGG_FUNC(column or expression),... (1）FROM left_table (3）J0IN right_table (2） ON tablename.column = other_tablename.column (4）WHERE constraint_expression(5)GROUP BY column (7)WITH CUBE | ROLLUP (8)HAVING constraint_expression(11)ORDER BY column ASCIDEsc (12)LIMIT count OFFSET count; ","date":"2024-09-04T23:18:48+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E5%AE%89%E8%A3%85mysql8.0.35/","title":"Ubuntu22.04安装MySQL8.0.35"},{"content":"可视化界面的操作简单易上手，主要是基于Gparted,非常适合新手。但是可视化界面的操作也有无法解决的问题，比如因为某些操作（例如编译系统或者下载大小未知的文件）可能会导致系统磁盘空间被占满从而无法下载GParted甚至无法正常开机的状况，这种时候就要使用命令行的扩容方案。\n可视化界面操作方案 VMware给虚拟机扩展空间（不是虚拟机可以直接跳过此步骤） 在虚拟机设置里的磁盘选项点击扩展，选择要扩展到的磁盘大小，我的虚拟机本来就有80G,所以这里就只扩展5G作为演示。 扩容完成后还需要在虚拟机里分区和扩展文件系统。 查看系统占用情况 可视化界面可以在左侧点击文件管理后，点击其他位置，正上方的计算机那里可以看到计算机的使用情况 终端界面，可以输入df -h或者sudo fdisk -l查看系统占用情况 下载分区管理软件GParted 在终端中输入下面的命令下载GParted\n1 sudo apt-get install gparted 对磁盘进行分区 点击左下角的九个点（显示应用程序），找到GParted。因为涉及磁盘数据，所以需要root权限，在弹出界面输入root用户密码即可。 打开软件可以看到目前虚拟机的磁盘情况，灰色的是刚刚在VMware给虚拟机扩容的5G,还没有分配，需要手动分配。 可以看到，/dev/sda3是被挂载到了根目录下面，我们扩容也是要对根目录扩容。右键/dev/sda3，点击调整大小/移动选项，直接滑动条拉到底或者编辑新大小调整空间。然后点击调整大小。 然后左下角会显示一项操作待处理，点击绿色的√后点击应用操作 此时计算机的空间就成功扩容5G 出现无法调整只读文件系统的大小(cannot resize read-only file system)的解决办法 右键/dev/sda3，点击信息，查看挂载点，重新挂载文件夹目录的读写权限。这里系统挂载了几个目录就要重新挂载几个目录的文件读写权限。 这里因为演示的这台机是一台新机，还没有用过火狐浏览器什么的，所以只挂载了根目录。我用另外一台机进行演示 打开终端，重新挂载这几个目录的读写权限\n1 2 sudo mount -o remount -rw / sudo mount -o remount -rw /var/snap/firefox/common/host-hunspell 重新打开GParted或者点击左上角的刷新设备，再次对磁盘进行分区（参照上述教程）\n命令行操作方案 参考资料：https://blog.csdn.net/ynstxx/article/details/129068856\n硬盘占满无法开机 如果磁盘空间还能支持开机，就开机后使用终端； 如果磁盘已经满了不能正常开机（开机时卡在/dev/sda* clean），使用CTRL+ALT+F2进入终端界面，输入用户名和密码进入系统。 下面就可以按照正常的命令行操作步骤进行操作（按照此方法打开的终端会有文本显示错误的问题，如上图中的棱形，这是中文显示错误，不影响实际操作，为了演示美观，我就不用这个演示了）\n查看系统占用情况 扩展虚拟机空间和查看系统占用情况可参照上述教程 输入命令 sudo fdisk -l 用parted -l命令解决爆红部分的分区表问题 使用parted命令将/dev/sda3扩容 1 2 3 4 5 6 7 8 9 10 11 12 13 sudo parted /dev/sda unit s（设置Size单位） p free（查看磁盘详情） resizepart 3（对第3个盘进行扩容） Yes（分区正在使用中，确认是否继续） 188743646s（需要将磁盘扩容到的大小值，此处参考自己的空间） q（退出磁盘分区模式） 使用sudo resize2fs /dev/sda3命令更新磁盘3的容量 1 sudo resize2fs /dev/sda3 再次通过df -h或者sudo fdisk -l命令查看磁盘情况 1 2 df -h sudo fdisk -l 扩容完成，如果是硬盘占满通过CTRL+ALT+F2打开的终端，此时输入命令reboot重启电脑即可正常启动\n","date":"2024-09-04T23:10:48+08:00","permalink":"https://rusthx.github.io/p/ubuntu22.04%E6%89%A9%E5%B1%95%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4/","title":"Ubuntu22.04扩展虚拟机的磁盘空间"},{"content":"安装虚拟机 下载vmvare https://www.vmware.com/products/desktop-hypervisor.html 现在的vmvare安装变得麻烦起来了，还要登陆网站。这里我用的是我之前下载的vmvare16pro。如果官网下载不太容易也可以直接搜索vmvare16安装，浏览器上应该还有留存的资源。\n下载Ubuntu24.04 https://cn.ubuntu.com/download/desktop Ubuntu的镜像文件大了很多，Ubuntu22.04只有3G左右，Ubuntu24.04就已经5G了。自行选择吧，两种都可以。 安装虚拟机 这次设置多少其实都可以，但是建议不要给的太少。尤其是磁盘空间，调整起来有点麻烦。官网给了最少资源。个人建议初始安装内存给4G，磁盘给40G。 这里安装扩展集合也可以，会比默认集合多装一些办公应用、视频播放器之类的东西。 谨慎设置用户名，电脑主机名修改比较容易，但是用户名修改比较困难。密码的话随意设置一个就可以。反正虚拟机是学习使用，太复杂的还是防自己罢了。 这个安装过程可能会很慢，应该会花几分钟。 重启后会提示按enter键。\nUbuntu配置 配置网络 刚安装的虚拟机里没有网，需要手动设置网络。设置网络有两种，一种是修改配置文件，一种是可视化界面（GUI）修改。我个人倾向GUI修改。 配置文件修改的方案参考https://blog.csdn.net/zyw2002/article/details/123486055 的2.2小节 可视化界面修改网络配置信息步骤如下： 点击左上角的编辑，虚拟网络编辑器，VMnet8。记住子网IP。比如我的是192.168.146.0 我的VMnet8子网ip为192.168.146.0。所以这里ip地址我填入了192.168.146.xxx（这里xxx可以填入小于255的数，但是建议不要填10以内的数）。子网掩码填入255.255.255.0即可。网关和DNS参照我的填入即可。 应用配置后重启一下连接就可以正常连接网络。右上角的网络标记变成图中的标志时就表示联网正常。没有可视化界面的可以ping一下外网查看情况。ping baidu.com。按CTRL+C即可终止命令。 安装vmtools 安装vmtools后可以解决屏幕显示比例的问题，可以在虚拟机外复制然后粘贴到虚拟机里，这很重要！ 右键，在终端中打开。安装命令如下\n1 2 3 4 sudo apt update sudo apt install open-vm-tools -y sudo apt install open-vm-tools-desktop -y sudo reboot 配置用户免密 在使用sudo命令时普通用户需要输入密码。每次都需要输入密码很麻烦，设置免密后使用sudo就不需要再输入密码。\n1 2 3 sudo passwd root #给root用户设置一个密码 sudo apt install gedit # gedit是一个文本编辑器，可以让ubuntu上编辑文件像Windows的记事本 sudo apt install vim #vim 是一个纯终端的文本编辑器 1 2 3 4 5 sudo chmod 777 /etc/sudoers # /etc/sudoers是一个只读文件，需要修改权限 su root gedit /etc/sudoers #在文件中添加如下标注部分 chmod 440 /etc/sudoers #将文件权限修改回来，解决报错/etc/sudoers可被任何人写 exit; 安装SSH 1 2 3 sudo apt-get install openssh-server sudo gedit /etc/ssh/sshd_config reboot #重启生效设置 尝试ssh远程登陆 设置主机映射 修改Windows的C:\\Windows\\System32\\drivers\\etc下的hosts文件。这里修改可能会有权限问题，可以复制hosts文件，粘贴到能修改的地方，比如桌面。修改完成后删除C:\\Windows\\System32\\drivers\\etc下的hosts，将修改后的hosts文件粘贴进来。 打开Windows的命令终端，输入命令\n1 ssh rust@bird # rust为ubuntu的用户名，bird为刚才设置的主机映射名 安装Java Ubuntu安装java可以手动安装，也可以命令行安装。手动安装需要设置环境变量，建议使用命令行安装。命令行安装卸载比较方便。\n1 2 sudo apt-get install openjdk-8-jdk java -version #查看安装是否成功 ","date":"2024-09-03T23:14:48+08:00","permalink":"https://rusthx.github.io/p/vmvare%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"VMvare安装配置Ubuntu虚拟机"}]