<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hive on rustWood</title>
        <link>https://rusthx.github.io/tags/hive/</link>
        <description>Recent content in Hive on rustWood</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>rustWood</copyright>
        <lastBuildDate>Fri, 25 Oct 2024 21:12:09 +0800</lastBuildDate><atom:link href="https://rusthx.github.io/tags/hive/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Hive启动参数</title>
        <link>https://rusthx.github.io/p/hive%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0/</link>
        <pubDate>Fri, 25 Oct 2024 21:12:09 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/hive%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0/</guid>
        <description>&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: hive
 -d,--define &amp;lt;key=value&amp;gt;          Variable substitution to apply to Hive
                                  commands. e.g. -d A=B or --define A=B
    --database &amp;lt;databasename&amp;gt;     Specify the database to use
 -e &amp;lt;quoted-query-string&amp;gt;         SQL from command line    (SQL来自命令行)
 -f &amp;lt;filename&amp;gt;                    SQL from files   (SQL来自文件)
 -H,--help                        Print help information
    --hiveconf &amp;lt;property=value&amp;gt;   Use value for given property   (通过命令行参数的方式进行配置信息的设置)
    --hivevar &amp;lt;key=value&amp;gt;         Variable substitution to apply to Hive
                                  commands. e.g. --hivevar A=B
 -i &amp;lt;filename&amp;gt;                    Initialization SQL file
 -S,--silent                      Silent mode in interactive shell
 -v,--verbose                     Verbose mode (echo executed SQL to the console)   (详细模式,在控制台输出SQL执行)

&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>数仓(Hive)数据倾斜产生原因及处理方式</title>
        <link>https://rusthx.github.io/p/%E6%95%B0%E4%BB%93hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/</link>
        <pubDate>Thu, 19 Sep 2024 08:54:12 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/%E6%95%B0%E4%BB%93hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;参考资料：&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1TC4y1n7sx/?spm_id_from=333.999.0.0&amp;amp;vd_source=2db7c64d895a2907954a5b8725db55d5&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;B站@左美美_ 相关视频&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;数据倾斜定义&#34;&gt;数据倾斜定义
&lt;/h2&gt;&lt;p&gt; 任务进度长时间维持在99%，查看任务监异页面，发现只有少量1个或几个reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多，最长时长远大于平均时长。&lt;/p&gt;
&lt;h2 id=&#34;数据倾斜产生原因&#34;&gt;数据倾斜产生原因
&lt;/h2&gt;&lt;p&gt; map输出数据按keyHash的分配到reduce中，由于key分布不均匀、业务数据本身的特性、建表时考虑不周、某些SQL语句本身就有数据倾斜等原因，造成的reduce上的数据量差异过大，所以如何将数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。&lt;/p&gt;
&lt;h3 id=&#34;key为空引起数据倾斜&#34;&gt;Key为空引起数据倾斜
&lt;/h3&gt;&lt;h4 id=&#34;倾斜原因&#34;&gt;倾斜原因
&lt;/h4&gt;&lt;p&gt;join的key值发生倾斜，key值包含很多空值或是异常值。&lt;/p&gt;
&lt;h4 id=&#34;解决方案&#34;&gt;解决方案
&lt;/h4&gt;&lt;p&gt;对值为空的key进行打散，为空key赋一个随机的值，使得key值为空的数据随机均匀地分布到不同的reducer上。&lt;/p&gt;
&lt;h4 id=&#34;测试案例&#34;&gt;测试案例
&lt;/h4&gt;&lt;p&gt;设置多个reduce任务&lt;/p&gt;
&lt;p&gt;&lt;code&gt;set mapreduce.job.reduces = 5;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;两张大表join，做全连接&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;select t.id
       ,t.year
       ,t.temperature
       ,s.state
  from temperature t 
    full join station s on nvl(t.id,rand())=s.id
  limit 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;备注：&lt;code&gt;nvl()&lt;/code&gt;为空值转换函数，&lt;code&gt;rand()&lt;/code&gt;为随机函数。也可以使用&lt;code&gt;ifnull()&lt;/code&gt;、&lt;code&gt;coalesce()&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;group-by-引起数据倾斜&#34;&gt;group by 引起数据倾斜
&lt;/h3&gt;&lt;h4 id=&#34;倾斜原因-1&#34;&gt;倾斜原因
&lt;/h4&gt;&lt;p&gt;默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。&lt;/p&gt;
&lt;h4 id=&#34;解决方案-1&#34;&gt;解决方案
&lt;/h4&gt;&lt;p&gt;并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。
&lt;img src=&#34;https://rusthx.github.io/p/%E6%95%B0%E4%BB%93hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/1.png&#34;
	width=&#34;820&#34;
	height=&#34;371&#34;
	srcset=&#34;https://rusthx.github.io/p/%E6%95%B0%E4%BB%93hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/1_hu16475614890463934495.png 480w, https://rusthx.github.io/p/%E6%95%B0%E4%BB%93hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/1_hu11470359486358445655.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;221&#34;
		data-flex-basis=&#34;530px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;实现方式1&lt;/p&gt;
&lt;p&gt;1）是否在Map端进行聚合，默认为True（使用Combiner局部合并）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;set hive.map.aggr = true;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;2）设置map端预聚合的行数阈值&lt;/p&gt;
&lt;p&gt;&lt;code&gt;set hive.groupby.mapaggr.checkinterval=100000;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;实现方式2&lt;/p&gt;
&lt;p&gt;有数据倾斜的时候进行负载均衡（默认是false）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;set hive.groupby.skelindata= true;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt; 当遇到数据倾斜时，groupby会启动两个MR job。第一个job会将map端数据随机输入reducer，每个reducer做部分聚合，相同的key就会分布在不同的reducer中。第二个job再将前面预处理过的数据按key聚合并输出结果，这样就起到了均衡的效果。&lt;/p&gt;
&lt;h4 id=&#34;测试案例-1&#34;&gt;测试案例
&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;set hive.map.aggr =true;
set hive.groupby.mapaggr.checkinterval= 100000;
set hive.groupby.skewindata= true;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;select id,count(*)
  from temperature 
  group by id;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;count-distinct引起数据倾斜&#34;&gt;count distinct引起数据倾斜
&lt;/h3&gt;&lt;h4 id=&#34;倾斜原因-2&#34;&gt;倾斜原因
&lt;/h4&gt;&lt;p&gt;count distinct聚合时存在大量特殊值，比如存在大量值为NULL或空的记录。&lt;/p&gt;
&lt;h4 id=&#34;解决方案-2&#34;&gt;解决方案
&lt;/h4&gt;&lt;p&gt;做count distinct时，将值为空的情况单独处理。&lt;/p&gt;
&lt;p&gt;1）如果只是统计去重后的记录数，可以不用处理空值，先把空值过滤掉，然后在最后结果中加1即可&lt;/p&gt;
&lt;p&gt;2）如果还包含其他计算，需要进行groupby操作，先将值为空的记录单独处理，然后再跟其他计算结果union操作。&lt;/p&gt;
&lt;h3 id=&#34;join操作引起数据倾斜&#34;&gt;join操作引起数据倾斜
&lt;/h3&gt;&lt;h4 id=&#34;大表join小表hive旧版本&#34;&gt;大表join小表(hive旧版本)
&lt;/h4&gt;&lt;p&gt;新版本(Hive3)已经自动自动优化&lt;/p&gt;
&lt;p&gt;1）产生原因&lt;/p&gt;
&lt;p&gt;业务数据本身就存在key分布不均匀的情况，一般情况会产生数据倾斜&lt;/p&gt;
&lt;p&gt;2）解决方式&lt;/p&gt;
&lt;p&gt;使用map join让小的维度表先进内存，在map端完成join&lt;/p&gt;
&lt;p&gt;3）实现原理&lt;/p&gt;
&lt;p&gt;使用map join，直接在map端就完成表的join操作，进入map端的数据都是经过split得到的，没有根据key分区这一操作，所以数据都是相对均匀地分布在每个maptask中的，所以就不会产生数据倾斜。&lt;/p&gt;
&lt;h4 id=&#34;大表join大表&#34;&gt;大表join大表
&lt;/h4&gt;&lt;p&gt;1）产生原因&lt;/p&gt;
&lt;p&gt;业务数据本身的特性，导致两个表都是大表。&lt;/p&gt;
&lt;p&gt;2）解决方式&lt;/p&gt;
&lt;p&gt;业务消减&lt;/p&gt;
&lt;p&gt;3）实现原理&lt;/p&gt;
&lt;p&gt; 业务数据有数据倾斜的风险，但是这些导致数据倾斜风险的key一般都是无效的，如uid为空，因为uid为空的记录是没有意义的。&lt;/p&gt;
&lt;p&gt; 所以当业务数据很大，但是数据中的大部分（一般都是80%）可能都是无效数据，那么就可以在join时过滤掉空值uid，没有了这些无效数据，自然就不存在这么大量集中的key，数据倾斜的风险就会消失。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Scala连接MySQL和Hive</title>
        <link>https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/</link>
        <pubDate>Sat, 07 Sep 2024 11:47:55 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/</guid>
        <description>&lt;h2 id=&#34;连接mysql&#34;&gt;连接MySQL
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/Jaryer/p/13671449.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cnblogs.com/Jaryer/p/13671449.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;maven添加依赖&#34;&gt;maven添加依赖
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;com.mysql&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;mysql-connector-j&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;8.0.33&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;连接数据库&#34;&gt;连接数据库
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val host = &amp;quot;localhost&amp;quot;
val port = 3306
val database = &amp;quot;sparktest&amp;quot;
val jdbcUrl = s&amp;quot;jdbc:mysql://$host:$port/$database?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;quot;
val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, &amp;quot;root&amp;quot;, &amp;quot;123456&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;执行查询&#34;&gt;执行查询
&lt;/h3&gt;&lt;p&gt;SQL语句在执行时有三种：&lt;code&gt;executeQuery&lt;/code&gt;,&lt;code&gt;executeUpdate&lt;/code&gt;,&lt;code&gt;execute&lt;/code&gt;。具体细节可查看此节开头的参考资料。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;    val statement: Statement = mysqlConn.createStatement()
    //插入数据
    statement.executeUpdate(&amp;quot;insert into employee values (3,&#39;Mary&#39;,&#39;F&#39;,26)&amp;quot;)
    statement.executeUpdate(&amp;quot;insert into employee values (4,&#39;Tom&#39;,&#39;M&#39;,23)&amp;quot;)

    val result: ResultSet = statement.executeQuery(&amp;quot;select max(age) as max_age,avg(age) as avg_age from employee&amp;quot;)
    while (result.next()) {
      println(result.getString(&amp;quot;max_age&amp;quot;),result.getString(&amp;quot;avg_age&amp;quot;))
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;完整代码&#34;&gt;完整代码
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package sparkjob5

import java.sql.{Connection, DriverManager, ResultSet, Statement}

object task3 {
  def main(args: Array[String]): Unit = {
    //连接mysql
    val host = &amp;quot;localhost&amp;quot;
    val port = 3306
    val database = &amp;quot;sparktest&amp;quot;
    val jdbcUrl = s&amp;quot;jdbc:mysql://$host:$port/$database?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;quot;
    val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, &amp;quot;root&amp;quot;, &amp;quot;123456&amp;quot;)

    val statement: Statement = mysqlConn.createStatement()
    //插入数据
    statement.executeUpdate(&amp;quot;insert into employee values (3,&#39;Mary&#39;,&#39;F&#39;,26)&amp;quot;)
    statement.executeUpdate(&amp;quot;insert into employee values (4,&#39;Tom&#39;,&#39;M&#39;,23)&amp;quot;)

    val result: ResultSet = statement.executeQuery(&amp;quot;select max(age) as max_age,avg(age) as avg_age from employee&amp;quot;)
    while (result.next()) {
      println(result.getString(&amp;quot;max_age&amp;quot;),result.getString(&amp;quot;avg_age&amp;quot;))
    }

    result.close()
    statement.close()
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;连接hive&#34;&gt;连接Hive
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;a class=&#34;link&#34; href=&#34;https://www.jianshu.com/p/27a798013990&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.jianshu.com/p/27a798013990&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;连接Hive前需要开启Hive的metastore和hiverserver2。开启命令如下。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;开启Hadoop集群&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;start-all.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;开启Hive,第二三行的启动命令需要分别开一个终端启动，输出的日志在&lt;code&gt;/usr/local/hive/logs&lt;/code&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive
hive --service metastore &amp;gt;logs/metastore.log 2&amp;gt;&amp;amp;1
hive --service hiveserver2 &amp;gt;logs/hiveServer2.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;添加依赖&#34;&gt;添加依赖
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;spark-hive_2.12&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;3.3.4&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.hive&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;hive-jdbc&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;3.1.3&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;完整依赖如下（包含了Scala连接MySQL的依赖）&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;project xmlns=&amp;quot;http://maven.apache.org/POM/4.0.0&amp;quot;
         xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;
         xsi:schemaLocation=&amp;quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&amp;quot;&amp;gt;
    &amp;lt;parent&amp;gt;
        &amp;lt;artifactId&amp;gt;Spark&amp;lt;/artifactId&amp;gt;
        &amp;lt;groupId&amp;gt;org.example&amp;lt;/groupId&amp;gt;
        &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;
    &amp;lt;/parent&amp;gt;
    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;

    &amp;lt;artifactId&amp;gt;sparkCore&amp;lt;/artifactId&amp;gt;
    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-core_2.12&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-sql_2.12&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;com.mysql&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;mysql-connector-j&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;8.0.33&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-hive_2.12&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.4&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.hive&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;hive-jdbc&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.1.3&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

    &amp;lt;/dependencies&amp;gt;

    &amp;lt;properties&amp;gt;
        &amp;lt;maven.compiler.source&amp;gt;17&amp;lt;/maven.compiler.source&amp;gt;
        &amp;lt;maven.compiler.target&amp;gt;17&amp;lt;/maven.compiler.target&amp;gt;
        &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt;
    &amp;lt;/properties&amp;gt;
    &amp;lt;build&amp;gt;
        &amp;lt;plugins&amp;gt;
            &amp;lt;!-- 该插件用于将 Scala 代码编译成 class 文件 --&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;net.alchim31.maven&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;scala-maven-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;3.2.2&amp;lt;/version&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;!-- 声明绑定到 maven 的 compile 阶段 --&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;testCompile&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;3.1.0&amp;lt;/version&amp;gt;
                &amp;lt;configuration&amp;gt;
                    &amp;lt;descriptorRefs&amp;gt;
                        &amp;lt;descriptorRef&amp;gt;jar-with-dependencies&amp;lt;/descriptorRef&amp;gt;
                    &amp;lt;/descriptorRefs&amp;gt;
                &amp;lt;/configuration&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;id&amp;gt;make-assembly&amp;lt;/id&amp;gt;
                        &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
    &amp;lt;/build&amp;gt;
&amp;lt;/project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;修改配置文件hive-sitexml&#34;&gt;修改配置文件hive-site.xml
&lt;/h3&gt;&lt;p&gt;在resource下新建一个&lt;code&gt;hive-site.xml&lt;/code&gt;，填入下列内容。注意：要把&lt;code&gt;hadoop1&lt;/code&gt;修改成自己的Hadoop集群主节点名字或者ip。
&lt;img src=&#34;https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/1.png&#34;
	width=&#34;1261&#34;
	height=&#34;798&#34;
	srcset=&#34;https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/1_hu6304836396205212170.png 480w, https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/1_hu6195403910822788867.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;379px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&amp;quot;text/xsl&amp;quot; href=&amp;quot;configuration.xsl&amp;quot;?&amp;gt;

&amp;lt;configuration&amp;gt;
    &amp;lt;!-- 添加文件调用 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.exec.scratchdir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://hadoop1:8020/user/hive/tmp&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://hadoop1:8020/user/hive/warehouse&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.querylog.location&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://hadoop1:8020/user/hive/log&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;!-- 指定存储元数据要连接的地址 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.uris&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;thrift://hadoop1:9083&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的URL --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true&amp;amp;amp;characterEncodeing=UTF-8&amp;amp;amp;allowPublicKeyRetrieval=true&amp;amp;amp;useSSL=false&amp;amp;amp;serverTimezone=GMT&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的Driver--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的username--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的password --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;123456&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;!-- 指定hiveserver2连接的host --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.thrift.bind.host&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hadoop1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- 指定hiveserver2连接的端口号 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.thrift.port&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;10000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.active.passive.ha.enable&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scala代码&#34;&gt;Scala代码
&lt;/h3&gt;&lt;p&gt;在&lt;code&gt;spark.sql()&lt;/code&gt;里写上正常的SQL语句即可完成查询。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package sparkjob5


import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession


object task4 {
  val driverName = &amp;quot;org.apache.hive.jdbc.HiveDriver&amp;quot;
  try {
    Class.forName(driverName)
  } catch {
    case e: ClassNotFoundException =&amp;gt;
    println(&amp;quot;Missing Class&amp;quot;, e)
  }

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster(&amp;quot;local[3]&amp;quot;).setAppName(&amp;quot;hive&amp;quot;)
    val spark = SparkSession.builder().config(conf).enableHiveSupport().getOrCreate()

    spark.sql(&amp;quot;use spark_test&amp;quot;)
    spark.sql(&amp;quot;show tables&amp;quot;).show()
    spark.close()
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;补充：将查询结果保存到hdfs上，如果想保存到本地，则可以将save的路径改成本地路径。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;    val dataFrame = spark.sql(&amp;quot;select uid,keyword from sougou_records where keyword like &#39;%仙剑奇侠传%&#39;&amp;quot;)

    dataFrame.write
      .format(&amp;quot;csv&amp;quot;)
      .option(&amp;quot;header&amp;quot;, &amp;quot;false&amp;quot;)
      .option(&amp;quot;sep&amp;quot;, &amp;quot;\t&amp;quot;)
      .save(&amp;quot;hdfs://hadoop1:8020/xianJianTest&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果想以表格保存到MySQL或者Hive,可以使用&lt;code&gt;saveAsTable()&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val host = &amp;quot;localhost&amp;quot;
val port = 3306
val database = &amp;quot;sparktest&amp;quot;
val jdbcUrl = s&amp;quot;jdbc:mysql://$host:$port/$database?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;quot;
val connectionProperties = new java.util.Properties()
connectionProperties.put(&amp;quot;user&amp;quot;, &amp;quot;root&amp;quot;)
connectionProperties.put(&amp;quot;password&amp;quot;, &amp;quot;123456&amp;quot;)

df.write.mode(SaveMode.Overwrite).jdbc(jdbcUrl,&amp;quot;company&amp;quot;,connectionProperties)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df.write.mode(SaveMode.Overwrite).saveAsTable(&amp;quot;spark_test.company&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Ubuntu22.04配置Hive及Hive on Spark</title>
        <link>https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/</link>
        <pubDate>Sat, 07 Sep 2024 11:15:48 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/</guid>
        <description>&lt;h2 id=&#34;前置准备配置hive的mysql连接用户&#34;&gt;前置准备：配置Hive的MySQL连接用户
&lt;/h2&gt;&lt;p&gt;MySQL的配置可参考我的教程 &lt;a class=&#34;link&#34; href=&#34;https://rusthx.github.io/p/ubuntu22.04%E5%AE%89%E8%A3%85mysql8.0.35/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MySQL安装教程&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建Hive元数据库&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create database metastore;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;创建用户hive，设置密码为123456&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create user &#39;hive&#39;@&#39;%&#39; identified by &#39;123456&#39;;
grant all privileges on metastore.* to &#39;hive&#39;@&#39;%&#39; with grant option;
flush privileges;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;安装hive&#34;&gt;安装Hive
&lt;/h2&gt;&lt;p&gt;参考资料：B站尚硅谷
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1mG411o7Lt?p=62&amp;amp;vd_source=2db7c64d895a2907954a5b8725db55d5&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;062.Hive的安装部署_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载Hive安装包&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：apache原装的Hive只支持Spark2.3.0，不支持Spark3.3.0，需要重新编译Hive的源码，尚硅谷已经编译好了，这里我就直接使用了&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;修改配置文件（cd 到hive下的conf文件夹，这里我已经将Hive安装包改名为hive并移动到/usr/local/下）&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo mv hive-default.xml.template hive-default.xml
sudo vim hive-site.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将以下内容写入文件&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&amp;quot;text/xsl&amp;quot; href=&amp;quot;configuration.xsl&amp;quot;?&amp;gt;

&amp;lt;configuration&amp;gt;
    &amp;lt;!-- jdbc连接的URL --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true&amp;amp;amp;characterEncodeing=UTF-8&amp;amp;amp;allowPublicKeyRetrieval=true&amp;amp;amp;useSSL=false&amp;amp;amp;serverTimezone=GMT&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
    &amp;lt;!-- jdbc连接的Driver--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
	&amp;lt;!-- jdbc连接的username--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的password --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;123456&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
    &amp;lt;!-- Hive默认在HDFS的工作目录 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/user/hive/warehouse&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
        &amp;lt;!-- 指定存储元数据要连接的地址 --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.metastore.uris&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;thrift://hadoop1:9083&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
    &amp;lt;!-- 指定hiveserver2连接的host --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.server2.thrift.bind.host&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;hadoop1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- 指定hiveserver2连接的端口号 --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.server2.thrift.port&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;10000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.server2.active.passive.ha.enable&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.cli.print.header&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.cli.print.current.db&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;配置环境变量&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo vim ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在下面添加&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;启动hive&#34;&gt;启动Hive
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;启动Hadoop&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;start-all.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;初始化Hive&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive
./bin/schematool -dbType mysql -initSchema
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正常初始化会日志刷屏并出现大片空白，然后最后一行出现succeed或者complete的字样
如果没有正常初始化就复制最下面几行中的报错信息，粘贴到必应进行查找&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;启动Hive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;启动Hive前需要先启动Hive的元数据库metastore和hiveserver2
注意：这里的metastore和hiveserver2每个都要单独开启一个终端，开启一个后再开一个新的终端进行命令
日志被重定向到了logs文件夹下，需要查看日志可以在这个文件夹下查看&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive/
hive --service metastore &amp;gt;logs/metastore.log 2&amp;gt;&amp;amp;1
hive --service hiveserver2 &amp;gt;logs/hiveServer2.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/hive
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正常启动会出现一个交互界面如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hive(default)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;解决Hive shell中打印大量日志的问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当在Hive的命令行中查询时出现大量日志时，可以在conf下新建日志配置文件如下&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/conf/
vim log4j.properties
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;粘贴如下内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;log4j.rootLogger=WARN, CA
log4j.appender.CA=org.apache.log4j.ConsoleAppender
log4j.appender.CA.layout=org.apache.log4j.PatternLayout
log4j.appender.CA.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hive-on-spark配置&#34;&gt;Hive on Spark配置
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;在官网下载纯净版Spark（不带Hadoop依赖的）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://spark.apache.org/downloads.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://spark.apache.org/downloads.html&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;解压Spark&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tar -zxvf spark-3.3.1-bin-without-hadoop.tgz -C /usr/local/
mv /usr/local/spark-3.3.1-bin-without-hadoop /usr/local/spark
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;修改spark-env.sh配置文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mv /usr/local/spark/conf/spark-env.sh.template /usr/local/spark/conf/spark-env.sh
vim /usr/local/spark/conf/spark-env.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;增添下面内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export SPARK_DIST_CLASSPATH=$(hadoop classpath)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;配置Spark环境变量&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo vim ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;添加下列内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;在Hive中创建spark配置文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim /usr/local/hive/conf/spark-defaults.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;添加如下内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;spark.master                               yarn
spark.eventLog.enabled                   true
spark.eventLog.dir                        hdfs://hadoop1:8020/spark-history
spark.executor.memory                    1g
spark.driver.memory					     1g
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在HDFS中创建如下路径，用于存储历史日志&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hadoop fs -mkdir /spark-history
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;向HDFS上传Spark纯净版jar包&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;说明1：采用Spark纯净版jar包，不包含hadoop和hive相关依赖，能避免依赖冲突。
说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hadoop fs -mkdir /spark-jars
hadoop fs -put /usr/local/spark/jars/* /spark-jars
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;修改hive-site.xml文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim /usr/local/hive/conf/hive-site.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;添加如下内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;spark.yarn.jars&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hdfs://hadoop1:8020/spark-jars/*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;!--Hive执行引擎--&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.execution.engine&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;spark&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;!--连接超时时间--&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.spark.client.connect.timeout&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;30000ms&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hive-on-spark测试&#34;&gt;Hive on Spark测试
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;启动hive客户端&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive/
bin/hive
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;创建一张测试表&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;hive (default)&amp;gt; create table student(id int, name string);
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;通过insert测试效果&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;hive (default)&amp;gt; insert into table student values(1,&#39;abc&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果如下则配置成功
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/1.png&#34;
	width=&#34;1643&#34;
	height=&#34;686&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/1_hu18263344740042083967.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/1_hu11299025821609503695.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/2.png&#34;
	width=&#34;914&#34;
	height=&#34;659&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/2_hu13084986199049206096.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/2_hu2576634140735695222.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;yarn环境配置&#34;&gt;Yarn环境配置
&lt;/h2&gt;&lt;p&gt;增加ApplicationMaster资源比例
容量调度器对每个资源队列中同时运行的Application Master占用的资源进行了限制，该限制通过yarn.scheduler.capacity.maximum-am-resource-percent参数实现，其默认值是0.1，表示每个资源队列上Application Master最多可使用的资源为该队列总资源的10%，目的是防止大部分资源都被Application Master占用，而导致Map/Reduce Task无法执行。
生产环境该参数可使用默认值。但学习环境，集群资源总数很少，如果只分配10%的资源给Application Master，则可能出现，同一时刻只能运行一个Job的情况，因为一个Application Master使用的资源就可能已经达到10%的上限了。故此处可将该值适当调大。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在hadoop1的/usr/local/hadoop/etc/hadoop/capacity-scheduler.xml文件中&lt;strong&gt;修改&lt;/strong&gt;如下参数值&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim capacity-scheduler.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;lt;property&amp;gt;
   &amp;lt;name&amp;gt;yarn.scheduler.capacity.maximum-am-resource-percent&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;0.8&amp;lt;/value&amp;gt;
&amp;lt;/property
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;分发capacity-scheduler.xml配置文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;xsync capacity-scheduler.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;重启集群&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;stop-all.sh
start-all.sh
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
