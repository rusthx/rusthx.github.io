<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Scala on rustWood</title>
        <link>https://rusthx.github.io/tags/scala/</link>
        <description>Recent content in Scala on rustWood</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>rustWood</copyright>
        <lastBuildDate>Sat, 07 Sep 2024 11:47:55 +0800</lastBuildDate><atom:link href="https://rusthx.github.io/tags/scala/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Scala连接MySQL和Hive</title>
        <link>https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/</link>
        <pubDate>Sat, 07 Sep 2024 11:47:55 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/</guid>
        <description>&lt;h2 id=&#34;连接mysql&#34;&gt;连接MySQL
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/Jaryer/p/13671449.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cnblogs.com/Jaryer/p/13671449.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;maven添加依赖&#34;&gt;maven添加依赖
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;com.mysql&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;mysql-connector-j&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;8.0.33&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;连接数据库&#34;&gt;连接数据库
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val host = &amp;quot;localhost&amp;quot;
val port = 3306
val database = &amp;quot;sparktest&amp;quot;
val jdbcUrl = s&amp;quot;jdbc:mysql://$host:$port/$database?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;quot;
val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, &amp;quot;root&amp;quot;, &amp;quot;123456&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;执行查询&#34;&gt;执行查询
&lt;/h3&gt;&lt;p&gt;SQL语句在执行时有三种：&lt;code&gt;executeQuery&lt;/code&gt;,&lt;code&gt;executeUpdate&lt;/code&gt;,&lt;code&gt;execute&lt;/code&gt;。具体细节可查看此节开头的参考资料。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;    val statement: Statement = mysqlConn.createStatement()
    //插入数据
    statement.executeUpdate(&amp;quot;insert into employee values (3,&#39;Mary&#39;,&#39;F&#39;,26)&amp;quot;)
    statement.executeUpdate(&amp;quot;insert into employee values (4,&#39;Tom&#39;,&#39;M&#39;,23)&amp;quot;)

    val result: ResultSet = statement.executeQuery(&amp;quot;select max(age) as max_age,avg(age) as avg_age from employee&amp;quot;)
    while (result.next()) {
      println(result.getString(&amp;quot;max_age&amp;quot;),result.getString(&amp;quot;avg_age&amp;quot;))
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;完整代码&#34;&gt;完整代码
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package sparkjob5

import java.sql.{Connection, DriverManager, ResultSet, Statement}

object task3 {
  def main(args: Array[String]): Unit = {
    //连接mysql
    val host = &amp;quot;localhost&amp;quot;
    val port = 3306
    val database = &amp;quot;sparktest&amp;quot;
    val jdbcUrl = s&amp;quot;jdbc:mysql://$host:$port/$database?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;quot;
    val mysqlConn: Connection = DriverManager.getConnection(jdbcUrl, &amp;quot;root&amp;quot;, &amp;quot;123456&amp;quot;)

    val statement: Statement = mysqlConn.createStatement()
    //插入数据
    statement.executeUpdate(&amp;quot;insert into employee values (3,&#39;Mary&#39;,&#39;F&#39;,26)&amp;quot;)
    statement.executeUpdate(&amp;quot;insert into employee values (4,&#39;Tom&#39;,&#39;M&#39;,23)&amp;quot;)

    val result: ResultSet = statement.executeQuery(&amp;quot;select max(age) as max_age,avg(age) as avg_age from employee&amp;quot;)
    while (result.next()) {
      println(result.getString(&amp;quot;max_age&amp;quot;),result.getString(&amp;quot;avg_age&amp;quot;))
    }

    result.close()
    statement.close()
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;连接hive&#34;&gt;连接Hive
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;a class=&#34;link&#34; href=&#34;https://www.jianshu.com/p/27a798013990&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.jianshu.com/p/27a798013990&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;连接Hive前需要开启Hive的metastore和hiverserver2。开启命令如下。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;开启Hadoop集群&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;start-all.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;开启Hive,第二三行的启动命令需要分别开一个终端启动，输出的日志在&lt;code&gt;/usr/local/hive/logs&lt;/code&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive
hive --service metastore &amp;gt;logs/metastore.log 2&amp;gt;&amp;amp;1
hive --service hiveserver2 &amp;gt;logs/hiveServer2.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;添加依赖&#34;&gt;添加依赖
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;spark-hive_2.12&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;3.3.4&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.hive&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;hive-jdbc&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;3.1.3&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;完整依赖如下（包含了Scala连接MySQL的依赖）&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;project xmlns=&amp;quot;http://maven.apache.org/POM/4.0.0&amp;quot;
         xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;
         xsi:schemaLocation=&amp;quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&amp;quot;&amp;gt;
    &amp;lt;parent&amp;gt;
        &amp;lt;artifactId&amp;gt;Spark&amp;lt;/artifactId&amp;gt;
        &amp;lt;groupId&amp;gt;org.example&amp;lt;/groupId&amp;gt;
        &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;
    &amp;lt;/parent&amp;gt;
    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;

    &amp;lt;artifactId&amp;gt;sparkCore&amp;lt;/artifactId&amp;gt;
    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-core_2.12&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-sql_2.12&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;com.mysql&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;mysql-connector-j&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;8.0.33&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-hive_2.12&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.3.4&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.hive&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;hive-jdbc&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;3.1.3&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

    &amp;lt;/dependencies&amp;gt;

    &amp;lt;properties&amp;gt;
        &amp;lt;maven.compiler.source&amp;gt;17&amp;lt;/maven.compiler.source&amp;gt;
        &amp;lt;maven.compiler.target&amp;gt;17&amp;lt;/maven.compiler.target&amp;gt;
        &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt;
    &amp;lt;/properties&amp;gt;
    &amp;lt;build&amp;gt;
        &amp;lt;plugins&amp;gt;
            &amp;lt;!-- 该插件用于将 Scala 代码编译成 class 文件 --&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;net.alchim31.maven&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;scala-maven-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;3.2.2&amp;lt;/version&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;!-- 声明绑定到 maven 的 compile 阶段 --&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;testCompile&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;3.1.0&amp;lt;/version&amp;gt;
                &amp;lt;configuration&amp;gt;
                    &amp;lt;descriptorRefs&amp;gt;
                        &amp;lt;descriptorRef&amp;gt;jar-with-dependencies&amp;lt;/descriptorRef&amp;gt;
                    &amp;lt;/descriptorRefs&amp;gt;
                &amp;lt;/configuration&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;id&amp;gt;make-assembly&amp;lt;/id&amp;gt;
                        &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
    &amp;lt;/build&amp;gt;
&amp;lt;/project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;修改配置文件hive-sitexml&#34;&gt;修改配置文件hive-site.xml
&lt;/h3&gt;&lt;p&gt;在resource下新建一个&lt;code&gt;hive-site.xml&lt;/code&gt;，填入下列内容。注意：要把&lt;code&gt;hadoop1&lt;/code&gt;修改成自己的Hadoop集群主节点名字或者ip。
&lt;img src=&#34;https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/1.png&#34;
	width=&#34;1261&#34;
	height=&#34;798&#34;
	srcset=&#34;https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/1_hu6304836396205212170.png 480w, https://rusthx.github.io/p/scala%E8%BF%9E%E6%8E%A5mysql%E5%92%8Chive/1_hu6195403910822788867.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;379px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&amp;quot;text/xsl&amp;quot; href=&amp;quot;configuration.xsl&amp;quot;?&amp;gt;

&amp;lt;configuration&amp;gt;
    &amp;lt;!-- 添加文件调用 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.exec.scratchdir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://hadoop1:8020/user/hive/tmp&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://hadoop1:8020/user/hive/warehouse&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.querylog.location&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://hadoop1:8020/user/hive/log&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;!-- 指定存储元数据要连接的地址 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.uris&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;thrift://hadoop1:9083&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的URL --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true&amp;amp;amp;characterEncodeing=UTF-8&amp;amp;amp;allowPublicKeyRetrieval=true&amp;amp;amp;useSSL=false&amp;amp;amp;serverTimezone=GMT&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的Driver--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的username--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的password --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;123456&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;!-- 指定hiveserver2连接的host --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.thrift.bind.host&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hadoop1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- 指定hiveserver2连接的端口号 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.thrift.port&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;10000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.server2.active.passive.ha.enable&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;scala代码&#34;&gt;Scala代码
&lt;/h3&gt;&lt;p&gt;在&lt;code&gt;spark.sql()&lt;/code&gt;里写上正常的SQL语句即可完成查询。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package sparkjob5


import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession


object task4 {
  val driverName = &amp;quot;org.apache.hive.jdbc.HiveDriver&amp;quot;
  try {
    Class.forName(driverName)
  } catch {
    case e: ClassNotFoundException =&amp;gt;
    println(&amp;quot;Missing Class&amp;quot;, e)
  }

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster(&amp;quot;local[3]&amp;quot;).setAppName(&amp;quot;hive&amp;quot;)
    val spark = SparkSession.builder().config(conf).enableHiveSupport().getOrCreate()

    spark.sql(&amp;quot;use spark_test&amp;quot;)
    spark.sql(&amp;quot;show tables&amp;quot;).show()
    spark.close()
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;补充：将查询结果保存到hdfs上，如果想保存到本地，则可以将save的路径改成本地路径。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;    val dataFrame = spark.sql(&amp;quot;select uid,keyword from sougou_records where keyword like &#39;%仙剑奇侠传%&#39;&amp;quot;)

    dataFrame.write
      .format(&amp;quot;csv&amp;quot;)
      .option(&amp;quot;header&amp;quot;, &amp;quot;false&amp;quot;)
      .option(&amp;quot;sep&amp;quot;, &amp;quot;\t&amp;quot;)
      .save(&amp;quot;hdfs://hadoop1:8020/xianJianTest&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果想以表格保存到MySQL或者Hive,可以使用&lt;code&gt;saveAsTable()&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val host = &amp;quot;localhost&amp;quot;
val port = 3306
val database = &amp;quot;sparktest&amp;quot;
val jdbcUrl = s&amp;quot;jdbc:mysql://$host:$port/$database?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;quot;
val connectionProperties = new java.util.Properties()
connectionProperties.put(&amp;quot;user&amp;quot;, &amp;quot;root&amp;quot;)
connectionProperties.put(&amp;quot;password&amp;quot;, &amp;quot;123456&amp;quot;)

df.write.mode(SaveMode.Overwrite).jdbc(jdbcUrl,&amp;quot;company&amp;quot;,connectionProperties)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df.write.mode(SaveMode.Overwrite).saveAsTable(&amp;quot;spark_test.company&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        <item>
        <title>Scala中函数与Spark的算子的区别</title>
        <link>https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/</link>
        <pubDate>Sat, 07 Sep 2024 11:46:08 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/</guid>
        <description>&lt;p&gt;Scala中的部分函数和RDD中的部分算子名字一样，功能一样，用起来也差不多。但是为什么一个叫函数，一个却要叫算子，函数和算子的区别在哪，这让我有些好奇。于是查看了源码，对函数和算子进行了比较。下面以&lt;code&gt;map&lt;/code&gt;为例。&lt;/p&gt;
&lt;h2 id=&#34;scala中的map函数&#34;&gt;Scala中的map函数
&lt;/h2&gt;&lt;p&gt;Scala中的map通常定义在集合类中，例如&lt;code&gt;Map&lt;/code&gt;、&lt;code&gt;List&lt;/code&gt;、&lt;code&gt;Seq&lt;/code&gt;、&lt;code&gt;Set&lt;/code&gt;。作用是对该可迭代集合的所有元素应用一个函数，从而建立一个新的可迭代集合。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Builds a new iterable collection by applying a function to all elements of this iterable collection.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：&lt;code&gt;List&lt;/code&gt;属于&lt;code&gt;scala.collection.immutable&lt;/code&gt;的子类，而&lt;code&gt;Map、Seq、Set&lt;/code&gt;属于&lt;code&gt;scala.collection&lt;/code&gt;的子类
&lt;code&gt;map&lt;/code&gt;函数最底层的源码应该是&lt;code&gt;scala.collection&lt;/code&gt;里的如下代码
&lt;img src=&#34;https://img2024.cnblogs.com/blog/3289663/202404/3289663-20240414125942145-277602576.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;
具体实现以&lt;code&gt;List&lt;/code&gt;中的&lt;code&gt;map&lt;/code&gt;举例
源码为&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  final override def map[B](f: A =&amp;gt; B): List[B] = {
    if (this eq Nil) Nil else {
      val h = new ::[B](f(head), Nil)
      var t: ::[B] = h
      var rest = tail
      while (rest ne Nil) {
        val nx = new ::(f(rest.head), Nil)
        t.next = nx
        t = nx
        rest = rest.tail
      }
      releaseFence()
      h
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;List&lt;/code&gt;中的&lt;code&gt;map&lt;/code&gt;重写了其父类&lt;code&gt;collection.IterableOnce&lt;/code&gt;的&lt;code&gt;map&lt;/code&gt;函数，定义了一个匿名函数&lt;code&gt;(f:A=&amp;gt;B)&lt;/code&gt;,对List中的每个元素A处理后输出B类型的&lt;code&gt;List&lt;/code&gt;。比如&lt;code&gt;List[String]&lt;/code&gt;经过&lt;code&gt;map&lt;/code&gt;处理后可以变成&lt;code&gt;List[Interger]&lt;/code&gt;
其他collection的代码可以自行查看，也可以查看Scala的官方文档&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.scala-lang.org/api/current/scala/collection&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.scala-lang.org/api/current/scala/collection&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;spark中的map算子&#34;&gt;Spark中的map算子
&lt;/h2&gt;&lt;p&gt;Spark中的算子分为转换算子（Transformations (return a new RDD)）和行动算子（Actions (launch a job to return a value to the user program)）， 转换算子根据数据处理方式的不同将算子整体上分为 Value 类型、双 Value 类型和 Key-Value 类型  。具体不再细讲，可以自行查询。
RDD中的&lt;code&gt;map&lt;/code&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   * Return a new RDD by applying a function to all elements of this RDD.
   */
  def map[U: ClassTag](f: T =&amp;gt; U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (_, _, iter) =&amp;gt; iter.map(cleanF))
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看出，这里的&lt;code&gt;map&lt;/code&gt;首先创建了一个&lt;code&gt;MapPartitionsRDD&lt;/code&gt;并生成可迭代对象&lt;code&gt;iter&lt;/code&gt;,然后调用了Scala的&lt;code&gt;map&lt;/code&gt;函数处理&lt;code&gt;iter&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论
&lt;/h2&gt;&lt;p&gt;Scala里的&lt;code&gt;map&lt;/code&gt;函数首先定义在&lt;code&gt;scala.collection&lt;/code&gt;里，然后子类（&lt;code&gt;List&lt;/code&gt;、&lt;code&gt;Set&lt;/code&gt;）重写父类的函数。因为Scala里的类型是隐式的，并且查看源代码在一个子类里也只发现了一个&lt;code&gt;map&lt;/code&gt;函数，所以Scala里的&lt;code&gt;map&lt;/code&gt;并没有重载，而是通过定义父类，子类重写父类函数的方法实现对不同数据结构的操作。
Spark里的&lt;code&gt;map&lt;/code&gt;是对RDD进行操作的算子，实际使用了可迭代对象来调用Scala中的&lt;code&gt;map&lt;/code&gt;函数。算子本身的定义就是对RDD操作的函数，所以算子应该也可以被称为是函数，但是为了区分Scala中的函数，所以使用了不同的名字。&lt;/p&gt;
&lt;h2 id=&#34;补充&#34;&gt;补充
&lt;/h2&gt;&lt;p&gt;Spark中的算子并非全都有同名函数，原因可以从RDD的原理上分析。
行动算子需要进行&lt;code&gt;shuffle&lt;/code&gt;操作，在&lt;code&gt;shuffle&lt;/code&gt;时需要按键分区，对每个分区进行操作后输出。Scala中并没有&lt;code&gt;Shuffle&lt;/code&gt;操作，所以行动算子没有同名函数。
而转换算子是生成RDD或者将RDD转换成另外的RDD，Scala本身也有将&lt;code&gt;collection&lt;/code&gt;转换为&lt;code&gt;collection&lt;/code&gt;的函数，并且转换算子本身就调用了Scala的函数，所以有同名的也正常。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
