<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Spark on rustWood</title>
        <link>https://rusthx.github.io/tags/spark/</link>
        <description>Recent content in Spark on rustWood</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>rustWood</copyright>
        <lastBuildDate>Mon, 09 Dec 2024 22:01:04 +0800</lastBuildDate><atom:link href="https://rusthx.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Spark源码学习 Shuffle</title>
        <link>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/</link>
        <pubDate>Mon, 09 Dec 2024 22:01:04 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;作者由于水平问题，文中也许有一些错误遗漏的地方，欢迎联系指正(&lt;code&gt;2024087171@qq.com&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考资料：https://blog.csdn.net/weixin_42868529/article/details/84622803&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Shuffle 过程本质上都是将 Map 端获得的数据使用分区器进行划分，并将数据发送给对应的 Reducer 的过程。&lt;/p&gt;
&lt;p&gt;前一个stage的ShuffleMapTask进行shuffle write，把数据存储在blockManager上面，并且把数据元信息上报到dirver的mapOutTarck组件中，下一个stage根据数据位置源信息，进行shuffle read，拉取上一个stage的输出数据&lt;/p&gt;
&lt;h2 id=&#34;hadoopmapreduce-shuffle&#34;&gt;Hadoop(MapReduce) shuffle
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考资料：尚硅谷Hadoop相关课程&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;MapReduce的shuffle机制：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MapTask收集我们的map()方法输出的kv对，放到环形缓冲区中&lt;/li&gt;
&lt;li&gt;从环形缓冲区不断溢写本地磁盘文件，可能会溢出多个文件&lt;/li&gt;
&lt;li&gt;多个溢出文件会被合并成大的溢出文件&lt;/li&gt;
&lt;li&gt;在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序&lt;/li&gt;
&lt;li&gt;ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据&lt;/li&gt;
&lt;li&gt;ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）&lt;/li&gt;
&lt;li&gt;合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/1.png&#34;
	width=&#34;1058&#34;
	height=&#34;513&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/1_hu3767354565587542623.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/1_hu10364141614106133650.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;494px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;spark-shuffle详细机制&#34;&gt;Spark shuffle详细机制
&lt;/h2&gt;&lt;p&gt;Spark的shuffle机制：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;前提条件：Spark的代码在运行到action算子时触发任务（job），然后DAGscheduler按算子间的血缘（依赖关系）划分形成DAG图（有向无环图），
有shuffle操作的依赖关系称为宽依赖，没有的称为窄依赖。DAGscheduler按宽依赖将任务划分为多个stage，stage的数量就等于宽依赖数量+1。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;根据 spark.shuffle.manager 设置，SparkEnv 会在driver和每个executor上创建一个 ShuffleManager。 driver在其中注册shuffle，executor（或在driver中本地运行的任务）可以要求读写数据。&lt;/li&gt;
&lt;li&gt;前一个stage的ShuffleMapTask将mapTaskID和partitions传入sortShuffleManager，调用getWriter（）方法后，首先会判断是否需要对计算结果进行聚合，然后将最终结果按照不同的 reduce 端进行区分，返回writeHandle,
根据不同的writeHandle选择不同的writer（&lt;code&gt;UnsafeShuffleWriter&lt;/code&gt;、&lt;code&gt;BypassMergeSortShuffleWriter&lt;/code&gt;、&lt;code&gt;SortShuffleWriter&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;writer将数据写入到executor的blockManager中&lt;/li&gt;
&lt;li&gt;shuffleManager为后一个stage创建一个&lt;code&gt;BlockStoreShuffleReader&lt;/code&gt;，根据位置信息（&lt;code&gt;startMapIndex, endMapIndex, startPartition, endPartition&lt;/code&gt;）拉取blockManger中的数据，根据数据的 Key 进行聚合，然后判断是否需要排序，最后生成新的 RDD。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/2.png&#34;
	width=&#34;1920&#34;
	height=&#34;1030&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/2_hu3537821498809195093.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-shuffle/2_hu9124599422288604458.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;186&#34;
		data-flex-basis=&#34;447px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  
  /**
   * Obtains a [[ShuffleHandle]] to pass to tasks.
   */
  override def registerShuffle[K, V, C](
      shuffleId: Int,
      dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {
    if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) {
      // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don&#39;t
      // need map-side aggregation, then write numPartitions files directly and just concatenate
      // them at the end. This avoids doing serialization and deserialization twice to merge
      // together the spilled files, which would happen with the normal code path. The downside is
      // having multiple files open at a time and thus more memory allocated to buffers.
      new BypassMergeSortShuffleHandle[K, V](
        shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
    } else if (SortShuffleManager.canUseSerializedShuffle(dependency)) {
      // Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:
      new SerializedShuffleHandle[K, V](
        shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
    } else {
      // Otherwise, buffer map outputs in a deserialized form:
      new BaseShuffleHandle(shuffleId, dependency)
    }
  }

  /**
   * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive) to
   * read from a range of map outputs(startMapIndex to endMapIndex-1, inclusive).
   * If endMapIndex=Int.MaxValue, the actual endMapIndex will be changed to the length of total map
   * outputs of the shuffle in `getMapSizesByExecutorId`.
   *
   * Called on executors by reduce tasks.
   */
  override def getReader[K, C](
      handle: ShuffleHandle,
      startMapIndex: Int,
      endMapIndex: Int,
      startPartition: Int,
      endPartition: Int,
      context: TaskContext,
      metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C] = {
    val baseShuffleHandle = handle.asInstanceOf[BaseShuffleHandle[K, _, C]]
    val (blocksByAddress, canEnableBatchFetch) =
      if (baseShuffleHandle.dependency.isShuffleMergeFinalizedMarked) {
        val res = SparkEnv.get.mapOutputTracker.getPushBasedShuffleMapSizesByExecutorId(
          handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)
        (res.iter, res.enableBatchFetch)
      } else {
        val address = SparkEnv.get.mapOutputTracker.getMapSizesByExecutorId(
          handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)
        (address, true)
      }
    new BlockStoreShuffleReader(
      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], blocksByAddress, context, metrics,
      shouldBatchFetch =
        canEnableBatchFetch &amp;amp;&amp;amp; canUseBatchFetch(startPartition, endPartition, context))
  }

  /** Get a writer for a given partition. Called on executors by map tasks. */
  override def getWriter[K, V](
      handle: ShuffleHandle,
      mapId: Long,
      context: TaskContext,
      metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V] = {
    val mapTaskIds = taskIdMapsForShuffle.computeIfAbsent(
      handle.shuffleId, _ =&amp;gt; new OpenHashSet[Long](16))
    mapTaskIds.synchronized { mapTaskIds.add(mapId) }
    val env = SparkEnv.get
    handle match {
      case unsafeShuffleHandle: SerializedShuffleHandle[K @unchecked, V @unchecked] =&amp;gt;
        new UnsafeShuffleWriter(
          env.blockManager,
          context.taskMemoryManager(),
          unsafeShuffleHandle,
          mapId,
          context,
          env.conf,
          metrics,
          shuffleExecutorComponents)
      case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =&amp;gt;
        new BypassMergeSortShuffleWriter(
          env.blockManager,
          bypassMergeSortHandle,
          mapId,
          env.conf,
          metrics,
          shuffleExecutorComponents)
      case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =&amp;gt;
        new SortShuffleWriter(other, mapId, context, shuffleExecutorComponents)
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;与hadoopmapreduce-shuffle的区别&#34;&gt;与Hadoop(MapReduce) shuffle的区别
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考：https://zhuanlan.zhihu.com/p/136466667&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;功能上，MR的shuffle和Spark的shuffle是没啥区别的，都是对Map端的数据进行分区，要么聚合排序，要么不聚合排序，然后Reduce端或者下一个调度阶段进行拉取数据，完成map端到reduce端的数据传输功能。&lt;/li&gt;
&lt;li&gt;方案上，有很大的区别，MR的shuffle是基于合并排序的思想，在数据进入reduce端之前，都会进行sort，为了方便后续的reduce端的全局排序，而Spark的shuffle是可选择的聚合，特别是1.2之后，需要通过调用特定的算子才会触发排序聚合的功能。&lt;/li&gt;
&lt;li&gt;流程上，MR的Map端和Reduce区分非常明显，两块涉及到操作也是各司其职，而Spark的RDD是内存级的数据转换，不落盘，所以没有明确的划分，只是区分不同的调度阶段，不同的算子模型。&lt;/li&gt;
&lt;li&gt;数据拉取，MR的reduce是直接拉取Map端的分区数据，而Spark是根据MapId和TaskContext读取，而且是在action触发的时候才会拉取数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;shufflewriter及其选择策略&#34;&gt;ShuffleWriter及其选择策略
&lt;/h2&gt;&lt;h3 id=&#34;unsafeshufflewriter&#34;&gt;UnsafeShuffleWriter
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@VisibleForTesting
public void write(Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {
  write(JavaConverters.asScalaIteratorConverter(records).asScala());
}
@Override
public void write(scala.collection.Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {
    // Keep track of success so we know if we encountered an exception
    // We do this rather than a standard try/catch/re-throw to handle
    // generic throwables.
    boolean success = false;
    try {
      while (records.hasNext()) {
        insertRecordIntoSorter(records.next());
      }
      closeAndWriteOutput();
      success = true;
    } finally {
      if (sorter != null) {
        try {
          sorter.cleanupResources();
        } catch (Exception e) {
          // Only throw this error if we won&#39;t be masking another
          // error.
          if (success) {
            throw e;
          } else {
            logger.error(&amp;quot;In addition to a failure during writing, we failed during &amp;quot; +
                         &amp;quot;cleanup.&amp;quot;, e);
          }
        }
      }
    }
  }

@VisibleForTesting
void insertRecordIntoSorter(Product2&amp;lt;K, V&amp;gt; record) throws IOException {
  assert(sorter != null);
  final K key = record._1();
  final int partitionId = partitioner.getPartition(key);
  serBuffer.reset();
  serOutputStream.writeKey(key, OBJECT_CLASS_TAG);
  serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);
  serOutputStream.flush();
  final int serializedRecordSize = serBuffer.size();
  assert (serializedRecordSize &amp;gt; 0);
  sorter.insertRecord(
    serBuffer.getBuf(), Platform.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当shuffle后的分区数小于等于&lt;code&gt;sortShuffleManager&lt;/code&gt;的最大分区数时，进行&lt;code&gt;unsafeShuffle&lt;/code&gt;。主要步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将内存中的对象通过Java可迭代对象转换器转换为Scala的可迭代对象（并没有进行序列化相关操作，只是为了兼容性）&lt;/li&gt;
&lt;li&gt;判断排序器（&lt;code&gt;sorter&lt;/code&gt;）是否为空，使用分区器(&lt;code&gt;partitioner&lt;/code&gt;)确认记录所属分区&lt;/li&gt;
&lt;li&gt;重置序列化缓冲区，遍历可迭代对象（&lt;code&gt;iterator&lt;/code&gt;），将记录的键和值依次序列化后写入，并刷写脏页，确保数据写入缓冲区&lt;/li&gt;
&lt;li&gt;将序列化记录插入到排序器中。sorter负责按分区组织记录，并&lt;font color=red&gt;可能&lt;/font&gt;在每个分区内对其进行排序。方法使用缓冲区、偏移量、大小和分区ID来正确放置记录。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;sorter什么时候排序？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shuffle操作的需求（最高优先级）：
如果上层的Spark操作（如sortByKey）要求数据在每个分区内有序，那么排序器会对数据进行排序。
对于不需要排序的操作（如groupByKey），排序器可能只负责将数据按分区组织，而不进行排序。&lt;/li&gt;
&lt;li&gt;排序器的类型和实现：
Spark中有多种排序器实现，例如UnsafeExternalSorter。这些排序器可以根据需要对数据进行排序。
如果排序器的实现支持排序，并且配置要求排序，那么数据会在每个分区内被排序。&lt;/li&gt;
&lt;li&gt;配置和优化（最低优先级）：
Spark的某些配置参数可以影响排序行为。例如，spark.shuffle.sort.bypassMergeThreshold可以决定在某些情况下是否绕过排序。
在某些优化场景下，为了提高性能，Spark可能会选择不进行排序。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bypassmergesortshufflewriter&#34;&gt;BypassMergeSortShuffleWriter
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@Override
  public void write(Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {
    assert (partitionWriters == null);
    ShuffleMapOutputWriter mapOutputWriter = shuffleExecutorComponents
        .createMapOutputWriter(shuffleId, mapId, numPartitions);
    try {
      if (!records.hasNext()) {
        partitionLengths = mapOutputWriter.commitAllPartitions(
          ShuffleChecksumHelper.EMPTY_CHECKSUM_VALUE).getPartitionLengths();
        mapStatus = MapStatus$.MODULE$.apply(
          blockManager.shuffleServerId(), partitionLengths, mapId);
        return;
      }
      final SerializerInstance serInstance = serializer.newInstance();
      final long openStartTime = System.nanoTime();
      partitionWriters = new DiskBlockObjectWriter[numPartitions];
      partitionWriterSegments = new FileSegment[numPartitions];
      for (int i = 0; i &amp;lt; numPartitions; i++) {
        final Tuple2&amp;lt;TempShuffleBlockId, File&amp;gt; tempShuffleBlockIdPlusFile =
            blockManager.diskBlockManager().createTempShuffleBlock();
        final File file = tempShuffleBlockIdPlusFile._2();
        final BlockId blockId = tempShuffleBlockIdPlusFile._1();
        DiskBlockObjectWriter writer =
          blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);
        if (partitionChecksums.length &amp;gt; 0) {
          writer.setChecksum(partitionChecksums[i]);
        }
        partitionWriters[i] = writer;
      }
      // Creating the file to write to and creating a disk writer both involve interacting with
      // the disk, and can take a long time in aggregate when we open many files, so should be
      // included in the shuffle write time.
      writeMetrics.incWriteTime(System.nanoTime() - openStartTime);

      while (records.hasNext()) {
        final Product2&amp;lt;K, V&amp;gt; record = records.next();
        final K key = record._1();
        partitionWriters[partitioner.getPartition(key)].write(key, record._2());
      }

      for (int i = 0; i &amp;lt; numPartitions; i++) {
        try (DiskBlockObjectWriter writer = partitionWriters[i]) {
          partitionWriterSegments[i] = writer.commitAndGet();
        }
      }

      partitionLengths = writePartitionedData(mapOutputWriter);
      mapStatus = MapStatus$.MODULE$.apply(
        blockManager.shuffleServerId(), partitionLengths, mapId);
    } catch (Exception e) {
      try {
        mapOutputWriter.abort(e);
      } catch (Exception e2) {
        logger.error(&amp;quot;Failed to abort the writer after failing to write map output.&amp;quot;, e2);
        e.addSuppressed(e2);
      }
      throw e;
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;BypassMergeSortShuffleWriter，专门用于处理小规模的 shuffle 操作。它通过绕过排序步骤来提高性能，适用于分区数较少的情况。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化和检查：确保在写入开始时，partitionWriters 为空，防止重复初始化，然后创建一个用于写入 shuffle 输出的对象(&lt;code&gt;ShuffleMapOutputWriter&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;处理空记录集：如果没有记录需要写入，更新 map 状态,直接向blockkManager提交所有分区并返回。&lt;/li&gt;
&lt;li&gt;初始化序列化和写入器：创建一个新的序列化实例，初始化分区磁盘写入器数组(&lt;code&gt;DiskBlockObjectWriter[numPartitions]&lt;/code&gt;)，初始化分区文件段数组。&lt;/li&gt;
&lt;li&gt;创建分区写入器：循环遍历每个分区，创建临时的 shuffle 块和对应的磁盘写入器。为每个分区创建一个磁盘写入器。&lt;/li&gt;
&lt;li&gt;写入记录： 遍历所有记录，根据键的分区，将记录写入对应的分区写入器。&lt;/li&gt;
&lt;li&gt;提交和获取分区数据：遍历每个分区，提交写入的数据并获取文件段。提交写入并获取文件段信息。&lt;/li&gt;
&lt;li&gt;写入分区数据和更新状态： 将分区数据写入输出。更新 map 状态。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt; 文件段信息通常指的是每个分区在磁盘上的物理存储信息，包括：文件路径(数据在磁盘上的具体存储位置)、偏移量(数据在文件中的起始位置)、
长度(数据的字节长度)。&lt;/p&gt;
&lt;p&gt; 更新 map 状态是指在 shuffle 写入完成后，更新 Spark 的内部状态以反映当前任务的输出状态。具体来说：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MapStatus: 这是 Spark 用于跟踪每个 map 任务输出状态的对象（在一个 stage 中，map 任务完成后生成的输出信息。这些信息用于指导后续的 shuffle 操作，确保数据能够正确地传递到下一个 stage 的 reduce 任务中。）。它包含了每个分区的数据长度信息。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;源码中介绍如下：&lt;/p&gt;
&lt;p&gt;Result returned by a ShuffleMapTask to a scheduler. Includes the block manager address that the task has shuffle files stored on as well as the sizes of outputs for each reducer, for passing on to the reduce tasks.&lt;/p&gt;
&lt;p&gt;ShuffleMapTask 返回给调度程序的结果。 包括该任务存储了 Shuffle 文件的blockManager地址，以及给每个reducer的输出文件大小，以便传递给reduce。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;blockManager.shuffleServerId(): 这是当前节点的标识符，用于标识数据存储的位置。&lt;/li&gt;
&lt;li&gt;partitionLengths: 这是一个数组，包含了每个分区的数据长度。
更新 map 状态的目的是为了让 Spark 的调度器和后续的 reduce 任务知道每个分区的数据存储在哪里，以及每个分区的数据大小。这对于后续的 shuffle 读取操作至关重要，因为 reduce 任务需要知道从哪里读取数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;也即是，前一个stage写数据确定分区是通过分区器，而后一个stage读数据确定分区是通过&lt;code&gt;MapStatus&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;sortshufflewriter&#34;&gt;SortShuffleWriter
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  override def write(records: Iterator[Product2[K, V]]): Unit = {
    sorter = if (dep.mapSideCombine) {
      new ExternalSorter[K, V, C](
        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    } else {
      // In this case we pass neither an aggregator nor an ordering to the sorter, because we don&#39;t
      // care whether the keys get sorted in each partition; that will be done on the reduce side
      // if the operation being run is sortByKey.
      new ExternalSorter[K, V, V](
        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)
    }
    sorter.insertAll(records)

    // Don&#39;t bother including the time to open the merged output file in the shuffle write time,
    // because it just opens a single file, so is typically too fast to measure accurately
    // (see SPARK-3570).
    val mapOutputWriter = shuffleExecutorComponents.createMapOutputWriter(
      dep.shuffleId, mapId, dep.partitioner.numPartitions)
    sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter)
    partitionLengths = mapOutputWriter.commitAllPartitions(sorter.getChecksums).getPartitionLengths
    mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths, mapId)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;初始化一个sorter，如果map侧（上一个stage）需要聚合，那么就创建带&lt;code&gt;aggregater&lt;/code&gt;并按键排序的sorter，否则创建一个不带&lt;code&gt;aggregater&lt;/code&gt;的sorter;&lt;/li&gt;
&lt;li&gt;创建一个shuffleMapOutputWriter，写入器开启一个输出流，然后将给定reduce任务分区id的字节流持久化;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Creates a writer that can open an output stream to persist bytes targeted for a given reduce partition id.
The chunk corresponds to bytes in the given reduce partition. This will not be called twice for the same partition within any given map task. The partition identifier will be in the range of precisely 0 (inclusive) to numPartitions (exclusive), where numPartitions was provided upon the creation of this map output writer via ShuffleExecutorComponents.createMapOutputWriter(int, long, int).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;将记录写入sorter中，在sorter中经过处理后写出分区器分区后的数据&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;虽然叫sorter，但是如果map侧没有排序的需求，不会进行排序，如果map侧没有聚合的需求，也不会进行聚合。&lt;/p&gt;
&lt;p&gt;什么时候会排序：sortByKey,sortBy&lt;/p&gt;
&lt;p&gt;补充：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;orderBy(SparkSQL中，RDD没有这个api):对 DataFrame 或 Dataset 进行全局排序。
类似于 SQL 中的 ORDER BY，会对整个数据集进行排序。
需要进行 shuffle 操作，以确保全局排序。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;sortWithinPartitions:数据被写入 ExternalSorter，在内存中进行排序。
如果数据量超过内存限制，ExternalSorter 会将部分数据溢出到磁盘，并在需要时进行归并排序。
最终，排序后的数据被取出并生成新的 RDD。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;什么时候会聚合：reduceByKey,combineByKey,aggregateByKey(map侧和reduce侧都进行聚合，支持不同的聚合操作),foldByKey(map侧和reduce侧聚合操作相同时等同于aggregateByKey)&lt;/p&gt;
&lt;p&gt;注：map侧也叫分区内，reduce侧也叫分区间&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;向blockManager提交分区数据，并更新&lt;code&gt;MapStatus&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;选择策略&#34;&gt;选择策略
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  override def getWriter[K, V](
      handle: ShuffleHandle,
      mapId: Long,
      context: TaskContext,
      metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V] = {
    val mapTaskIds = taskIdMapsForShuffle.computeIfAbsent(
      handle.shuffleId, _ =&amp;gt; new OpenHashSet[Long](16))
    mapTaskIds.synchronized { mapTaskIds.add(mapId) }
    val env = SparkEnv.get
    handle match {
      case unsafeShuffleHandle: SerializedShuffleHandle[K @unchecked, V @unchecked] =&amp;gt;
        new UnsafeShuffleWriter(
          env.blockManager,
          context.taskMemoryManager(),
          unsafeShuffleHandle,
          mapId,
          context,
          env.conf,
          metrics,
          shuffleExecutorComponents)
      case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =&amp;gt;
        new BypassMergeSortShuffleWriter(
          env.blockManager,
          bypassMergeSortHandle,
          mapId,
          env.conf,
          metrics,
          shuffleExecutorComponents)
      case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =&amp;gt;
        new SortShuffleWriter(other, mapId, context, shuffleExecutorComponents)
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; 由源码可以看出，Spark的shuffle选择&lt;code&gt;shuffleWriter&lt;/code&gt;的策略是匹配shuffleHandle，依次匹配SerializedShuffleHandle、BypassMergeSortShuffleHandle、BaseShuffleHandle。&lt;/p&gt;
&lt;p&gt;而shuffleHandle的类型如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SerializedShuffleHandle:适用于需要高效序列化的场景。通常与 UnsafeShuffleWriter 搭配使用。
这种 handle 主要用于 Spark 的 Tungsten 引擎优化路径，利用了 Spark 的内存管理和序列化优化。&lt;/li&gt;
&lt;li&gt;BypassMergeSortShuffleHandle:适用于小规模 shuffle 操作，特别是当分区数较少时。
通常与 BypassMergeSortShuffleWriter 搭配使用。
这种 handle 通过绕过排序步骤来提高性能，适合分区数小于 spark.shuffle.sort.bypassMergeThreshold 的情况。&lt;/li&gt;
&lt;li&gt;BaseShuffleHandle:这是一个通用的 handle 类型，适用于大多数 shuffle 操作。
通常与 SortShuffleWriter 搭配使用。
适合需要排序的 shuffle 操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;选择策略&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;依赖类型:
Spark 的 shuffle 依赖（ShuffleDependency）在 RDD 的转换操作中被创建。
依赖类型决定了 shuffle 的处理方式。例如，ShuffleDependency 中的 serializer 和 partitioner 会影响 ShuffleHandle 的选择。&lt;/li&gt;
&lt;li&gt;配置参数:
spark.shuffle.sort.bypassMergeThreshold: 这个参数决定了是否使用 BypassMergeSortShuffleHandle。如果分区数小于这个阈值，Spark 会选择 BypassMergeSortShuffleHandle。
spark.shuffle.manager: 这个参数可以配置 shuffle 的管理方式，影响 ShuffleHandle 的选择。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;相关代码位于&lt;code&gt;org.apache.spark.internal.config.package.scala&lt;/code&gt;1497行&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  private[spark] val SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD =
    ConfigBuilder(&amp;quot;spark.shuffle.sort.bypassMergeThreshold&amp;quot;)
      .doc(&amp;quot;In the sort-based shuffle manager, avoid merge-sorting data if there is no &amp;quot; +
        &amp;quot;map-side aggregation and there are at most this many reduce partitions&amp;quot;)
      .version(&amp;quot;1.1.1&amp;quot;)
      .intConf
      .createWithDefault(200)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;数据规模和分区数:
小规模数据和少量分区通常会使用 BypassMergeSortShuffleHandle。
大规模数据和大量分区通常会使用 SerializedShuffleHandle 或 BaseShuffleHandle。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;shufflereader&#34;&gt;ShuffleReader
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  
  /**
   * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive) to
   * read from a range of map outputs(startMapIndex to endMapIndex-1, inclusive).
   * If endMapIndex=Int.MaxValue, the actual endMapIndex will be changed to the length of total map
   * outputs of the shuffle in `getMapSizesByExecutorId`.
   *
   * Called on executors by reduce tasks.
   */
  override def getReader[K, C](
      handle: ShuffleHandle,
      startMapIndex: Int,
      endMapIndex: Int,
      startPartition: Int,
      endPartition: Int,
      context: TaskContext,
      metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C] = {
    val baseShuffleHandle = handle.asInstanceOf[BaseShuffleHandle[K, _, C]]
    val (blocksByAddress, canEnableBatchFetch) =
      if (baseShuffleHandle.dependency.isShuffleMergeFinalizedMarked) {
        val res = SparkEnv.get.mapOutputTracker.getPushBasedShuffleMapSizesByExecutorId(
          handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)
        (res.iter, res.enableBatchFetch)
      } else {
        val address = SparkEnv.get.mapOutputTracker.getMapSizesByExecutorId(
          handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)
        (address, true)
      }
    new BlockStoreShuffleReader(
      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], blocksByAddress, context, metrics,
      shouldBatchFetch =
        canEnableBatchFetch &amp;amp;&amp;amp; canUseBatchFetch(startPartition, endPartition, context))
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spark的shuffleReader比起shuffleWriter来说就简单很多，只有一个&lt;code&gt;BlockStoreShuffleReader&lt;/code&gt;子类。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ShuffleHandle 转换：ShuffleHandle 被转换为 BaseShuffleHandle，以便访问 shuffle 依赖的详细信息。&lt;/li&gt;
&lt;li&gt;获取块地址和批量获取能力：
代码检查是否完成了 shuffle 合并（isShuffleMergeFinalizedMarked）。
如果合并已完成，使用推送式 shuffle 方法 getPushBasedShuffleMapSizesByExecutorId 获取块大小和批量获取能力。
否则，使用常规方法 getMapSizesByExecutorId 获取块地址。&lt;/li&gt;
&lt;li&gt;创建 BlockStoreShuffleReader：
使用获取的块地址和批量获取能力创建 BlockStoreShuffleReader。
shouldBatchFetch 参数决定是否启用批量获取，取决于 canEnableBatchFetch 和 canUseBatchFetch 的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;BlockStoreShuffleReader&lt;/code&gt;读数据流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化&lt;/li&gt;
&lt;li&gt;获取数据块：
创建&lt;code&gt;ShuffleBlockFetcherIterator&lt;/code&gt;实例，用于从其他节点获取数据块。
该迭代器负责处理数据块的网络传输、反序列化和错误处理。
支持批量获取连续的数据块（如果条件允许），以提高网络传输效率。&lt;/li&gt;
&lt;li&gt;反序列化：
使用&lt;code&gt;serializerManager.wrapStream&lt;/code&gt;包装从网络获取的输入流。
使用&lt;code&gt;serializerInstance.deserializeStream&lt;/code&gt;将流反序列化为键值对迭代器。&lt;/li&gt;
&lt;li&gt;聚合(可选)：
如果&lt;code&gt;dep.aggregator&lt;/code&gt;被定义，使用聚合器对数据进行聚合。
如果&lt;code&gt;mapSideCombine&lt;/code&gt;为 true，则数据已经在 map 端部分聚合，使用 combineCombinersByKey。
否则，使用 combineValuesByKey 进行聚合。&lt;/li&gt;
&lt;li&gt;排序(可选)：
如果&lt;code&gt;dep.keyOrdering&lt;/code&gt;被定义，使用 ExternalSorter 对数据进行排序。
ExternalSorter 可以处理大规模数据集，通过将数据溢出到磁盘来进行排序。&lt;/li&gt;
&lt;li&gt;迭代器包装：
使用 InterruptibleIterator 包装最终的结果迭代器，以支持任务取消。
确保在任务取消时能够及时中断数据处理。&lt;/li&gt;
&lt;li&gt;结果返回：
返回一个 Iterator[Product2[K, C]]，包含所有读取和处理后的键值对&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Spark源码学习 Join策略</title>
        <link>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/</link>
        <pubDate>Sun, 17 Nov 2024 23:55:36 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/</guid>
        <description>&lt;p&gt;学习源码所用的Spark的版本是Spark3.3.2_2.12(Scala2.12写的Spark3.3.2)&lt;/p&gt;
&lt;h2 id=&#34;类别&#34;&gt;类别
&lt;/h2&gt;&lt;p&gt;Spark底层有五种join实现方式&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/1.png&#34;
	width=&#34;1920&#34;
	height=&#34;1030&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/1_hu16373391895739476467.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/1_hu16192374523649459711.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;186&#34;
		data-flex-basis=&#34;447px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;前置介绍hashjoin&#34;&gt;前置介绍：HashJoin
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;参考资料：https://www.6aiq.com/article/1533984288407&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;先来看看这样一条SQL语句：&lt;code&gt;select * from order,item where item.id = order.i_id&lt;/code&gt;，很简单一个Join节点，参与join的两张表是item和order，join key分别是item.id以及order.i_id。现在假设这个Join采用的是hash join算法，整个过程会经历三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;确定Build Table以及Probe Table：这个概念比较重要，Build Table使用join key构建Hash Table，而Probe Table使用join key进行探测，探测成功就可以join在一起。通常情况下，小表会作为Build Table，大表作为Probe Table。此事例中item为Build Table，order为Probe Table。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;构建Hash Table：依次读取Build Table（item）的数据，对于每一行数据根据join key（item.id）进行hash，hash到对应的Bucket，生成hash table中的一条记录。数据缓存在内存中，如果内存放不下需要dump到外存。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;探测：再依次扫描Probe Table（order）的数据，使用相同的hash函数映射Hash Table中的记录，映射成功之后再检查join条件（item.id = order.i_id），如果匹配成功就可以将两者join在一起。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/2.png&#34;
	width=&#34;640&#34;
	height=&#34;392&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/2_hu12648101483709186989.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/2_hu10412664838980808260.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;163&#34;
		data-flex-basis=&#34;391px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;基本流程可以参考上图，这里有两个小问题需要关注：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;hash join性能如何？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;很显然，hash join基本都只扫描两表一次，可以认为o(a+b)，较之最极端的笛卡尔集运算a*b，效率大大提升。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;为什么Build Table选择小表？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;道理很简单，因为构建的Hash Table最好能全部加载在内存，效率最高；这也决定了hash join算法只适合至少一个小表的join场景，对于两个大表的join场景并不适用。&lt;/p&gt;
&lt;p&gt;hash join是传统数据库中的单机join算法，在分布式环境下需要经过一定的分布式改造，就是尽可能利用分布式计算资源进行并行化计算，提高总体效率。hash join分布式改造一般有两种经典方案：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;broadcast hash join&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;将其中一张小表广播分发到另一张大表所在的分区节点上，分别并发地与其上的分区记录进行hash join。broadcast适用于小表很小，可以直接广播的场景。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;shuffled hash join&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一旦小表数据量较大，此时就不再适合进行广播分发。这种情况下，可以根据join key相同必然分区相同的原理，将两张表分别按照join key进行重新组织分区，这样就可以将join分而治之，划分为很多小join，充分利用集群资源并行化。&lt;/p&gt;
&lt;p&gt;下面分别进行详细讲解。&lt;/p&gt;
&lt;h3 id=&#34;broadcasthashjoinexec&#34;&gt;BroadcastHashJoinExec
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;broadcast阶段：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;将小表广播分发到大表所在的所有主机。广播算法可以有很多，最简单的是先发给driver，driver再统一分发给所有executor；要不就是基于BitTorrent的TorrentBroadcast。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;hash join阶段：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在每个executor上执行单机版hash join，小表映射，大表试探。
&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/3.png&#34;
	width=&#34;640&#34;
	height=&#34;351&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/3_hu5878782776316041114.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/3_hu14107248078688552454.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;182&#34;
		data-flex-basis=&#34;437px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;SparkSQL规定broadcast hash join执行的基本条件为被广播小表必须小于参数&lt;code&gt;spark.sql.autoBroadcastJoinThreshold&lt;/code&gt;，默认为10M。
源码位于&lt;code&gt;org.apache.spark.sql.internal.SQLConf.scala&lt;/code&gt;,没找到SQLConf文件可以在BaseSessionStateBuilder里找到conf引用，然后&lt;code&gt;CTRL+左键&lt;/code&gt;查看源码。
&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/4.png&#34;
	width=&#34;1920&#34;
	height=&#34;1030&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/4_hu12917825796720597905.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/4_hu12430590560678393374.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;186&#34;
		data-flex-basis=&#34;447px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;shuffledhashjoinexec&#34;&gt;ShuffledHashJoinExec
&lt;/h3&gt;&lt;p&gt;在大数据条件下如果一张表很小，执行join操作最优的选择无疑是broadcast hash join，效率最高。但是一旦小表数据量增大，广播所需内存、带宽等资源必然就会太大，broadcast hash join就不再是最优方案。此时可以按照join key进行分区，根据key相同必然分区相同的原理，就可以将大表join分而治之，划分为很多小表的join，充分利用集群资源并行化。如下图所示，shuffle hash join也可以分为两步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;shuffle阶段:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;分别将两个表按照join key进行分区，将相同join key的记录重分布到同一节点，两张表的数据会被重分布到集群中所有节点。这个过程称为shuffle。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;hash join阶段：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;每个分区节点上的数据单独执行单机hash join算法。
&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/5.png&#34;
	width=&#34;640&#34;
	height=&#34;222&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/5_hu13792204227143600783.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/5_hu13175992835024326903.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;288&#34;
		data-flex-basis=&#34;691px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;sortmergejoinexec&#34;&gt;SortMergeJoinExec
&lt;/h3&gt;&lt;p&gt;SparkSQL对两张大表join采用了全新的算法——sort merge join，整个过程分为三个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;shuffle阶段：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;sort阶段：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对单个分区节点的两表数据，分别进行排序。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;merge阶段：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边。如下图所示：
&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/6.png&#34;
	width=&#34;395&#34;
	height=&#34;237&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/6_hu8829640617333598188.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/6_hu1107606516430444359.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/7.png&#34;
	width=&#34;640&#34;
	height=&#34;330&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/7_hu268948268638298732.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-join%E7%AD%96%E7%95%A5/7_hu6909571737077120363.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;cartesianproductexec&#34;&gt;CartesianProductExec:
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;初始化：两个数据集被加载并分区。&lt;/li&gt;
&lt;li&gt;分区组合：每个分区的所有元素与另一个分区的所有元素组合。这在逻辑上类似于嵌套循环。&lt;/li&gt;
&lt;li&gt;生成所有组合：对每一对元素生成一个元组，形成笛卡尔积。结果集的大小为两个数据集大小的乘积。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;笛卡尔积会占用大量内存，使用笛卡尔积之前最好先过滤掉无用数据，其中一张表为极小表时建议广播。&lt;/p&gt;
&lt;h3 id=&#34;broadcastnestedloopjoinexec&#34;&gt;BroadcastNestedLoopJoinExec
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;broadcast阶段：将小表的数据复制并广播到大表分区数据所在的每个执行节点。&lt;/li&gt;
&lt;li&gt;嵌套循环连接：
&lt;ul&gt;
&lt;li&gt;在每个执行节点上，对于分区中的每一行，逐行扫描广播的小表。&lt;/li&gt;
&lt;li&gt;对每一对行执行连接条件，生成匹配的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;生成结果：将匹配的行组合成结果集。结果在每个节点独立生成，最后输出为完整的连接结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外，Spark对非等值连接的支持只有这种和笛卡尔积（CartesianProduct）。
如果两张表都很大且无法广播，Spark 可能需要通过优化或变通的方法来处理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据过滤：在连接前尽量过滤数据，减少处理的数据量。&lt;/li&gt;
&lt;li&gt;分区和缓存：有效地分区和缓存数据以提高性能。&lt;/li&gt;
&lt;li&gt;自定义 UDF：在某些情况下，使用自定义 UDF 可能会帮助实现复杂的连接逻辑。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;选择join的机制&#34;&gt;选择join的机制
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;相关源码位于&lt;code&gt;org.apache.spark.sql.execution&lt;/code&gt;下的141行&lt;code&gt;JoinSelection&lt;/code&gt;对象&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;根据连接策略提示、等价连接键的可用性和连接关系的大小，选择合适的连接物理计划。 以下是现有的连接策略、其特点和局限性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;broadcast hash join（BHJ）： 仅支持等价连接，而连接键无需可排序。 支持除全外部连接外的所有连接类型。 当广播方较小时，BHJ 通常比其他连接算法执行得更快。 不过，广播表是一种网络密集型操作，在某些情况下可能会导致 OOM 或性能不佳，尤其是当构建/广播方较大时。&lt;/li&gt;
&lt;li&gt;shuffled hash join： 仅支持等价连接，而连接键无需可排序。 支持所有连接类型。 从表中构建哈希映射是一个内存密集型操作，当联编侧很大时可能会导致 OOM。&lt;/li&gt;
&lt;li&gt;shuffle sort merge join（SMJ）： 仅支持等连接，且连接键必须是可排序的。 支持所有连接类型。&lt;/li&gt;
&lt;li&gt;Broadcast nested loop join(广播嵌套循环连接 BNLJ)： 支持等连接和非等连接。 支持所有连接类型，但优化了以下方面的实现： 1）在右外连接中广播左侧；2）在左外、左半、左反或存在连接中广播右侧；3）在类内连接中广播任一侧。 对于其他情况，我们需要多次扫描数据，这可能会相当慢。&lt;/li&gt;
&lt;li&gt;Shuffle-and-replicate nested loop join (洗牌复制嵌套循环连接,又称笛卡尔积连接)： 支持等连接和非等连接。 只支持内同类连接。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;源码中关于等值连接选择join的机制介绍如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;If it is an equi-join, we first look at the join hints w.r.t. the following order:
   1. broadcast hint: pick broadcast hash join if the join type is supported. If both sides
      have the broadcast hints, choose the smaller side (based on stats) to broadcast.
   2. sort merge hint: pick sort merge join if join keys are sortable.
   3. shuffle hash hint: We pick shuffle hash join if the join type is supported. If both
      sides have the shuffle hash hints, choose the smaller side (based on stats) as the
      build side.
   4. shuffle replicate NL hint: pick cartesian product if join type is inner like.

 If there is no hint or the hints are not applicable, we follow these rules one by one:
   1. Pick broadcast hash join if one side is small enough to broadcast, and the join type
      is supported. If both sides are small, choose the smaller side (based on stats)
      to broadcast.
   2. Pick shuffle hash join if one side is small enough to build local hash map, and is
      much smaller than the other side, and `spark.sql.join.preferSortMergeJoin` is false.
   3. Pick sort merge join if the join keys are sortable.
   4. Pick cartesian product if join type is inner like.
   5. Pick broadcast nested loop join as the final solution. It may OOM but we don&#39;t have
      other choice.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;翻译如下：&lt;/p&gt;
&lt;p&gt;如果是等值连接，我们首先按以下顺序查看连接提示(join hints)：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;broadcast hint：如果支持广播散列连接(BHJ)，则选择广播散列连接。
如果双方有广播提示，则选择较小的一方（根据统计信息）进行广播。&lt;/li&gt;
&lt;li&gt;sort merge hint：如果连接键可排序，则选择排序合并连接。&lt;/li&gt;
&lt;li&gt;shuffle hash hint：如果支持连接类型，我们会选择洗牌散列连接。 如果双方都有洗牌散列提示，则选择较小的一方（基于统计信息）作为构建方。
构建侧。&lt;/li&gt;
&lt;li&gt;shuffle replicate NL 提示：如果连接类型是内连接，则选择笛卡尔积。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果没有提示或提示不适用，我们将逐一遵循这些规则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果有一方足够小，可以广播，且连接类型支持，则选择广播散列连接。
支持。 如果两边都很小，则选择较小的一边（根据统计数据）
进行广播。&lt;/li&gt;
&lt;li&gt;如果一方的规模小到足以建立本地哈希映射，并且比另一方小很多，并且支持 &amp;ldquo;space&amp;rdquo;，则选择 &amp;ldquo;洗牌哈希连接&amp;rdquo;。
且 &lt;code&gt;spark.sql.join.preferSortMergeJoin&lt;/code&gt; 为 false。&lt;/li&gt;
&lt;li&gt;如果连接键可排序，则选择排序合并连接。&lt;/li&gt;
&lt;li&gt;如果连接类型为inner join（就是不明写join的join，即类似&lt;code&gt;select * from order,item&lt;/code&gt;），则选择笛卡尔积。&lt;/li&gt;
&lt;li&gt;选择广播嵌套循环连接作为最终解决方案。 可能会出现 OOM，但我们没有
其他选择。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;相关关键源码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def createJoinWithoutHint() = {
  createBroadcastHashJoin(false)
    .orElse(createShuffleHashJoin(false))
    .orElse(createSortMergeJoin())
    .orElse(createCartesianProduct())
    .getOrElse {
      // This join could be very slow or OOM
      val buildSide = getSmallerSide(left, right)
      Seq(joins.BroadcastNestedLoopJoinExec(
        planLater(left), planLater(right), buildSide, joinType, j.condition))
    }
}
        
if (hint.isEmpty) {
  createJoinWithoutHint()
} else {
  createBroadcastHashJoin(true)
    .orElse { if (hintToSortMergeJoin(hint)) createSortMergeJoin() else None }
    .orElse(createShuffleHashJoin(true))
    .orElse { if (hintToShuffleReplicateNL(hint)) createCartesianProduct() else None }
    .getOrElse(createJoinWithoutHint())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;综上所述：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有连接提示时,优先级从高到低：BroadcastHashJoin &amp;gt; SortMergeJoin &amp;gt; ShuffledHashJoin &amp;gt; CartesianProduct&lt;/li&gt;
&lt;li&gt;没有连接提示时，优先级从高到低(join代价从小到大)：BroadcastHashJoin &amp;gt; ShuffledHashJoin &amp;gt; SortMergeJoin &amp;gt; CartesianProduct &amp;gt; BroadcastNestedLoopJoin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：有提示时优先&lt;code&gt;SortMergeJoin&lt;/code&gt;,然后&lt;code&gt;ShuffledHashJoin&lt;/code&gt;；而没有提示时，优先&lt;code&gt;ShuffledHashJoin&lt;/code&gt;,然后&lt;code&gt;SortMergeJoin&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;总结：
数据仓库设计时最好避免大表与大表的join查询，SparkSQL也可以根据内存资源、带宽资源适量将参数spark.sql.autoBroadcastJoinThreshold调大，让更多join实际执行为broadcast hash join。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;大表join极小表(表大小小于10M,可调整&lt;code&gt;spark.sql.autoBroadcastJoinThreshold&lt;/code&gt;参数进行修改)，用BroadcastHashJoin&lt;/li&gt;
&lt;li&gt;大表join小表，用ShuffledHashJoin&lt;/li&gt;
&lt;li&gt;大表join大表，用SortMergeJoin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;非等值连接仅CartesianProduct 和 BroadcastNestedLoopJoin支持，常用BroadcastNestedLoopJoin。&lt;/p&gt;
&lt;h2 id=&#34;补充rdd中的join&#34;&gt;补充：RDD中的join
&lt;/h2&gt;&lt;p&gt;前文中介绍的join都是SparkSQL中join的底层实现，但是在Spark的RDD中，也有一个join函数。
具体实现如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each
   * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and
   * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD.
   */
  def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope {
    this.cogroup(other, partitioner).flatMapValues( pair =&amp;gt;
      for (v &amp;lt;- pair._1.iterator; w &amp;lt;- pair._2.iterator) yield (v, w)
    )
  }

  /**
   * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the
   * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the
   * pair (k, (v, None)) if no elements in `other` have key k. Uses the given Partitioner to
   * partition the output RDD.
   */
  def leftOuterJoin[W](
      other: RDD[(K, W)],
      partitioner: Partitioner): RDD[(K, (V, Option[W]))] = self.withScope {
    this.cogroup(other, partitioner).flatMapValues { pair =&amp;gt;
      if (pair._2.isEmpty) {
        pair._1.iterator.map(v =&amp;gt; (v, None))
      } else {
        for (v &amp;lt;- pair._1.iterator; w &amp;lt;- pair._2.iterator) yield (v, Some(w))
      }
    }
  }

  /**
   * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the
   * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the
   * pair (k, (None, w)) if no elements in `this` have key k. Uses the given Partitioner to
   * partition the output RDD.
   */
  def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner)
      : RDD[(K, (Option[V], W))] = self.withScope {
    this.cogroup(other, partitioner).flatMapValues { pair =&amp;gt;
      if (pair._1.isEmpty) {
        pair._2.iterator.map(w =&amp;gt; (None, w))
      } else {
        for (v &amp;lt;- pair._1.iterator; w &amp;lt;- pair._2.iterator) yield (Some(v), w)
      }
    }
  }

  /**
   * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the
   * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or
   * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each
   * element (k, w) in `other`, the resulting RDD will either contain all pairs
   * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements
   * in `this` have key k. Uses the given Partitioner to partition the output RDD.
   */
  def fullOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner)
      : RDD[(K, (Option[V], Option[W]))] = self.withScope {
    this.cogroup(other, partitioner).flatMapValues {
      case (vs, Seq()) =&amp;gt; vs.iterator.map(v =&amp;gt; (Some(v), None))
      case (Seq(), ws) =&amp;gt; ws.iterator.map(w =&amp;gt; (None, Some(w)))
      case (vs, ws) =&amp;gt; for (v &amp;lt;- vs.iterator; w &amp;lt;- ws.iterator) yield (Some(v), Some(w))
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RDD的join实际是匹配两个RDD,以join为例，遍历(iterator)两个RDD的分区，调用cogroup函数将两个RDD的相同分区联合成一个turple返回。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Spark源码学习 累加器</title>
        <link>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/</link>
        <pubDate>Sat, 09 Nov 2024 20:39:50 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt; 累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在
Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，
传回 Driver 端进行 merge。&lt;/p&gt;
&lt;h2 id=&#34;快速上手&#34;&gt;快速上手
&lt;/h2&gt;&lt;p&gt;数据如下，数据格式为学生姓名，学生课程，课程成绩。要求计算选择了Database课程的人数&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/1.png&#34;
	width=&#34;966&#34;
	height=&#34;650&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/1_hu14818999790699540141.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/1_hu16384632438318607173.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;356px&#34;
	
&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package SprakReview

import org.apache.spark.util.LongAccumulator
import org.apache.spark.{SparkConf, SparkContext}

object AccumulatorTry {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;累加器学习&amp;quot;)
    val sc = new SparkContext(sparkConf)

    val data = sc.textFile(&amp;quot;E:\\TestData\\sparkjob3\\task1\\Data01.txt&amp;quot;)
    val dbCount: LongAccumulator = sc.longAccumulator(&amp;quot;dbCount&amp;quot;)
    data.foreach { line =&amp;gt;
      val course = line.split(&amp;quot;,&amp;quot;)(1)
      if (course == &amp;quot;DataBase&amp;quot;) {
        // 每当课程为&amp;quot;DataBase&amp;quot;时，累加器的值加1
        dbCount.add(1)
      }
    }
    println(&amp;quot;DataBase count: &amp;quot; + dbCount.value)
    //累加器实现查找选择了DataBase课的学生

    sc.stop()
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/2.png&#34;
	width=&#34;1920&#34;
	height=&#34;1030&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/2_hu4940220256607226227.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%B4%AF%E5%8A%A0%E5%99%A8/2_hu5074419408996378584.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;186&#34;
		data-flex-basis=&#34;447px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;原理简介&#34;&gt;原理简介
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def longAccumulator: LongAccumulator = {
    val acc = new LongAccumulator
    register(acc)
    acc
  }

  /**
   * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def longAccumulator(name: String): LongAccumulator = {
    val acc = new LongAccumulator
    register(acc, name)
    acc
  }

  /**
   * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def doubleAccumulator: DoubleAccumulator = {
    val acc = new DoubleAccumulator
    register(acc)
    acc
  }

  /**
   * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def doubleAccumulator(name: String): DoubleAccumulator = {
    val acc = new DoubleAccumulator
    register(acc, name)
    acc
  }

  /**
   * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates
   * inputs by adding them into the list.
   */
  def collectionAccumulator[T]: CollectionAccumulator[T] = {
    val acc = new CollectionAccumulator[T]
    register(acc)
    acc
  }

  /**
   * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates
   * inputs by adding them into the list.
   */
  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = {
    val acc = new CollectionAccumulator[T]
    register(acc, name)
    acc
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spark中的累加器有三种&lt;code&gt;doubleAccumulator&lt;/code&gt;、&lt;code&gt;longAccumulator&lt;/code&gt;、&lt;code&gt;collectionAccumulator&lt;/code&gt;。分别有传入累加器名字和不穿名字的函数。
累加器会新建一个对应的累加器类，然后在driver注册，传不传名字的区别是注册的时候会不会传入名字，最后返回已注册的累加器。&lt;/p&gt;
&lt;p&gt;源码中对注册的解释如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Registers an AccumulatorV2 created on the driver such that it can be used on the executors.
All accumulators registered here can later be used as a container for accumulating partial values across multiple tasks. This is what org.apache.spark.scheduler.DAGScheduler does. Note: if an accumulator is registered here, it should also be registered with the active context cleaner for cleanup so as to avoid memory leaks.
If an AccumulatorV2 with the same ID was already registered, this does nothing instead of overwriting it. We will never register same accumulator twice, this is just a sanity check.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt; 注册在driver上创建的 AccumulatorV2，以便在executor上使用。 在此注册的所有累加器以后都可用作容器，用于累加多个任务中的部分值。 这就是 org.apache.spark.scheduler.DAGScheduler 的作用。 注意：如果在这里注册了累加器，那么它也应注册到活动上下文清理器中进行清理，以避免内存泄漏。 如果已经注册了具有相同 ID 的 AccumulatorV2，则不会做任何操作，而会覆盖它。 我们永远不会注册同一个累加器两次，这只是为了进行合理性检查。&lt;/p&gt;
&lt;h2 id=&#34;累加器类&#34;&gt;累加器类
&lt;/h2&gt;&lt;h3 id=&#34;longaccumulator&#34;&gt;LongAccumulator
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;
/**
 * An [[AccumulatorV2 accumulator]] for computing sum, count, and average of 64-bit integers.
 *
 * @since 2.0.0
 */
class LongAccumulator extends AccumulatorV2[jl.Long, jl.Long] {
  private var _sum = 0L
  private var _count = 0L

  /**
   * Returns false if this accumulator has had any values added to it or the sum is non-zero.
   *
   * @since 2.0.0
   */
  override def isZero: Boolean = _sum == 0L &amp;amp;&amp;amp; _count == 0

  override def copy(): LongAccumulator = {
    val newAcc = new LongAccumulator
    newAcc._count = this._count
    newAcc._sum = this._sum
    newAcc
  }

  override def reset(): Unit = {
    _sum = 0L
    _count = 0L
  }

  /**
   * Adds v to the accumulator, i.e. increment sum by v and count by 1.
   * @since 2.0.0
   */
  override def add(v: jl.Long): Unit = {
    _sum += v
    _count += 1
  }

  /**
   * Adds v to the accumulator, i.e. increment sum by v and count by 1.
   * @since 2.0.0
   */
  def add(v: Long): Unit = {
    _sum += v
    _count += 1
  }

  /**
   * Returns the number of elements added to the accumulator.
   * @since 2.0.0
   */
  def count: Long = _count

  /**
   * Returns the sum of elements added to the accumulator.
   * @since 2.0.0
   */
  def sum: Long = _sum

  /**
   * Returns the average of elements added to the accumulator.
   * @since 2.0.0
   */
  def avg: Double = _sum.toDouble / _count

  override def merge(other: AccumulatorV2[jl.Long, jl.Long]): Unit = other match {
    case o: LongAccumulator =&amp;gt;
      _sum += o.sum
      _count += o.count
    case _ =&amp;gt;
      throw new UnsupportedOperationException(
        s&amp;quot;Cannot merge ${this.getClass.getName} with ${other.getClass.getName}&amp;quot;)
  }

  private[spark] def setValue(newValue: Long): Unit = _sum = newValue

  override def value: jl.Long = _sum
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LongAccumulator类是一个自定义累加器，用于计算64位整数的总和、计数和平均值。它继承自AccumulatorV2。
私有变量_sum:(存储累加的总和)、_count(存储累加的元素个数)。
用于计算整型数据的总和、计数和平均值，并在driver中访问结果。&lt;/p&gt;
&lt;p&gt;isZero方法:检查累加器是否未添加任何值或总和为零。&lt;/p&gt;
&lt;p&gt;copy方法:创建累加器的副本，包括当前的_sum和_count。&lt;/p&gt;
&lt;p&gt;reset方法:将_sum和_count重置为零。&lt;/p&gt;
&lt;p&gt;add方法:增加一个值到累加器中，更新_sum和_count。有两个重载版本，接收jl.Long和Long类型。
(jl.Long是java.lang.Long的缩写。是Java的类类型，用于包装一个long的值。
提供了许多方法来处理long类型的数据，比如转换、比较等。用于与Java类进行交互时的包装器类型。
Long是Scala的基本数据类型。直接表示一个64位的整数。)&lt;/p&gt;
&lt;p&gt;count方法:返回添加到累加器中的元素个数。&lt;/p&gt;
&lt;p&gt;sum方法:返回累加器中的元素总和。&lt;/p&gt;
&lt;p&gt;avg方法:返回累加器中元素的平均值。&lt;/p&gt;
&lt;p&gt;merge方法:将另一个LongAccumulator的值合并到当前累加器。如果尝试与非LongAccumulator类型合并，会抛出异常。&lt;/p&gt;
&lt;p&gt;setValue方法:设置累加器的总和值（仅用于内部）。&lt;/p&gt;
&lt;p&gt;value方法:返回累加器的当前总和。&lt;/p&gt;
&lt;h3 id=&#34;doubleaccumulator&#34;&gt;DoubleAccumulator
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;
/**
 * An [[AccumulatorV2 accumulator]] for computing sum, count, and averages for double precision
 * floating numbers.
 *
 * @since 2.0.0
 */
class DoubleAccumulator extends AccumulatorV2[jl.Double, jl.Double] {
  private var _sum = 0.0
  private var _count = 0L

  /**
   * Returns false if this accumulator has had any values added to it or the sum is non-zero.
   */
  override def isZero: Boolean = _sum == 0.0 &amp;amp;&amp;amp; _count == 0

  override def copy(): DoubleAccumulator = {
    val newAcc = new DoubleAccumulator
    newAcc._count = this._count
    newAcc._sum = this._sum
    newAcc
  }

  override def reset(): Unit = {
    _sum = 0.0
    _count = 0L
  }

  /**
   * Adds v to the accumulator, i.e. increment sum by v and count by 1.
   * @since 2.0.0
   */
  override def add(v: jl.Double): Unit = {
    _sum += v
    _count += 1
  }

  /**
   * Adds v to the accumulator, i.e. increment sum by v and count by 1.
   * @since 2.0.0
   */
  def add(v: Double): Unit = {
    _sum += v
    _count += 1
  }

  /**
   * Returns the number of elements added to the accumulator.
   * @since 2.0.0
   */
  def count: Long = _count

  /**
   * Returns the sum of elements added to the accumulator.
   * @since 2.0.0
   */
  def sum: Double = _sum

  /**
   * Returns the average of elements added to the accumulator.
   * @since 2.0.0
   */
  def avg: Double = _sum / _count

  override def merge(other: AccumulatorV2[jl.Double, jl.Double]): Unit = other match {
    case o: DoubleAccumulator =&amp;gt;
      _sum += o.sum
      _count += o.count
    case _ =&amp;gt;
      throw new UnsupportedOperationException(
        s&amp;quot;Cannot merge ${this.getClass.getName} with ${other.getClass.getName}&amp;quot;)
  }

  private[spark] def setValue(newValue: Double): Unit = _sum = newValue

  override def value: jl.Double = _sum
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;DoubleAccumulator&lt;/code&gt;与&lt;code&gt;LongAccumulator&lt;/code&gt;区别不大，主要区别在&lt;code&gt;DoubleAccumulator&lt;/code&gt;的_sum为Double类型。&lt;/p&gt;
&lt;h3 id=&#34;collectionaccumulator&#34;&gt;CollectionAccumulator
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;
/**
 * An [[AccumulatorV2 accumulator]] for collecting a list of elements.
 *
 * @since 2.0.0
 */
class CollectionAccumulator[T] extends AccumulatorV2[T, java.util.List[T]] {
  private var _list: java.util.List[T] = _

  private def getOrCreate = {
    _list = Option(_list).getOrElse(new java.util.ArrayList[T]())
    _list
  }

  /**
   * Returns false if this accumulator instance has any values in it.
   */
  override def isZero: Boolean = this.synchronized(getOrCreate.isEmpty)

  override def copyAndReset(): CollectionAccumulator[T] = new CollectionAccumulator

  override def copy(): CollectionAccumulator[T] = {
    val newAcc = new CollectionAccumulator[T]
    this.synchronized {
      newAcc.getOrCreate.addAll(getOrCreate)
    }
    newAcc
  }

  override def reset(): Unit = this.synchronized {
    _list = null
  }

  override def add(v: T): Unit = this.synchronized(getOrCreate.add(v))

  override def merge(other: AccumulatorV2[T, java.util.List[T]]): Unit = other match {
    case o: CollectionAccumulator[T] =&amp;gt; this.synchronized(getOrCreate.addAll(o.value))
    case _ =&amp;gt; throw new UnsupportedOperationException(
      s&amp;quot;Cannot merge ${this.getClass.getName} with ${other.getClass.getName}&amp;quot;)
  }

  override def value: java.util.List[T] = this.synchronized {
    java.util.Collections.unmodifiableList(new ArrayList[T](getOrCreate))
  }

  private[spark] def setValue(newValue: java.util.List[T]): Unit = this.synchronized {
    _list = null
    getOrCreate.addAll(newValue)
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; CollectionAccumulator类是一个用于收集元素列表的自定义累加器。它继承自AccumulatorV2(在Spark中创建累加器的基类,自定义累加器也需要继承这个类)。具有私有变量 _list，用于存储累积的元素列表，初始为null。使用synchronized确保了对_list的操作是线程安全的。主要用途是在任务中收集元素并在driver上提供收集到的数据。 _list类型是java.util.List，所以java.util.List的方法都可以在这里正常使用（&lt;code&gt;setValue&lt;/code&gt;就是先情况_list，再调用List的&lt;code&gt;addAll&lt;/code&gt;添加一个&lt;code&gt;java.util.List&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;getOrCreate&lt;/code&gt;方法:确保_list已初始化，如果为null，则创建一个新的ArrayList(java.util.ArrayList)。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;isZero&lt;/code&gt;方法:检查累加器是否为空，如果为空则返回true。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;copyAndReset&lt;/code&gt;方法:创建一个没有累积值的新CollectionAccumulator。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;copy&lt;/code&gt;方法:创建一个新的CollectionAccumulator并复制当前元素。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;reset&lt;/code&gt;方法:通过将_list设置为null来清空累加器。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;add&lt;/code&gt;方法:向累加器中添加一个元素。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;merge&lt;/code&gt;方法:将另一个累加器的值合并到这个累加器中，仅支持与另一个CollectionAccumulator合并。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;value&lt;/code&gt;方法:返回累积列表的不可修改视图。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;setValue&lt;/code&gt;方法:重置累加器并设置为新的元素列表。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Spark源码学习 广播变量</title>
        <link>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</link>
        <pubDate>Thu, 07 Nov 2024 14:51:19 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</guid>
        <description>&lt;h2 id=&#34;简介&#34;&gt;简介
&lt;/h2&gt;&lt;p&gt;广播变量允许程序员在每台机器上缓存只读变量，而不是随任务一起发送副本。 例如，它们可以用来以高效的方式为每个节点提供一个大型输入数据集的副本。 Spark 还尝试使用高效的广播算法分发广播变量，以降低通信成本。 广播变量是通过调用 broadcast 从变量 v 中创建的。 广播变量是 v 的包装器，其值可通过调用 value 方法访问。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/1.png&#34;
	width=&#34;1249&#34;
	height=&#34;889&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/1_hu9234698187790083378.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/1_hu17366738427305287415.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;337px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/2.png&#34;
	width=&#34;1763&#34;
	height=&#34;952&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/2_hu1508007316500837288.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/2_hu13925634685911099771.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;444px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;快速上手&#34;&gt;快速上手
&lt;/h2&gt;&lt;p&gt;广播变量使用如简介中所说，使用&lt;code&gt;sc.broadcast()&lt;/code&gt;包装一个变量，就创建了一个广播变量。访问广播变量的值可以通过调用其value方法，即&lt;code&gt;broadV.value()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.broadcast.Broadcast
import org.apache.spark.{SparkConf, SparkContext}

object BroadCastTry {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;广播变量练习&amp;quot;)
    val sc = new SparkContext(sparkConf)

    val v = Array(1,2,3,4,5,6)
    // 创建广播变量
    val broadV:Broadcast[Array[Int]] = sc.broadcast(v)

    //打印广播变量
    println(broadV.value.mkString(&amp;quot;Array(&amp;quot;, &amp;quot;, &amp;quot;, &amp;quot;)&amp;quot;))

    //销毁广播变量
    broadV.destroy()

    sc.stop()
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/3.png&#34;
	width=&#34;1920&#34;
	height=&#34;1030&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/3_hu12592535932714623619.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/3_hu13777497629336887710.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;186&#34;
		data-flex-basis=&#34;447px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;原理简介&#34;&gt;原理简介
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;参考资料：&lt;a class=&#34;link&#34; href=&#34;https://cloud.tencent.com/developer/article/1484260&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Spark Core源码精读计划11 | Spark广播机制的实现&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：本次源码阅读使用的是Spark_2.12-3.3.2(scala2.12版本写的Spark3.3.2)&lt;/p&gt;
&lt;p&gt;简单地说，广播变量的流程如下&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;广播变量由Application的Driver使用&lt;code&gt;BroadcastManager&lt;/code&gt;创建，并存储在&lt;code&gt;BlockManager&lt;/code&gt;中&lt;/li&gt;
&lt;li&gt;Driver将广播变量的值写到Block中，这样在driver上执行的tasks不会再创建一份新的广播变量副本&lt;/li&gt;
&lt;li&gt;Executor需要使用这个变量时，先从本地&lt;code&gt;BlockManager&lt;/code&gt;中查找有无该变量，没有的话从Driver的&lt;code&gt;BlockManager&lt;/code&gt;远程读取该变量&lt;/li&gt;
&lt;li&gt;Executor获取到这个广播变量后就将它缓存到本地的&lt;code&gt;BlockManager&lt;/code&gt;中，避免重复地远程获取，提高性能。
&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/4.png&#34;
	width=&#34;1830&#34;
	height=&#34;872&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/4_hu6247838819089611269.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/4_hu5942886300715633446.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;209&#34;
		data-flex-basis=&#34;503px&#34;
	
&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用广播变量能提高性能的原因是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少数据传输：广播变量将数据从Driver节点传输到每个Executor，只进行一次网络传输，而不是在每个任务中重复传输。&lt;/li&gt;
&lt;li&gt;本地缓存：一旦Executor接收到广播变量，它会将其缓存本地，供后续任务使用，避免重复的远程获取。&lt;/li&gt;
&lt;li&gt;内存效率：通过共享相同的数据副本，广播变量减少了Executor内存中的数据冗余。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些特性一起显著减少了网络I/O和内存开销，提升了分布式计算的性能和效率。但是广播变量只能用在只读变量上，而且只适合用在比较大的变量上。&lt;/p&gt;
&lt;p&gt;因为对于比较小的变量，直接传递给每个任务的开销很低，而且广播机制增大了任务复杂性。不能用于可变变量的原因也很明显，广播变量是由Driver创建，并由Executor远程读取。Driver或者Executor修改后都会导致计算结果错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;补充：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BroadcastManager&lt;/code&gt;是位于&lt;code&gt;org.apache.spark.broadcast&lt;/code&gt;下的类，用于创建和管理广播变量，&lt;code&gt;BlockManager&lt;/code&gt;是位于&lt;code&gt;org.apache.spark.storage&lt;/code&gt;下的类，用于在每个节点（Driver和Executor）上运行的管理器，为本地和远程向各种存储（内存、磁盘和堆外）放入和检索数据块提供接口。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Broadcast&lt;/code&gt;是位于&lt;code&gt;org.apache.spark.broadcast&lt;/code&gt;下的抽象类，只有&lt;code&gt;TorrentBroadcast&lt;/code&gt;一个子类，早期还有一个&lt;code&gt;HttpBroadcast&lt;/code&gt;子类。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TorrentBroadcast&lt;/code&gt;是类似于 BitTorrent 的 Broadcast 实现。有同名类和对象。其机制如下： Driver将序列化对象分成小块，并将这些小块存储在Executor的 BlockManager 中。 在每个Executor上，Executor首先尝试从其 BlockManager 中获取对象。 如果对象不存在，Executor就会使用远程获取功能，从Driver和/或其他Executor（如果有的话）中获取小块对象。 获取小块后，它会将小块放入自己的 BlockManager 中，供其他执行器取用。 这样，驱动程序就不会成为发送多份广播数据（每个执行器一份）的瓶颈。 初始化时，TorrentBroadcast 对象会读取 SparkEnv.get.conf 文件。&lt;/p&gt;
&lt;h2 id=&#34;广播管理器broadcastmanager&#34;&gt;广播管理器BroadcastManager
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;
package org.apache.spark.broadcast

import java.util.Collections
import java.util.concurrent.atomic.AtomicLong

import scala.reflect.ClassTag

import org.apache.commons.collections4.map.AbstractReferenceMap.ReferenceStrength
import org.apache.commons.collections4.map.ReferenceMap

import org.apache.spark.SparkConf
import org.apache.spark.api.python.PythonBroadcast
import org.apache.spark.internal.Logging

private[spark] class BroadcastManager(
    val isDriver: Boolean, conf: SparkConf) extends Logging {

  private var initialized = false
  private var broadcastFactory: BroadcastFactory = null

  initialize()

  // Called by SparkContext or Executor before using Broadcast
  private def initialize(): Unit = {
    synchronized {
      if (!initialized) {
        broadcastFactory = new TorrentBroadcastFactory
        broadcastFactory.initialize(isDriver, conf)
        initialized = true
      }
    }
  }

  def stop(): Unit = {
    broadcastFactory.stop()
  }

  private val nextBroadcastId = new AtomicLong(0)

  private[broadcast] val cachedValues =
    Collections.synchronizedMap(
      new ReferenceMap(ReferenceStrength.HARD, ReferenceStrength.WEAK)
        .asInstanceOf[java.util.Map[Any, Any]]
    )

  def newBroadcast[T: ClassTag](value_ : T, isLocal: Boolean): Broadcast[T] = {
    val bid = nextBroadcastId.getAndIncrement()
    value_ match {
      case pb: PythonBroadcast =&amp;gt;
        // SPARK-28486: attach this new broadcast variable&#39;s id to the PythonBroadcast,
        // so that underlying data file of PythonBroadcast could be mapped to the
        // BroadcastBlockId according to this id. Please see the specific usage of the
        // id in PythonBroadcast.readObject().
        pb.setBroadcastId(bid)

      case _ =&amp;gt; // do nothing
    }
    broadcastFactory.newBroadcast[T](value_, isLocal, bid)
  }

  def unbroadcast(id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit = {
    broadcastFactory.unbroadcast(id, removeFromDriver, blocking)
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;BroadcastManager创建时传入两个参数&lt;code&gt;isDriver(是否是Driver)&lt;/code&gt;、&lt;code&gt;conf(Spark配置文件)&lt;/code&gt;。早期版本还有第三个变量&lt;code&gt;securityManager（对应的SecurityManager）&lt;/code&gt;,这里我只是看参考资料中的源码知道的，第三个变量具体的作用不做了解。&lt;/p&gt;
&lt;h3 id=&#34;成员变量&#34;&gt;成员变量
&lt;/h3&gt;&lt;p&gt;BroadcastManager内有四个成员变量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;initialized&lt;/code&gt;表示BroadcastManager是否已经初始化完成。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;broadcastFactory&lt;/code&gt;持有广播工厂的实例（类型是BroadcastFactory特征的实现类）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nextBroadcastId&lt;/code&gt;表示下一个广播变量的唯一标识（AtomicLong类型的）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cachedValues&lt;/code&gt;用来缓存已广播出去的变量。它属于ReferenceMap类型，是apache-commons提供的一个弱引用映射数据结构。与我们常见的各种Map不同，它的键值对有可能会在GC过程中被回收。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;初始化逻辑&#34;&gt;初始化逻辑
&lt;/h3&gt;&lt;p&gt;initialize()方法做的事情也非常简单，它首先判断BroadcastManager是否已初始化。如果未初始化，就新建广播工厂TorrentBroadcastFactory，将其初始化，然后将初始化标记设为true。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  private def initialize(): Unit = {
    synchronized {
      if (!initialized) {
        broadcastFactory = new TorrentBroadcastFactory
        broadcastFactory.initialize(isDriver, conf)
        initialized = true
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;对外提供的方法&#34;&gt;对外提供的方法
&lt;/h3&gt;&lt;p&gt;BroadcastManager提供的方法有两个：newBroadcast()方法，用于创建一个新的广播变量；以及unbroadcast()方法，将已存在的广播变量取消广播。它们都是直接调用了&lt;code&gt;TorrentBroadcastFactory&lt;/code&gt;中的同名方法。因此我们必须通过阅读&lt;code&gt;TorrentBroadcastFactory&lt;/code&gt;的相关源码，才能了解Spark广播机制的细节。&lt;/p&gt;
&lt;h2 id=&#34;广播工厂类torrentbroadcastfactory&#34;&gt;广播工厂类TorrentBroadcastFactory
&lt;/h2&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package org.apache.spark.broadcast

import scala.reflect.ClassTag

import org.apache.spark.SparkConf

/**
 * A [[org.apache.spark.broadcast.Broadcast]] implementation that uses a BitTorrent-like
 * protocol to do a distributed transfer of the broadcasted data to the executors. Refer to
 * [[org.apache.spark.broadcast.TorrentBroadcast]] for more details.
 */
private[spark] class TorrentBroadcastFactory extends BroadcastFactory {

  override def initialize(isDriver: Boolean, conf: SparkConf): Unit = { }

  override def newBroadcast[T: ClassTag](value_ : T, isLocal: Boolean, id: Long): Broadcast[T] = {
    new TorrentBroadcast[T](value_, id)
  }

  override def stop(): Unit = { }

  /**
   * Remove all persisted state associated with the torrent broadcast with the given ID.
   * @param removeFromDriver Whether to remove state from the driver.
   * @param blocking Whether to block until unbroadcasted
   */
  override def unbroadcast(id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit = {
    TorrentBroadcast.unpersist(id, removeFromDriver, blocking)
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由源码可知，&lt;code&gt;TorrentBroadcastFactory&lt;/code&gt;的&lt;code&gt;newBroadcast()&lt;/code&gt;方法实际是新建了一个&lt;code&gt;TorrentBroadcast&lt;/code&gt;类，并传入了这个类的id和值。
&lt;code&gt;TorrentBroadcast&lt;/code&gt;类的详情参见下节。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TorrentBroadcastFactory&lt;/code&gt;的&lt;code&gt;unbroadcast()&lt;/code&gt;方法传入了TorrentBroadcast类的id、
removeFromDriver（是否从驱动程序中移除状态）、blocking （是否直到未广播仍然在堵塞）。
然后删除与给定 ID 的 torrent 广播相关的所有持久化状态。这个删除持久化状态实际是&lt;code&gt;SparkEnv.get.blockManager.master.removeBroadcast(id, removeFromDriver, blocking)&lt;/code&gt;。相关代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  def removeBroadcast(broadcastId: Long, removeFromMaster: Boolean, blocking: Boolean): Unit = {
    val future = driverEndpoint.askSync[Future[Seq[Int]]](
      RemoveBroadcast(broadcastId, removeFromMaster))
    future.failed.foreach(e =&amp;gt;
      logWarning(s&amp;quot;Failed to remove broadcast $broadcastId&amp;quot; +
        s&amp;quot; with removeFromMaster = $removeFromMaster - ${e.getMessage}&amp;quot;, e)
    )(ThreadUtils.sameThread)
    if (blocking) {
      // the underlying Futures will timeout anyway, so it&#39;s safe to use infinite timeout here
      RpcUtils.INFINITE_TIMEOUT.awaitResult(future)
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;torrentbroadcast类&#34;&gt;TorrentBroadcast类
&lt;/h2&gt;&lt;h3 id=&#34;成员变量-1&#34;&gt;成员变量
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;_value：广播块的具体数据。注意它由lazy关键字定义，因此是懒加载的，也就是在TorrentBroadcast构造时不会调用readBroadcastBlock()方法获取数据，而会推迟到第一次访问_value时。&lt;/li&gt;
&lt;li&gt;compressionCodec：广播块的压缩编解码逻辑。当配置项spark.broadcast.compress为true时，会启用压缩。&lt;/li&gt;
&lt;li&gt;blockSize：广播块的大小。由spark.broadcast.blockSize配置项来控制，默认值4MB。&lt;/li&gt;
&lt;li&gt;broadcastId：广播变量的ID。BroadcastBlockId是个结构非常简单的case class，每产生一个新的广播变量就会自增。&lt;/li&gt;
&lt;li&gt;numBlocks：该广播变量包含的块数量。它与_value不同，并没有lazy关键字定义，因此在TorrentBroadcast构造时就会直接调用writeBlocks()方法。&lt;/li&gt;
&lt;li&gt;checksumEnabled：是否允许对广播块计算校验值，由spark.broadcast.checksum配置项控制，默认值true。&lt;/li&gt;
&lt;li&gt;checksums：广播块的校验值。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writeblocks&#34;&gt;writeBlocks()
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  private def writeBlocks(value: T): Int = {
    import StorageLevel._
    // Store a copy of the broadcast variable in the driver so that tasks run on the driver
    // do not create a duplicate copy of the broadcast variable&#39;s value.
    val blockManager = SparkEnv.get.blockManager
    if (!blockManager.putSingle(broadcastId, value, MEMORY_AND_DISK, tellMaster = false)) {
      throw new SparkException(s&amp;quot;Failed to store $broadcastId in BlockManager&amp;quot;)
    }
    try {
      val blocks =
        TorrentBroadcast.blockifyObject(value, blockSize, SparkEnv.get.serializer, compressionCodec)
      if (checksumEnabled) {
        checksums = new Array[Int](blocks.length)
      }
      blocks.zipWithIndex.foreach { case (block, i) =&amp;gt;
        if (checksumEnabled) {
          checksums(i) = calcChecksum(block)
        }
        val pieceId = BroadcastBlockId(id, &amp;quot;piece&amp;quot; + i)
        val bytes = new ChunkedByteBuffer(block.duplicate())
        if (!blockManager.putBytes(pieceId, bytes, MEMORY_AND_DISK_SER, tellMaster = true)) {
          throw new SparkException(s&amp;quot;Failed to store $pieceId of $broadcastId &amp;quot; +
            s&amp;quot;in local BlockManager&amp;quot;)
        }
      }
      blocks.length
    } catch {
      case t: Throwable =&amp;gt;
        logError(s&amp;quot;Store broadcast $broadcastId fail, remove all pieces of the broadcast&amp;quot;)
        blockManager.removeBroadcast(id, tellMaster = true)
        throw t
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;获取BlockManager实例，调用其putSingle()方法将广播数据作为单个对象写入本地存储。注意StorageLevel为MEMORY_AND_DISK，亦即在内存不足时会溢写到磁盘，且副本数为1，不会进行复制。&lt;/li&gt;
&lt;li&gt;调用blockifyObject()方法将广播数据转化为块(block)，即Spark存储的基本单元。
使用的序列化器为SparkEnv中指定的序列化器（默认Java自带的序列化，另外Spark实现了kryo序列化，可以在SparkEnv中指定）。
如果校验值开关有效，就用calcChecksum()方法为每个块计算校验值。&lt;/li&gt;
&lt;li&gt;为广播数据切分成的每个块（称为piece）都生成一个带&amp;quot;piece&amp;quot;的广播ID，调用BlockManager.putBytes()方法将各个块写入MemoryStore（内存）或DiskStore（磁盘）。StorageLevel为MEMORY_AND_DISK_SER，写入的数据会序列化。&lt;/li&gt;
&lt;li&gt;最终返回块的计数值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;readbroadcastblock&#34;&gt;readBroadcastBlock()
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  private def readBroadcastBlock(): T = Utils.tryOrIOException {
    TorrentBroadcast.torrentBroadcastLock.withLock(broadcastId) {
      // As we only lock based on `broadcastId`, whenever using `broadcastCache`, we should only
      // touch `broadcastId`.
      val broadcastCache = SparkEnv.get.broadcastManager.cachedValues

      Option(broadcastCache.get(broadcastId)).map(_.asInstanceOf[T]).getOrElse {
        setConf(SparkEnv.get.conf)
        val blockManager = SparkEnv.get.blockManager
        blockManager.getLocalValues(broadcastId) match {
          case Some(blockResult) =&amp;gt;
            if (blockResult.data.hasNext) {
              val x = blockResult.data.next().asInstanceOf[T]
              releaseBlockManagerLock(broadcastId)

              if (x != null) {
                broadcastCache.put(broadcastId, x)
              }

              x
            } else {
              throw new SparkException(s&amp;quot;Failed to get locally stored broadcast data: $broadcastId&amp;quot;)
            }
          case None =&amp;gt;
            val estimatedTotalSize = Utils.bytesToString(numBlocks * blockSize)
            logInfo(s&amp;quot;Started reading broadcast variable $id with $numBlocks pieces &amp;quot; +
              s&amp;quot;(estimated total size $estimatedTotalSize)&amp;quot;)
            val startTimeNs = System.nanoTime()
            val blocks = readBlocks()
            logInfo(s&amp;quot;Reading broadcast variable $id took ${Utils.getUsedTimeNs(startTimeNs)}&amp;quot;)

            try {
              val obj = TorrentBroadcast.unBlockifyObject[T](
                blocks.map(_.toInputStream()), SparkEnv.get.serializer, compressionCodec)
              // Store the merged copy in BlockManager so other tasks on this executor don&#39;t
              // need to re-fetch it.
              val storageLevel = StorageLevel.MEMORY_AND_DISK
              if (!blockManager.putSingle(broadcastId, obj, storageLevel, tellMaster = false)) {
                throw new SparkException(s&amp;quot;Failed to store $broadcastId in BlockManager&amp;quot;)
              }

              if (obj != null) {
                broadcastCache.put(broadcastId, obj)
              }

              obj
            } finally {
              blocks.foreach(_.dispose())
            }
        }
      }
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;使用广播id对读取操作加锁，保证线程安全&lt;/li&gt;
&lt;li&gt;调用&lt;code&gt;broadcastManager.cacheedValues&lt;/code&gt;,根据广播id检查广播变量是否已在本地缓存中，如果存在，直接返回&lt;/li&gt;
&lt;li&gt;调用setConf()传入配置信息&lt;/li&gt;
&lt;li&gt;尝试从本地blockManager调用&lt;code&gt;getLocalValues&lt;/code&gt;获取指定广播id的数据，如果有就直接读取并缓存&lt;/li&gt;
&lt;li&gt;如果本地blockManager不存在，就调用&lt;code&gt;readBlocks()&lt;/code&gt;方法，从driver和其他executor读取指定广播id对应的piece（片）数据&lt;/li&gt;
&lt;li&gt;从其他节点获取到分块数据后，将其反序列化，重建对象并存储在BlockManager中,并将重建的对象缓存&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;补充readblocks&#34;&gt;补充：readBlocks()
&lt;/h3&gt;&lt;p&gt;readBlocks()是在本地缓存和BlockManager都读取不到数据后，从其他节点读取数据的方法。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  private def readBlocks(): Array[BlockData] = {
    // Fetch chunks of data. Note that all these chunks are stored in the BlockManager and reported
    // to the driver, so other executors can pull these chunks from this executor as well.
    val blocks = new Array[BlockData](numBlocks)
    val bm = SparkEnv.get.blockManager

    for (pid &amp;lt;- Random.shuffle(Seq.range(0, numBlocks))) {
      val pieceId = BroadcastBlockId(id, &amp;quot;piece&amp;quot; + pid)
      logDebug(s&amp;quot;Reading piece $pieceId of $broadcastId&amp;quot;)
      // First try getLocalBytes because there is a chance that previous attempts to fetch the
      // broadcast blocks have already fetched some of the blocks. In that case, some blocks
      // would be available locally (on this executor).
      bm.getLocalBytes(pieceId) match {
        case Some(block) =&amp;gt;
          blocks(pid) = block
          releaseBlockManagerLock(pieceId)
        case None =&amp;gt;
          bm.getRemoteBytes(pieceId) match {
            case Some(b) =&amp;gt;
              if (checksumEnabled) {
                val sum = calcChecksum(b.chunks(0))
                if (sum != checksums(pid)) {
                  throw new SparkException(s&amp;quot;corrupt remote block $pieceId of $broadcastId:&amp;quot; +
                    s&amp;quot; $sum != ${checksums(pid)}&amp;quot;)
                }
              }
              // We found the block from remote executors/driver&#39;s BlockManager, so put the block
              // in this executor&#39;s BlockManager.
              if (!bm.putBytes(pieceId, b, StorageLevel.MEMORY_AND_DISK_SER, tellMaster = true)) {
                throw new SparkException(
                  s&amp;quot;Failed to store $pieceId of $broadcastId in local BlockManager&amp;quot;)
              }
              blocks(pid) = new ByteBufferBlockData(b, true)
            case None =&amp;gt;
              throw new SparkException(s&amp;quot;Failed to get $pieceId of $broadcastId&amp;quot;)
          }
      }
    }
    blocks
  }

&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;初始化一个&lt;code&gt;BlockData&lt;/code&gt;数组，长度为广播id对应piece数，获取&lt;code&gt;BlockManager&lt;/code&gt;实例&lt;/li&gt;
&lt;li&gt;通过&lt;code&gt;Random.shuffle()&lt;/code&gt;随机顺序遍历每个块的id，确保负载均衡&lt;/li&gt;
&lt;li&gt;如果本地&lt;code&gt;BlockManager&lt;/code&gt;存在块id对应数据块，直接获取并存储&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;这里好像有点冗余，因为readBlocks()本来就是在本地缓存和BlockManager中找不到，才远程访问其他节点时调用的方法。但代码中这么写的原因已在注释中给出:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;First try getLocalBytes because there is a chance that previous attempts to fetch the broadcast blocks have already fetched some of the blocks. In that case, some blocks would be available locally (on this executor).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;首先尝试 getLocalBytes，因为之前获取广播数据块的尝试有可能已经获取了部分数据块。在这种情况下，一些区块将在本地（在此executor上）可用。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;如果本地没有，调用&lt;code&gt;blockManager.getRemoteBytes(pieceId)&lt;/code&gt;从远程节点获取。若开启了校验和，则调用&lt;code&gt;calChecksum()&lt;/code&gt;计算校验和并比较，来验证数据完整性&lt;/li&gt;
&lt;li&gt;将从远端获取的数据块存储到本地&lt;code&gt;BlcokManager&lt;/code&gt;中。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;获取远端数据封装了很多层，大体读取顺序如下图
&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/5.png&#34;
	width=&#34;829&#34;
	height=&#34;573&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/5_hu7419393722578630441.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/5_hu8869478636972443548.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;
获取远端数据的最后一步，从其他节点读取数据属于BlockManager的部分，下次有机会再读一读&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结
&lt;/h2&gt;&lt;p&gt;广播变量的底层机制总结如下图：
&lt;img src=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/6.png&#34;
	width=&#34;742&#34;
	height=&#34;733&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/6_hu9127105277565457001.png 480w, https://rusthx.github.io/p/spark%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/6_hu14277366887381701722.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;101&#34;
		data-flex-basis=&#34;242px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>SparkStreaming使用socket</title>
        <link>https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/</link>
        <pubDate>Sat, 07 Sep 2024 11:55:09 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;参考：&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV11A411L7CK?p=188&amp;amp;vd_source=2db7c64d895a2907954a5b8725db55d5&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV11A411L7CK?p=188&amp;amp;vd_source=2db7c64d895a2907954a5b8725db55d5&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;终端打印大量日志影响结果查看可以看我首页的博客解决。
踩坑如下：
1.socket编程不会，写socket发送数据查了很多资料才写出来
2.Windows没有netcat命令，但是MACOS和Ubuntu有,所以理所当然的想到用虚拟机的端口来收集数据和输入数据，事实上这个想法确实没有问题，分别做的话是能正常实现的，但是这也为后续的错误埋下了大坑。想当然的把socket当成kafka用（producer和consumer），是我踩坑的一大原因。
3.被教程误导，socket发送数据到端口，但是不知道socket有服务器和客户端之分，发送数据和处理数据的都是客户端，导致发送端可以和nc -lk &lt;Port&gt;结合使用，能正常监听到数据；接收端也能和nc -lk &lt;Port&gt;结合使用，在监听的端口出输入数据可以正常计算；但是两者结合就没办法计算了&lt;/p&gt;
&lt;h2 id=&#34;windows安装netcat&#34;&gt;Windows安装netcat
&lt;/h2&gt;&lt;p&gt;下载链接： &lt;a class=&#34;link&#34; href=&#34;https://nmap.org/download.html#windows&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://nmap.org/download.html#windows&lt;/a&gt;
下载的是一个exe包，点击exe包一路next即可完成安装。&lt;/p&gt;
&lt;p&gt;端口同时接收数据和计算数据时使用命令监听会无法访问，即启动socket数据发送程序和SparkStreaming数据计算程序后无法监听。但是监听命令可以用来分别调试两个程序。&lt;/p&gt;
&lt;p&gt;注意：Windows的netcat命令与Ubuntu和MacOS都不一样。Windows的命令是 &lt;code&gt;ncat -lk &amp;lt;Port&amp;gt;&lt;/code&gt;，参数的意思可以通过&lt;code&gt;ncat -h&lt;/code&gt;查看。
&lt;img src=&#34;https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/1.png&#34;
	width=&#34;960&#34;
	height=&#34;634&#34;
	srcset=&#34;https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/1_hu3612333662547381058.png 480w, https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/1_hu16466038784867921191.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;363px&#34;
	
&gt;
&lt;img src=&#34;https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/2.png&#34;
	width=&#34;1481&#34;
	height=&#34;760&#34;
	srcset=&#34;https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/2_hu3790405143218445351.png 480w, https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/2_hu11535289130947449563.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;467px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;sparkstreaming-socket编程&#34;&gt;SparkStreaming socket编程
&lt;/h2&gt;&lt;p&gt;题目：1）写一个应用程序利用套接字每隔2秒生成20条大学主页用户访问日志（可以自定义内容），数据形式如下：“系统时间戳，位置城市，用户ID+姓名，访问大学主页”。其中城市自定义 9个，用户ID 10个，大学主页 7个。
2）写第二个程序每隔2秒不断获取套接字产生的数据，并将词频统计结果打印出来。
导入依赖&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;
   &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
   &amp;lt;artifactId&amp;gt;spark-streaming_2.12&amp;lt;/artifactId&amp;gt;
   &amp;lt;version&amp;gt;3.3.2&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package SparkStreaming1

import java.io.{BufferedWriter, IOException, OutputStreamWriter}
import java.net.ServerSocket
import scala.util.Random

object task11 {
  def main(args: Array[String]): Unit = {

    // 定义城市和用户ID
    val cities = Seq(&amp;quot;杭州&amp;quot;, &amp;quot;南京&amp;quot;, &amp;quot;长沙&amp;quot;, &amp;quot;天津&amp;quot;, &amp;quot;北京&amp;quot;, &amp;quot;上海&amp;quot;, &amp;quot;成都&amp;quot;, &amp;quot;广州&amp;quot;, &amp;quot;深圳&amp;quot;)
    val userIds = Seq(&amp;quot;0:阿良良木历&amp;quot;, &amp;quot;1:忍野忍&amp;quot;, &amp;quot;2:战场原黑仪&amp;quot;, &amp;quot;3:羽川翼&amp;quot;, &amp;quot;4:八九寺真宵&amp;quot;,
      &amp;quot;5:神原骏河&amp;quot;, &amp;quot;6:千石抚子&amp;quot;, &amp;quot;7:阿良良木火怜&amp;quot;, &amp;quot;8:阿良良木月火&amp;quot;, &amp;quot;9:姬丝秀忒·雅赛劳拉莉昂·刃下心&amp;quot;)
    val universityUrls = Seq(&amp;quot;www.nju.edu.cn&amp;quot;, &amp;quot;www.ustc.edu.cn&amp;quot;,
      &amp;quot;www.zju.edu.cn&amp;quot;, &amp;quot;www.fudan.edu.cn&amp;quot;, &amp;quot;www.tsinghua.edu.cn&amp;quot;, &amp;quot;www.pku.edu.cn&amp;quot;, &amp;quot;www.scu.edu.cn&amp;quot;)

    try {
      // 创建一个 socket 连接
//      val socket = new Socket(&amp;quot;hadoop3&amp;quot;, 9765)
      val socketServer = new ServerSocket(9765)
      val client = socketServer.accept()
      println(&amp;quot;连接！&amp;quot;)
      val out = new BufferedWriter(new OutputStreamWriter(client.getOutputStream))
//val in = new BufferedReader(new InputStreamReader(client.getInputStream))
      while (true){
        for (_ &amp;lt;- 1 to 20) {
          // 发送多条数据
          val currentTime = System.currentTimeMillis()
          val city = cities(Random.nextInt(cities.length))
          val userId = userIds(Random.nextInt(userIds.length))
          val universityUrl = universityUrls(Random.nextInt(universityUrls.length))
          val logLine = s&amp;quot;$currentTime $city $userId $universityUrl&amp;quot;

          out.write(logLine + &amp;quot;\n&amp;quot;) // 添加换行符以区分消息
          out.flush() // 确保数据被发送出去
          println(logLine)
        }
        Thread.sleep(2000) // 休眠2秒，模拟连续发送
      }

      // 关闭 socket
      out.close()
//      socket.close()
      client.close()
    } catch {
      case e: IOException =&amp;gt;
        e.printStackTrace()
    }

  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package SparkStreaming1

import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.{Seconds, StreamingContext}


object task12{
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;job7task12&amp;quot;)
    val spark = SparkSession.builder().config(sparkConf).getOrCreate()
    val ssc = new StreamingContext(spark.sparkContext, Seconds(2))

    // 从套接字获取数据流
    val lines = ssc.socketTextStream(&amp;quot;localhost&amp;quot;, 9765)
    
    lines.map(_.split(&amp;quot; &amp;quot;)(2))
      .map((_, 1))
      .reduceByKey(_ + _)
      .print()
    
    ssc.start()

    ssc.awaitTermination()
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动时需要先启动数据计算程序，再启动数据发送程序。
成功运行截图：
&lt;img src=&#34;https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/3.png&#34;
	width=&#34;692&#34;
	height=&#34;371&#34;
	srcset=&#34;https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/3_hu4369904911137244949.png 480w, https://rusthx.github.io/p/sparkstreaming%E4%BD%BF%E7%94%A8socket/3_hu4522820763415712476.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;186&#34;
		data-flex-basis=&#34;447px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Scala中函数与Spark的算子的区别</title>
        <link>https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/</link>
        <pubDate>Sat, 07 Sep 2024 11:46:08 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/</guid>
        <description>&lt;p&gt;Scala中的部分函数和RDD中的部分算子名字一样，功能一样，用起来也差不多。但是为什么一个叫函数，一个却要叫算子，函数和算子的区别在哪，这让我有些好奇。于是查看了源码，对函数和算子进行了比较。下面以&lt;code&gt;map&lt;/code&gt;为例。&lt;/p&gt;
&lt;h2 id=&#34;scala中的map函数&#34;&gt;Scala中的map函数
&lt;/h2&gt;&lt;p&gt;Scala中的map通常定义在集合类中，例如&lt;code&gt;Map&lt;/code&gt;、&lt;code&gt;List&lt;/code&gt;、&lt;code&gt;Seq&lt;/code&gt;、&lt;code&gt;Set&lt;/code&gt;。作用是对该可迭代集合的所有元素应用一个函数，从而建立一个新的可迭代集合。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Builds a new iterable collection by applying a function to all elements of this iterable collection.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：&lt;code&gt;List&lt;/code&gt;属于&lt;code&gt;scala.collection.immutable&lt;/code&gt;的子类，而&lt;code&gt;Map、Seq、Set&lt;/code&gt;属于&lt;code&gt;scala.collection&lt;/code&gt;的子类
&lt;code&gt;map&lt;/code&gt;函数最底层的源码应该是&lt;code&gt;scala.collection&lt;/code&gt;里的如下代码
&lt;img src=&#34;https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/1.png&#34;
	width=&#34;958&#34;
	height=&#34;855&#34;
	srcset=&#34;https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/1_hu9843272686499372415.png 480w, https://rusthx.github.io/p/scala%E4%B8%AD%E5%87%BD%E6%95%B0%E4%B8%8Espark%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%8C%BA%E5%88%AB/1_hu11302242169032022826.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;268px&#34;
	
&gt;
具体实现以&lt;code&gt;List&lt;/code&gt;中的&lt;code&gt;map&lt;/code&gt;举例
源码为&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  final override def map[B](f: A =&amp;gt; B): List[B] = {
    if (this eq Nil) Nil else {
      val h = new ::[B](f(head), Nil)
      var t: ::[B] = h
      var rest = tail
      while (rest ne Nil) {
        val nx = new ::(f(rest.head), Nil)
        t.next = nx
        t = nx
        rest = rest.tail
      }
      releaseFence()
      h
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;List&lt;/code&gt;中的&lt;code&gt;map&lt;/code&gt;重写了其父类&lt;code&gt;collection.IterableOnce&lt;/code&gt;的&lt;code&gt;map&lt;/code&gt;函数，定义了一个匿名函数&lt;code&gt;(f:A=&amp;gt;B)&lt;/code&gt;,对List中的每个元素A处理后输出B类型的&lt;code&gt;List&lt;/code&gt;。比如&lt;code&gt;List[String]&lt;/code&gt;经过&lt;code&gt;map&lt;/code&gt;处理后可以变成&lt;code&gt;List[Interger]&lt;/code&gt;
其他collection的代码可以自行查看，也可以查看Scala的官方文档&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.scala-lang.org/api/current/scala/collection&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.scala-lang.org/api/current/scala/collection&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;spark中的map算子&#34;&gt;Spark中的map算子
&lt;/h2&gt;&lt;p&gt;Spark中的算子分为转换算子（Transformations (return a new RDD)）和行动算子（Actions (launch a job to return a value to the user program)）， 转换算子根据数据处理方式的不同将算子整体上分为 Value 类型、双 Value 类型和 Key-Value 类型  。具体不再细讲，可以自行查询。
RDD中的&lt;code&gt;map&lt;/code&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   * Return a new RDD by applying a function to all elements of this RDD.
   */
  def map[U: ClassTag](f: T =&amp;gt; U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (_, _, iter) =&amp;gt; iter.map(cleanF))
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看出，这里的&lt;code&gt;map&lt;/code&gt;首先创建了一个&lt;code&gt;MapPartitionsRDD&lt;/code&gt;并生成可迭代对象&lt;code&gt;iter&lt;/code&gt;,然后调用了Scala的&lt;code&gt;map&lt;/code&gt;函数处理&lt;code&gt;iter&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论
&lt;/h2&gt;&lt;p&gt;Scala里的&lt;code&gt;map&lt;/code&gt;函数首先定义在&lt;code&gt;scala.collection&lt;/code&gt;里，然后子类（&lt;code&gt;List&lt;/code&gt;、&lt;code&gt;Set&lt;/code&gt;）重写父类的函数。因为Scala里的类型是隐式的，并且查看源代码在一个子类里也只发现了一个&lt;code&gt;map&lt;/code&gt;函数，所以Scala里的&lt;code&gt;map&lt;/code&gt;并没有重载，而是通过定义父类，子类重写父类函数的方法实现对不同数据结构的操作。
Spark里的&lt;code&gt;map&lt;/code&gt;是对RDD进行操作的算子，实际使用了可迭代对象来调用Scala中的&lt;code&gt;map&lt;/code&gt;函数。算子本身的定义就是对RDD操作的函数，所以算子应该也可以被称为是函数，但是为了区分Scala中的函数，所以使用了不同的名字。&lt;/p&gt;
&lt;h2 id=&#34;补充&#34;&gt;补充
&lt;/h2&gt;&lt;p&gt;Spark中的算子并非全都有同名函数，原因可以从RDD的原理上分析。
行动算子需要进行&lt;code&gt;shuffle&lt;/code&gt;操作，在&lt;code&gt;shuffle&lt;/code&gt;时需要按键分区，对每个分区进行操作后输出。Scala中并没有&lt;code&gt;Shuffle&lt;/code&gt;操作，所以行动算子没有同名函数。
而转换算子是生成RDD或者将RDD转换成另外的RDD，Scala本身也有将&lt;code&gt;collection&lt;/code&gt;转换为&lt;code&gt;collection&lt;/code&gt;的函数，并且转换算子本身就调用了Scala的函数，所以有同名的也正常。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Spark程序大量Info日志问题解决</title>
        <link>https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</link>
        <pubDate>Sat, 07 Sep 2024 11:39:08 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</guid>
        <description>&lt;p&gt;Spark程序在启动后会在控制台打印大量日志，找了很多教程也没有解决，本来一直可以忍受的。但是学SparkStreaming时实在受不了了，日志已经严重影响到我查看计算结果。遂痛下决心，解决这个一直困扰我的问题。如果使用方法1没有解决的可以直接去看第三步，第二步是解决日志依赖冲突的问题的&lt;/p&gt;
&lt;h2 id=&#34;常规解决办法&#34;&gt;常规解决办法
&lt;/h2&gt;&lt;p&gt;在resource下建立一个log4j.properties，填入下列内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/ddHH:mm:ss} %p %c{1}: %m%n
# Set the default spark-shell log level to ERROR. When running the spark-shell,the
# log level for this class is used to overwrite the root logger&#39;s log level, so that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=ERROR
# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/1.png&#34;
	width=&#34;1587&#34;
	height=&#34;603&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/1_hu5431730765514624941.png 480w, https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/1_hu12252453515852226048.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;631px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;查看日志寻求解决办法&#34;&gt;查看日志寻求解决办法
&lt;/h2&gt;&lt;p&gt;常规解决办法没有正常解决，遂查看日志寻求解决办法，查看日志可以明显看出有日志依赖冲突&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/D:/maven-3.8.6/respository/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/D:/maven-3.8.6/respository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;怀疑是日志冲突的问题（事实证明不是），分析日志依赖
&lt;img src=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/2.png&#34;
	width=&#34;632&#34;
	height=&#34;906&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/2_hu1458278568563639439.png 480w, https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/2_hu3302884581586329615.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;69&#34;
		data-flex-basis=&#34;167px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在SparkCore下面发现了slf4j的依赖，在spark-core的dependency里加入下列内容以屏蔽日志包。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;exclusions&amp;gt;
  &amp;lt;exclusion&amp;gt;
    &amp;lt;groupId&amp;gt;org.slf4j&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;slf4j-reload4j&amp;lt;/artifactId&amp;gt;
  &amp;lt;/exclusion&amp;gt;
&amp;lt;/exclusions&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/3.png&#34;
	width=&#34;811&#34;
	height=&#34;446&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/3_hu2070361217706267611.png 480w, https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/3_hu5119317489834807306.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;436px&#34;
	
&gt;
还是日志冲突，把所有dependency下面都加了排除日志依赖的标签
还是不起作用，观察日志可知冲突是因为reload4hj，干脆把仓库里的reload4j删掉，成功解决日志冲突
&lt;img src=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/4.png&#34;
	width=&#34;899&#34;
	height=&#34;375&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/4_hu3281290504749333552.png 480w, https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/4_hu6512393018722807227.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;575px&#34;
	
&gt;
但是但是，日志依赖冲突的问题解决了，大量info日志的问题却还在&lt;/p&gt;
&lt;h2 id=&#34;最终解决办法&#34;&gt;最终解决办法
&lt;/h2&gt;&lt;p&gt;在resource下新建一个&lt;code&gt;log4j2.xml&lt;/code&gt;文件，填入下面内容即可解决。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;Configuration status=&amp;quot;WARN&amp;quot;&amp;gt;
    &amp;lt;Appenders&amp;gt;
        &amp;lt;Console name=&amp;quot;Console&amp;quot; target=&amp;quot;SYSTEM_OUT&amp;quot;&amp;gt;
            &amp;lt;PatternLayout&amp;gt;
                &amp;lt;Pattern&amp;gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&amp;lt;/Pattern&amp;gt;
            &amp;lt;/PatternLayout&amp;gt;
        &amp;lt;/Console&amp;gt;
    &amp;lt;/Appenders&amp;gt;
    &amp;lt;Loggers&amp;gt;
        &amp;lt;Root level=&amp;quot;error&amp;quot;&amp;gt;
            &amp;lt;AppenderRef ref=&amp;quot;Console&amp;quot; /&amp;gt;
        &amp;lt;/Root&amp;gt;
    &amp;lt;/Loggers&amp;gt;
&amp;lt;/Configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/5.png&#34;
	width=&#34;1210&#34;
	height=&#34;624&#34;
	srcset=&#34;https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/5_hu14369273875121618743.png 480w, https://rusthx.github.io/p/spark%E7%A8%8B%E5%BA%8F%E5%A4%A7%E9%87%8Finfo%E6%97%A5%E5%BF%97%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/5_hu15208406693693291075.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Ubuntu22.04配置Spark3.3.1集群</title>
        <link>https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/</link>
        <pubDate>Sat, 07 Sep 2024 11:22:39 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/</guid>
        <description>&lt;p&gt;Spark启动方式有：local模式、standalone模式、Yarn模式、K8S和Mesos模式，本教程只涉及前三种模式，另外两种可以自行查找资料。&lt;/p&gt;
&lt;h2 id=&#34;local模式&#34;&gt;Local模式
&lt;/h2&gt;&lt;h3 id=&#34;1下载spark&#34;&gt;1.下载Spark
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://archive.apache.org/dist/spark/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://archive.apache.org/dist/spark/&lt;/a&gt;
由于我的Hadoop版本是3.1.3，所以下载的Spark版本也是Spark3,这里下的是Spark3.3.1，只要是Spark3都可以和Hadoop3兼容。&lt;/p&gt;
&lt;h3 id=&#34;2解压spark压缩包&#34;&gt;2.解压Spark压缩包
&lt;/h3&gt;&lt;p&gt;解压Spark的压缩包，移动到&lt;code&gt;/usr/local/&lt;/code&gt;下，修改文件夹的名字为spark&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ~/Downloads
sudo tar -zxvf spark-3.3.1-bin-hadoop3.tgz -C /usr/local/
cd /usr/local/
mv spark-3.3.1-bin-hadoop3.2 spark
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3local模式启动spark&#34;&gt;3.Local模式启动Spark
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/spark-shell
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动成功后，可以输入网址&lt;code&gt;主机名：4040&lt;/code&gt;进行 Web UI 监控页面访问
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/1.png&#34;
	width=&#34;692&#34;
	height=&#34;542&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/1_hu13411659414447418761.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/1_hu15091209146063601315.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;306px&#34;
	
&gt;
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/2.png&#34;
	width=&#34;692&#34;
	height=&#34;256&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/2_hu10899964117634986959.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/2_hu13561336368744297822.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;270&#34;
		data-flex-basis=&#34;648px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;standalone模式&#34;&gt;Standalone模式
&lt;/h2&gt;&lt;h3 id=&#34;1进入spark文件夹下的conf目录修改workerstemplate文件名为workers&#34;&gt;1.进入spark文件夹下的conf目录，修改workers.template文件名为workers
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd conf/
mv workers.template workers
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2修改workers文件添加worker节点&#34;&gt;2.修改workers文件，添加worker节点
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; vim workers 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/3.png&#34;
	width=&#34;586&#34;
	height=&#34;75&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/3_hu9370041268607244147.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/3_hu3025662213800880645.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;781&#34;
		data-flex-basis=&#34;1875px&#34;
	
&gt;
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/4.png&#34;
	width=&#34;1111&#34;
	height=&#34;428&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/4_hu17370934111469867891.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/4_hu4728401156525235443.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;259&#34;
		data-flex-basis=&#34;622px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;3修改spark-envshtemplate文件名为spark-envsh&#34;&gt;3.修改spark-env.sh.template文件名为spark-env.sh
&lt;/h3&gt;&lt;h3 id=&#34;4修改spark-envsh文件添加java_home环境变量和集群对应的master节点&#34;&gt;4.修改spark-env.sh文件，添加&lt;code&gt;JAVA_HOME&lt;/code&gt;环境变量和集群对应的master节点
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/5.png&#34;
	width=&#34;664&#34;
	height=&#34;44&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/5_hu18412699254950233080.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/5_hu15348095363903452333.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1509&#34;
		data-flex-basis=&#34;3621px&#34;
	
&gt;
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/6.png&#34;
	width=&#34;517&#34;
	height=&#34;151&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/6_hu11511121656198141294.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/6_hu15755966112917960182.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;342&#34;
		data-flex-basis=&#34;821px&#34;
	
&gt;
Java默认安装路径如下，手动安装的Java可以指定自己的Java路径
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/7.png&#34;
	width=&#34;692&#34;
	height=&#34;166&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/7_hu8520661546659697823.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/7_hu282144205048903332.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;416&#34;
		data-flex-basis=&#34;1000px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;5分发spark&#34;&gt;5.分发Spark
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/8.png&#34;
	width=&#34;451&#34;
	height=&#34;101&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/8_hu6963929853805175707.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/8_hu6241441082680902207.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;446&#34;
		data-flex-basis=&#34;1071px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;6standalone模式启动spark集群&#34;&gt;6.Standalone模式启动Spark集群
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd spark/
sbin/start-all.sh
xcall jps
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;7查看进程&#34;&gt;7.查看进程
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/9.png&#34;
	width=&#34;692&#34;
	height=&#34;343&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/9_hu11366282666986785103.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/9_hu12612634464676384337.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;484px&#34;
	
&gt;
Spark正常启动输入网址&lt;code&gt;主机名:8080&lt;/code&gt;进行监控
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/10.png&#34;
	width=&#34;692&#34;
	height=&#34;328&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/10_hu2060644001827612340.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/10_hu15856899553100142721.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;210&#34;
		data-flex-basis=&#34;506px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;8提交应用测试spark&#34;&gt;8.提交应用测试Spark
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/spark-submit 
--class org.apache.spark.examples.SparkPi 
--master spark://hadoop1:7077 
./examples/jars/spark-examples_2.12-3.3.1.jar 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意：&amp;ndash;master后面指定的主机名要改成自己的主机名（hadoop1改成自己的主机名）
指定的jar包要指定为自己的jar包，不同版本的示例jar包名字不同。
10是指当前应用的任务数量
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/11.png&#34;
	width=&#34;691&#34;
	height=&#34;121&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/11_hu5688388085362936308.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/11_hu7159536826896782106.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;571&#34;
		data-flex-basis=&#34;1370px&#34;
	
&gt;
提交任务时会有一个SparkSubmit进程，任务结束后进程停止
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/12.png&#34;
	width=&#34;547&#34;
	height=&#34;569&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/12_hu7418334220115483844.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/12_hu6016238825539299710.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;96&#34;
		data-flex-basis=&#34;230px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;yarn-模式&#34;&gt;Yarn 模式
&lt;/h2&gt;&lt;h3 id=&#34;1修改hadoop配置文件&#34;&gt;1.修改Hadoop配置文件
&lt;/h3&gt;&lt;p&gt;修改&lt;code&gt;/usr/local/hadoop/etc/hadoop/yarn-site.xml&lt;/code&gt;, 并分发&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim /usr/local/hadoop/etc/hadoop/yarn-site.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认
是true --&amp;gt; 
&amp;lt;property&amp;gt; 
&amp;lt;name&amp;gt;yarn.nodemanager.pmem-check-enabled&amp;lt;/name&amp;gt; 
&amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt; 
&amp;lt;/property&amp;gt; 
&amp;lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认
是true --&amp;gt; 
&amp;lt;property&amp;gt; 
&amp;lt;name&amp;gt;yarn.nodemanager.vmem-check-enabled&amp;lt;/name&amp;gt; 
&amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt; 
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/13.png&#34;
	width=&#34;692&#34;
	height=&#34;549&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/13_hu12271283889677969348.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/13_hu16736059404118154314.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;302px&#34;
	
&gt;
分发修改后的配置文件&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;xsync /usr/local/hadoop/etc/hadoop/yarn-site.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/14.png&#34;
	width=&#34;693&#34;
	height=&#34;176&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/14_hu6905782161851684135.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/14_hu1804595154733794608.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;393&#34;
		data-flex-basis=&#34;945px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-修改confspark-envsh添加-java_home-和-yarn_conf_dir-配置&#34;&gt;2. 修改conf/spark-env.sh，添加 &lt;code&gt;JAVA_HOME&lt;/code&gt; 和 &lt;code&gt;YARN_CONF_DIR &lt;/code&gt;配置
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim conf/spark-env.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/15.png&#34;
	width=&#34;692&#34;
	height=&#34;549&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/15_hu1429263151845228789.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/15_hu10707229636908163097.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;126&#34;
		data-flex-basis=&#34;302px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;3分发更改后的spark-envsh&#34;&gt;3.分发更改后的Spark-env.sh
&lt;/h3&gt;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;xsync conf/spark-env.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/16.png&#34;
	width=&#34;531&#34;
	height=&#34;213&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/16_hu14375517168970993541.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/16_hu10177607509824995458.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;249&#34;
		data-flex-basis=&#34;598px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;4yarn模式提交任务测试&#34;&gt;4.Yarn模式提交任务测试
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Client模式&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/spark-submit --class org.apache.spark.examples.SparkPi
--master yarn 
--deploy-mode client
./examples/jars/spark-examples_2.12-3.3.1.jar 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/17.png&#34;
	width=&#34;692&#34;
	height=&#34;83&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/17_hu9853654027330023775.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/17_hu14851843207891514165.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;833&#34;
		data-flex-basis=&#34;2000px&#34;
	
&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Cluster模式&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/spark-submit --class org.apache.spark.examples.SparkPi
--master yarn 
--deploy-mode cluster
./examples/jars/spark-examples_2.12-3.3.1.jar 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/18.png&#34;
	width=&#34;692&#34;
	height=&#34;111&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/18_hu14513159411837347509.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/18_hu5433995651513985956.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;623&#34;
		data-flex-basis=&#34;1496px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;5在hadoop18088查看程序运行成功&#34;&gt;5.在hadoop1:8088查看，程序运行成功
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/19.png&#34;
	width=&#34;692&#34;
	height=&#34;289&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/19_hu17058640190159608060.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEspark3.3.1%E9%9B%86%E7%BE%A4/19_hu1839875858801763919.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;补充提交参数说明&#34;&gt;补充：提交参数说明
&lt;/h2&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;参数&lt;/th&gt;
          &lt;th&gt;解释&lt;/th&gt;
          &lt;th&gt;可选值举例&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;ndash;class&lt;/td&gt;
          &lt;td&gt;Spark 程序中包含主函数的类&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;ndash;master&lt;/td&gt;
          &lt;td&gt;Spark 程序运行的模式(环境)&lt;/td&gt;
          &lt;td&gt;模式：local[*]、spark://hadoop1:7077、Yarn&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;ndash;executor-memory 1G&lt;/td&gt;
          &lt;td&gt;指定每个executor 可用内存为1G&lt;/td&gt;
          &lt;td&gt;符合集群内存配置即可，具体情况具体分析。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;ndash;total-executor-cores 2&lt;/td&gt;
          &lt;td&gt;指定所有executor使用的cpu核数析。为2个&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&amp;ndash;executor-cores&lt;/td&gt;
          &lt;td&gt;指定每个executor使用的cpu核数&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;application-jar&lt;/td&gt;
          &lt;td&gt;打包好的应用 jar，包含依赖。这个URL 在集群中全局可见。比如 hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;path 都包含同样的 jar&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;application-arguments&lt;/td&gt;
          &lt;td&gt;传给 main()方法的参数&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        <item>
        <title>Ubuntu22.04配置Hive及Hive on Spark</title>
        <link>https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/</link>
        <pubDate>Sat, 07 Sep 2024 11:15:48 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/</guid>
        <description>&lt;h2 id=&#34;前置准备配置hive的mysql连接用户&#34;&gt;前置准备：配置Hive的MySQL连接用户
&lt;/h2&gt;&lt;p&gt;MySQL的配置可参考我的教程 &lt;a class=&#34;link&#34; href=&#34;https://rusthx.github.io/p/ubuntu22.04%E5%AE%89%E8%A3%85mysql8.0.35/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MySQL安装教程&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建Hive元数据库&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create database metastore;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;创建用户hive，设置密码为123456&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create user &#39;hive&#39;@&#39;%&#39; identified by &#39;123456&#39;;
grant all privileges on metastore.* to &#39;hive&#39;@&#39;%&#39; with grant option;
flush privileges;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;安装hive&#34;&gt;安装Hive
&lt;/h2&gt;&lt;p&gt;参考资料：B站尚硅谷
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1mG411o7Lt?p=62&amp;amp;vd_source=2db7c64d895a2907954a5b8725db55d5&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;062.Hive的安装部署_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载Hive安装包&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：apache原装的Hive只支持Spark2.3.0，不支持Spark3.3.0，需要重新编译Hive的源码，尚硅谷已经编译好了，这里我就直接使用了&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;修改配置文件（cd 到hive下的conf文件夹，这里我已经将Hive安装包改名为hive并移动到/usr/local/下）&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo mv hive-default.xml.template hive-default.xml
sudo vim hive-site.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将以下内容写入文件&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&amp;quot;text/xsl&amp;quot; href=&amp;quot;configuration.xsl&amp;quot;?&amp;gt;

&amp;lt;configuration&amp;gt;
    &amp;lt;!-- jdbc连接的URL --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;jdbc:mysql://hadoop1:3306/metastore?useUnicode=true&amp;amp;amp;characterEncodeing=UTF-8&amp;amp;amp;allowPublicKeyRetrieval=true&amp;amp;amp;useSSL=false&amp;amp;amp;serverTimezone=GMT&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
    &amp;lt;!-- jdbc连接的Driver--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
	&amp;lt;!-- jdbc连接的username--&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hive&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- jdbc连接的password --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;123456&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
    &amp;lt;!-- Hive默认在HDFS的工作目录 --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hive.metastore.warehouse.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;/user/hive/warehouse&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
        &amp;lt;!-- 指定存储元数据要连接的地址 --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.metastore.uris&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;thrift://hadoop1:9083&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    
    &amp;lt;!-- 指定hiveserver2连接的host --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.server2.thrift.bind.host&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;hadoop1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- 指定hiveserver2连接的端口号 --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.server2.thrift.port&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;10000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;!-- hiveserver2的高可用参数，开启此参数可以提高hiveserver2的启动速度 --&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.server2.active.passive.ha.enable&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.cli.print.header&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hive.cli.print.current.db&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;配置环境变量&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo vim ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在下面添加&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;启动hive&#34;&gt;启动Hive
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;启动Hadoop&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;start-all.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;初始化Hive&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive
./bin/schematool -dbType mysql -initSchema
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正常初始化会日志刷屏并出现大片空白，然后最后一行出现succeed或者complete的字样
如果没有正常初始化就复制最下面几行中的报错信息，粘贴到必应进行查找&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;启动Hive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;启动Hive前需要先启动Hive的元数据库metastore和hiveserver2
注意：这里的metastore和hiveserver2每个都要单独开启一个终端，开启一个后再开一个新的终端进行命令
日志被重定向到了logs文件夹下，需要查看日志可以在这个文件夹下查看&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive/
hive --service metastore &amp;gt;logs/metastore.log 2&amp;gt;&amp;amp;1
hive --service hiveserver2 &amp;gt;logs/hiveServer2.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/hive
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;正常启动会出现一个交互界面如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hive(default)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;解决Hive shell中打印大量日志的问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当在Hive的命令行中查询时出现大量日志时，可以在conf下新建日志配置文件如下&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/conf/
vim log4j.properties
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;粘贴如下内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;log4j.rootLogger=WARN, CA
log4j.appender.CA=org.apache.log4j.ConsoleAppender
log4j.appender.CA.layout=org.apache.log4j.PatternLayout
log4j.appender.CA.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hive-on-spark配置&#34;&gt;Hive on Spark配置
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;在官网下载纯净版Spark（不带Hadoop依赖的）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://spark.apache.org/downloads.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://spark.apache.org/downloads.html&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;解压Spark&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tar -zxvf spark-3.3.1-bin-without-hadoop.tgz -C /usr/local/
mv /usr/local/spark-3.3.1-bin-without-hadoop /usr/local/spark
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;修改spark-env.sh配置文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mv /usr/local/spark/conf/spark-env.sh.template /usr/local/spark/conf/spark-env.sh
vim /usr/local/spark/conf/spark-env.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;增添下面内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export SPARK_DIST_CLASSPATH=$(hadoop classpath)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;配置Spark环境变量&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo vim ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;添加下列内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;在Hive中创建spark配置文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim /usr/local/hive/conf/spark-defaults.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;添加如下内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;spark.master                               yarn
spark.eventLog.enabled                   true
spark.eventLog.dir                        hdfs://hadoop1:8020/spark-history
spark.executor.memory                    1g
spark.driver.memory					     1g
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在HDFS中创建如下路径，用于存储历史日志&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hadoop fs -mkdir /spark-history
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;向HDFS上传Spark纯净版jar包&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;说明1：采用Spark纯净版jar包，不包含hadoop和hive相关依赖，能避免依赖冲突。
说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hadoop fs -mkdir /spark-jars
hadoop fs -put /usr/local/spark/jars/* /spark-jars
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;修改hive-site.xml文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim /usr/local/hive/conf/hive-site.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;添加如下内容&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;spark.yarn.jars&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;hdfs://hadoop1:8020/spark-jars/*&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;!--Hive执行引擎--&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.execution.engine&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;spark&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;!--连接超时时间--&amp;gt;
&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hive.spark.client.connect.timeout&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;30000ms&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hive-on-spark测试&#34;&gt;Hive on Spark测试
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;启动hive客户端&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /usr/local/hive/
bin/hive
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;创建一张测试表&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;hive (default)&amp;gt; create table student(id int, name string);
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;通过insert测试效果&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;hive (default)&amp;gt; insert into table student values(1,&#39;abc&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果如下则配置成功
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/1.png&#34;
	width=&#34;1643&#34;
	height=&#34;686&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/1_hu18263344740042083967.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/1_hu11299025821609503695.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;
&lt;img src=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/2.png&#34;
	width=&#34;914&#34;
	height=&#34;659&#34;
	srcset=&#34;https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/2_hu13084986199049206096.png 480w, https://rusthx.github.io/p/ubuntu22.04%E9%85%8D%E7%BD%AEhive%E5%8F%8Ahive-on-spark/2_hu2576634140735695222.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;yarn环境配置&#34;&gt;Yarn环境配置
&lt;/h2&gt;&lt;p&gt;增加ApplicationMaster资源比例
容量调度器对每个资源队列中同时运行的Application Master占用的资源进行了限制，该限制通过yarn.scheduler.capacity.maximum-am-resource-percent参数实现，其默认值是0.1，表示每个资源队列上Application Master最多可使用的资源为该队列总资源的10%，目的是防止大部分资源都被Application Master占用，而导致Map/Reduce Task无法执行。
生产环境该参数可使用默认值。但学习环境，集群资源总数很少，如果只分配10%的资源给Application Master，则可能出现，同一时刻只能运行一个Job的情况，因为一个Application Master使用的资源就可能已经达到10%的上限了。故此处可将该值适当调大。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在hadoop1的/usr/local/hadoop/etc/hadoop/capacity-scheduler.xml文件中&lt;strong&gt;修改&lt;/strong&gt;如下参数值&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vim capacity-scheduler.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;lt;property&amp;gt;
   &amp;lt;name&amp;gt;yarn.scheduler.capacity.maximum-am-resource-percent&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;0.8&amp;lt;/value&amp;gt;
&amp;lt;/property
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;分发capacity-scheduler.xml配置文件&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;xsync capacity-scheduler.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;重启集群&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;stop-all.sh
start-all.sh
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
