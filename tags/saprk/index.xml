<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Saprk on rustWood</title>
        <link>https://rusthx.github.io/tags/saprk/</link>
        <description>Recent content in Saprk on rustWood</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>rustWood</copyright>
        <lastBuildDate>Sun, 27 Oct 2024 23:18:18 +0800</lastBuildDate><atom:link href="https://rusthx.github.io/tags/saprk/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Spark面经</title>
        <link>https://rusthx.github.io/p/spark%E9%9D%A2%E7%BB%8F/</link>
        <pubDate>Sun, 27 Oct 2024 23:18:18 +0800</pubDate>
        
        <guid>https://rusthx.github.io/p/spark%E9%9D%A2%E7%BB%8F/</guid>
        <description>&lt;h2 id=&#34;saprk定义&#34;&gt;Saprk定义
&lt;/h2&gt;&lt;p&gt;Apache Spark 是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习。&lt;/p&gt;
&lt;!-- 图片居中 --&gt;
&lt;div align=center &gt;
	&lt;img src=&#34;1.png&#34;/&gt;
&lt;/div&gt;
&lt;!-- ![](1.png) --&gt;
## Spark组件
&lt;h2 id=&#34;spark三大抽象数据结构&#34;&gt;Spark三大抽象数据结构
&lt;/h2&gt;&lt;p&gt;RDD:弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行
计算的集合。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;弹性
&lt;ul&gt;
&lt;li&gt;存储的弹性：内存与磁盘的自动切换；&lt;/li&gt;
&lt;li&gt;容错的弹性：数据丢失可以自动恢复；&lt;/li&gt;
&lt;li&gt;计算的弹性：计算出错重试机制；&lt;/li&gt;
&lt;li&gt;分片的弹性：可根据需要重新分片。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;分布式：数据存储在大数据集群不同节点上&lt;/li&gt;
&lt;li&gt;数据集：RDD 封装了计算逻辑，并不保存数据&lt;/li&gt;
&lt;li&gt;数据抽象：RDD 是一个抽象类，需要子类具体实现&lt;/li&gt;
&lt;li&gt;不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑&lt;/li&gt;
&lt;li&gt;可分区、并行计算&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;累加器：累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在
Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，
传回 Driver 端进行 merge。&lt;/p&gt;
&lt;p&gt;广播变量：广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个
或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，
广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务
分别发送&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package RDD1

import org.apache.spark.util.LongAccumulator
import org.apache.spark.{SparkConf, SparkContext}

object task1 {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;job3task1&amp;quot;)
    val sc = new SparkContext(sparkConf)

    //通过文件生成RDD
    val data = sc.textFile(&amp;quot;hdfs://hadoop1:8020/Data01.txt&amp;quot;)

     println(data.map(
      x =&amp;gt; {
        val student = x.split(&amp;quot;,&amp;quot;)(0)
        student
      }
    ).distinct().count())//学生数量
    println(data.map(_.split(&amp;quot;,&amp;quot;)(1)).distinct().count())//课程数量

    val value = data.map(
      x =&amp;gt; {
        val student = x.split(&amp;quot;,&amp;quot;)(0)
        val grade = Integer.parseInt(x.split(&amp;quot;,&amp;quot;)(2))
        (student, grade)
      }).filter(_._1==&amp;quot;Tom&amp;quot;)
      .groupByKey().map{
      y=&amp;gt;{
      y._2.sum/y._2.size.toDouble
    }}
    value.foreach(println) //Tom的课程平均分

    val studentCourse = data.map(
      x=&amp;gt;{
        val student = x.split(&amp;quot;,&amp;quot;)(0)

        (student,1)
      }
    ).groupByKey().mapValues(_.size)
    studentCourse.foreach(println)//学生选课数

    val database = data.map(
      x=&amp;gt;{
        val course = x.split(&amp;quot;,&amp;quot;)(1)
        (course,1)
      }
    ).filter(_._1==&amp;quot;DataBase&amp;quot;).groupByKey().map(y=&amp;gt;y._2.size)
    database.foreach(println)//选择了DataBase课的学生

    val avgGrade = data.map(
      x=&amp;gt;{
        val course = x.split(&amp;quot;,&amp;quot;)(1)
        val grade = Integer.parseInt(x.split(&amp;quot;,&amp;quot;)(2))
        (course,grade)
      }
    ).groupByKey().mapValues(y=&amp;gt;{y.sum/y.size.toDouble})
    avgGrade.foreach(println)//每门课的平均分


    val dbCount:LongAccumulator =  sc.longAccumulator(&amp;quot;dbCount&amp;quot;)
    data.foreach { line =&amp;gt;
      val course = line.split(&amp;quot;,&amp;quot;)(1)
      if (course == &amp;quot;DataBase&amp;quot;) {
        // 每当课程为&amp;quot;DataBase&amp;quot;时，累加器的值加1
        dbCount.add(1)
      }
    }
    println(&amp;quot;DataBase count: &amp;quot; + dbCount.value)
    //累加器实现查找选择了DataBase课的学生

    sc.stop()
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package SprakReview

import org.apache.spark.broadcast.Broadcast
import org.apache.spark.{SparkConf, SparkContext}

object BroadCastTry {
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster(&amp;quot;local[*]&amp;quot;).setAppName(&amp;quot;广播变量练习&amp;quot;)
    val sc = new SparkContext(sparkConf)

    val v = Array(1,2,3,4,5,6)
    // 创建广播变量
    val broadV:Broadcast[Array[Int]] = sc.broadcast(v)

    //打印广播变量
    println(broadV.value.mkString(&amp;quot;Array(&amp;quot;, &amp;quot;, &amp;quot;, &amp;quot;)&amp;quot;))

    //销毁广播变量
    broadV.destroy()

    sc.stop()
  }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;spark行动算子&#34;&gt;Spark行动算子
&lt;/h2&gt;&lt;div align=center &gt;
	&lt;img src=&#34;2.png&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;spark转换算子&#34;&gt;Spark转换算子
&lt;/h2&gt;&lt;div align=center &gt;
	&lt;img src=&#34;3.png&#34;/&gt;
&lt;/div&gt;
&lt;h2 id=&#34;map-mappartitions-flatmap-区别&#34;&gt;map mapPartitions flatMap 区别
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;code&gt;map&lt;/code&gt;
将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   * Return a new RDD by applying a function to all elements of this RDD.
   */
  def map[U: ClassTag](f: T =&amp;gt; U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (_, _, iter) =&amp;gt; iter.map(cleanF))
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;code&gt;mapPartitions&lt;/code&gt; 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处
理，哪怕是过滤数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   * Return a new RDD by applying a function to each partition of this RDD.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn&#39;t modify the keys.
   */
  def mapPartitions[U: ClassTag](
      f: Iterator[T] =&amp;gt; Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (_: TaskContext, _: Int, iter: Iterator[T]) =&amp;gt; cleanedF(iter),
      preservesPartitioning)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;map 和 mapPartitions 的区别？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据处理角度
Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;功能的角度
Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。
MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，
所以可以增加或减少数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性能的角度
Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;code&gt;flatMap&lt;/code&gt;
将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   *  Return a new RDD by first applying a function to all elements of this
   *  RDD, and then flattening the results.
   */
  def flatMap[U: ClassTag](f: T =&amp;gt; TraversableOnce[U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (_, _, iter) =&amp;gt; iter.flatMap(cleanF))
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;补充：&lt;code&gt;mapPartitionsWithIndex&lt;/code&gt; 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处
理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;  /**
   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index
   * of the original partition.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn&#39;t modify the keys.
   */
  def mapPartitionsWithIndex[U: ClassTag](
      f: (Int, Iterator[T]) =&amp;gt; Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (_: TaskContext, index: Int, iter: Iterator[T]) =&amp;gt; cleanedF(index, iter),
      preservesPartitioning)
  }
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
        
    </channel>
</rss>
